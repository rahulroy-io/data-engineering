<path=>gluecontainer.txt<content=>
docker pull amazon/aws-glue-libs:glue_libs_3.0.0_image_01
docker run -it --name awsgluecontainer -p 2200:22 amazon/aws-glue-libs:glue_libs_3.0.0_image_01
docker run -it --name awsgluecontainer -p 2200:22 amazon/aws-glue-libs:glue_libs_3.0.0_image_01

docker run -it --name gluecontainer_dockerbind -p 2800:22 -v C:\Users\rahulr8\OneDriveRR\Project\SERVICENOW\Testing\bind:/home/glue_user/workspace/ amazon/aws-glue-libs:glue_libs_3.0.0_image_01

open in root
docker exec -u root -t -i 45bce70e325f /bin/bash

yum install -y passwd initscripts nano
usermod -aG wheel glue_user
sudo passwd glue_user 
<provide passwd>

yum install sudo

su using glue_user 
sudo su
<provide passwd>

kbxtdip -> directly download and install
cp from windows<path=>newprojectCreation.txt<content=>
./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.Analytics.DL.ServiceNow.Task

rename project folder
KBX.Analytics.DL.ServiceNow.Task.Transform -> KBX.Analytics.DL.ServiceNow.Task.Structured

open this project in vscode
shift + cntl + f
replace Task.Transform with Task.Structured<path=>o.txt<content=>
<path=>pythonpaths.txt<content=>
org
"C:\Program Files\Python39"

sp1
"C:\Users\rahulr8\sp1\Scripts\activate"


add in activate
set "JAVA_HOME=C:\Program Files\Java\jre1.8.0_151"

add interpreter path in vscode
cntl+shift+p -> add interpreter path -> C:\Users\rahulr8\sp1\Scripts\python.exe

set "HADOOP_HOME=C:\Users\rahulr8\sp1\Lib\site-packages\pyspark\jars"
<path=>readme.txt<content=>
tranform  https://dev.azure.com/kbxltrans/Platform-Data/_git/KBX.DL.CodeTemplates.Transform
airflow https://dev.azure.com/kbxltrans/Platform-Data/_git/KBX.DL.CodeTemplates.Workflow



run in templete folder
./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.Analytics.DL.SNow.Demo
Format MUST be: KBX.[Product].DL.[Entity]
KBX.Analytics.DL.SNow.Demo


two folders are created
KBX.Analytics.DL.SNow.Demo.Transform.Jobs
-> logic and modules

KBX.Analytics.DL.SNow.Demo.Transform.Infrastructure
-> cf scripts


move to ubuntu container
docker cp KBX.Analytics.DL.SNow.Demo.Transform dc250966b716:/sparkdev


ProjectName = KBX.Analytics.DL.SNow.Demo.Transform
ProductName = $ProjectName.Split('.')[1]
EntityName = $ProjectName.Split('.')[3]

prefix = kbxt-dl
product = Analytics
entity = SNow

bucket

Source = '${Prefix}-${Product}-${Entity}-${StageSource}-${Environment}'
kbxt-dl-Analytics-SNow-[raw | structured]-dev

Target = '${Prefix}-${Product}-${Entity}-${StageTarget}-${Environment}'
kbxt-dl-Analytics-SNow-[structured | curated]-dev


--------container----------
dummy aws config will create the path -> copy the cred from .aws [windows] by running kochid aws refresh
.aws
|-- config
`-- credentials
chk with 
aws sts get-caller-identity

for docker symlink
mklink /J OneDriveRR "C:\Users\rahulr8\OneDrive - kochind.com"

docker run -it --name gluecontainer_dockerbind -p 2800:22 -v C:\Users\rahulr8\OneDriveRR\Project\SERVICENOW\Testing\bind:/home/glue_user/workspace/ amazon/aws-glue-libs:glue_libs_3.0.0_image_01

<path=>.\ADO\Data<content=>
<path=>ADO\Development\athena\New Text Document.txt<content=>
SHOW PARTITIONS kbxt_dl_analytics_db_structured_dev.servicenow_task

ALTER TABLE kbxt_dl_analytics_db_structured_dev.servicenow_task
DROP PARTITION (ingest_date = '2022-07-25', country = 'IN');

ALTER TABLE kbxt_dl_analytics_db_structured_dev.servicenow_task ADD
  PARTITION (ingest_date = "2022-07-25");<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Projects.Curated"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Projects.Curated"
$replacementEntityName = "PROJECTS"
$replacementEntityNameLower = "projects"
$replacementEntityNamePascal = "Projects"
$replacementEntityNameUpper = "PROJECTS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsprojects"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Projects.Curated
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'projects' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Curated/KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Curated/KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Curated/KBX.Analytics.DL.AzureDevOps.Projects.Curated.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Curated/KBX.Analytics.DL.AzureDevOps.Projects.Curated.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix


  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3",
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain
      }

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated\KBX.Analytics.DL.AzureDevOps.Projects.Curated.Jobs\transform.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Task.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1800-01-01"
#date_partition_override = "ingest_date=1800-01-01"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-azuredevops-projects-raw-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-azuredevops-projects-structured-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'projects' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'projects'
DOMAIN = args.Domain
#DOMAIN = 'azuredevops'
JOB_NAME = args.JOB_NAME
#JOB_NAME = 'kbxt-dl-analytics-azuredevops-projects-structured-job-dev'


# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATE_DATE_PARTITION_TO_PROCESS_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition : {date_partition}')
job.logger().info(f, f'is_replay : {is_replay}')

#JSON_FROM_SCHEMA = '{"fields":[{"metadata":{},"name":"result","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"active","nullable":true,"type":"string"},{"metadata":{},"name":"activity_due","nullable":true,"type":"string"},{"metadata":{},"name":"additional_assignee_list","nullable":true,"type":"string"},{"metadata":{},"name":"agile_story","nullable":true,"type":"string"},{"metadata":{},"name":"approval","nullable":true,"type":"string"},{"metadata":{},"name":"approval_history","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"business_duration","nullable":true,"type":"string"},{"metadata":{},"name":"business_service","nullable":true,"type":"string"},{"metadata":{},"name":"calendar_duration","nullable":true,"type":"string"},{"metadata":{},"name":"close_notes","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_by","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci_business_app","nullable":true,"type":"string"},{"metadata":{},"name":"comments","nullable":true,"type":"string"},{"metadata":{},"name":"company","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"contract","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_display","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_id","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"due_date","nullable":true,"type":"string"},{"metadata":{},"name":"escalation","nullable":true,"type":"string"},{"metadata":{},"name":"expected_start","nullable":true,"type":"string"},{"metadata":{},"name":"follow_up","nullable":true,"type":"string"},{"metadata":{},"name":"group_list","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"string"},{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"order","nullable":true,"type":"string"},{"metadata":{},"name":"parent","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"string"},{"metadata":{},"name":"route_reason","nullable":true,"type":"string"},{"metadata":{},"name":"short_description","nullable":true,"type":"string"},{"metadata":{},"name":"skills","nullable":true,"type":"string"},{"metadata":{},"name":"sla_due","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_document","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_esignature_configuration","nullable":true,"type":"string"},{"metadata":{},"name":"state","nullable":true,"type":"string"},{"metadata":{},"name":"sys_class_name","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_on","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain_path","nullable":true,"type":"string"},{"metadata":{},"name":"sys_id","nullable":true,"type":"string"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"string"},{"metadata":{},"name":"sys_tags","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_on","nullable":true,"type":"string"},{"metadata":{},"name":"task_effective_number","nullable":true,"type":"string"},{"metadata":{},"name":"time_worked","nullable":true,"type":"string"},{"metadata":{},"name":"u_all_classes_configuration_items","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_escalate","nullable":true,"type":"string"},{"metadata":{},"name":"u_estimated_delivery_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_catalog_item","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_customer","nullable":true,"type":"string"},{"metadata":{},"name":"u_manual_routing","nullable":true,"type":"string"},{"metadata":{},"name":"u_new_hire","nullable":true,"type":"string"},{"metadata":{},"name":"u_start_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_support_tier","nullable":true,"type":"string"},{"metadata":{},"name":"universal_request","nullable":true,"type":"string"},{"metadata":{},"name":"upon_approval","nullable":true,"type":"string"},{"metadata":{},"name":"upon_reject","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"user_input","nullable":true,"type":"string"},{"metadata":{},"name":"watch_list","nullable":true,"type":"string"},{"metadata":{},"name":"work_end","nullable":true,"type":"string"},{"metadata":{},"name":"work_notes","nullable":true,"type":"string"},{"metadata":{},"name":"work_start","nullable":true,"type":"string"}],"type":"struct"},"type":"array"}}],"type":"struct"}'
#schemaFromJson = StructType.fromJson(json.loads(JSON_FROM_SCHEMA))

# functions

#Flatten array of structs and structs
job.logger().info(f, f'###################_TASK-2_UDF_###################')

def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df

def union_unmatched_columns(df1, df2) :
    for column in [column for column in df2.columns if column not in df1.columns]:
        df1 = df1.withColumn(column, F.lit(None))

    for column in [column for column in df1.columns if column not in df2.columns]:
        df2 = df2.withColumn(column, F.lit(None))
    
    return df1.unionByName(df2)

#%% last curated ingest date
job.logger().info(f, f'###################_TASK-3_LAST_STRUCTURED/CURATED_INGEST_DATE_###################')

try :
    df_structured = spark.sql(f'''
        select 
            * 
        from 
            {structured_db}.{tbl}
        where
            False
        ''')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = True
    job.logger().info(f, f'structured : {structured}')
    
    # Dropping "job_id", "source", "ingest_date"
    job.logger().info(f, f'Dropping Columns : "job_id", "source", "ingest_date"')
    df_structured = df_structured.drop("job_id", "source", "ingest_date")

except Exception as e:
    job.logger().info(f, e)
    df_structured = spark.createDataFrame([], '')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = False
    job.logger().info(f, f'structured : {structured}')


# raise Exception('Forced Exception')

#%%------------------------------------------Job Start------------------------------------------
try :
    # All files for a date partition that haven't been processed are 
    # returned, so be cognizent of the size of this dataframe.
    job.logger().info(f, f'###################_TASK-4_JOB_START_READ_DATAFRAME_###################')
    # raw file df
    # df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=JSON_FROM_SCHEMA)
    df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, file_format='json')
except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%%------------------------------------------Job Process------------------------------------------

try:    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-5_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])
        
        job.logger().info(f, f'raw_dataframe')
        df.printSchema()
        df.show()

        # flattening df
        projects = list(df.columns)
        for project in projects :
            if df.select(df[project]['value']).collect()[0][0] != [] :
                df_flatten = flatten(df.select(df[project]['value']).withColumnRenamed(f'{project}.value', project)).select(lit(f'{project}').alias("project_name"), "*")
                #df_structured.show(truncate=15)
                df_flatten = df_flatten.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_flatten.columns]).drop('empty_string_value')
                df_structured = union_unmatched_columns(df_structured, df_flatten)

        #df_str = df_structured.select([col(c).cast("string") for c in df_structured.columns])
        #df_str = df_structured.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_structured.columns])
        
        #df_structured = df_str
        
        job.logger().info(f, f'df_structured schema : after union_unmatched_columns')
        df_structured.printSchema()
        df_structured.show()
        
        df_structured.createOrReplaceTempView('raw_data')
        df_structured = spark.sql(f'''
            select
                '{JOB_ID}'                                                                                job_id,
                '{SOURCE}'                                                                                source,                                                          
                *
            from raw_data
        ''')
		
        df_transformed = df_structured

        job.logger().info(f, f'df_transformed')
        df_transformed.printSchema()
        df_transformed.show()
        job.logger().info(f, f'df_transformed.count() : {df_transformed.count()}')

        job.logger().info(f, f'###################_TASK-6_COMMIT_FILE_###################')
        # Commit files
        if not df_transformed.rdd.isEmpty() :
            job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))
        
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-7_REFRESH_PARTITION/RUN_CRAWLER_###################')
        df_table = spark.sql(f'''show tables in {structured_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
                job.logger().info(f, f"partion {date_partition.split('=')[1]} added to {tbl}")
            except Exception as e:
                job.logger().info(f, e)
        else :
            job.logger().info(f, f'initiating {crawler} run for first time')
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler {crawler} is {state.lower()}.")
                    state_previous = state
        
        
        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        job.logger().info(f, f'###################_TASK-8_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")
#%%------------------------------------------Job End------------------------------------------

job.runtime().end()

# %%
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Workitems.Curated"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Workitems.Curated"
$replacementEntityName = "PROJECTS"
$replacementEntityNameLower = "projects"
$replacementEntityNamePascal = "Projects"
$replacementEntityNameUpper = "PROJECTS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsprojects"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Workitems.Curated
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'projects' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Curated/KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Curated/KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Curated/KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Curated/KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix


  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3",
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain
      }

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated\KBX.Analytics.DL.AzureDevOps.Workitems.Curated.Jobs\transform.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Task.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1800-01-01"
#date_partition_override = "ingest_date=1800-01-01"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-azuredevops-projects-raw-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-azuredevops-projects-structured-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'projects' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'projects'
DOMAIN = args.Domain
#DOMAIN = 'azuredevops'
JOB_NAME = args.JOB_NAME
#JOB_NAME = 'kbxt-dl-analytics-azuredevops-projects-structured-job-dev'


# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATE_DATE_PARTITION_TO_PROCESS_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition : {date_partition}')
job.logger().info(f, f'is_replay : {is_replay}')

#JSON_FROM_SCHEMA = '{"fields":[{"metadata":{},"name":"result","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"active","nullable":true,"type":"string"},{"metadata":{},"name":"activity_due","nullable":true,"type":"string"},{"metadata":{},"name":"additional_assignee_list","nullable":true,"type":"string"},{"metadata":{},"name":"agile_story","nullable":true,"type":"string"},{"metadata":{},"name":"approval","nullable":true,"type":"string"},{"metadata":{},"name":"approval_history","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"business_duration","nullable":true,"type":"string"},{"metadata":{},"name":"business_service","nullable":true,"type":"string"},{"metadata":{},"name":"calendar_duration","nullable":true,"type":"string"},{"metadata":{},"name":"close_notes","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_by","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci_business_app","nullable":true,"type":"string"},{"metadata":{},"name":"comments","nullable":true,"type":"string"},{"metadata":{},"name":"company","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"contract","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_display","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_id","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"due_date","nullable":true,"type":"string"},{"metadata":{},"name":"escalation","nullable":true,"type":"string"},{"metadata":{},"name":"expected_start","nullable":true,"type":"string"},{"metadata":{},"name":"follow_up","nullable":true,"type":"string"},{"metadata":{},"name":"group_list","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"string"},{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"order","nullable":true,"type":"string"},{"metadata":{},"name":"parent","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"string"},{"metadata":{},"name":"route_reason","nullable":true,"type":"string"},{"metadata":{},"name":"short_description","nullable":true,"type":"string"},{"metadata":{},"name":"skills","nullable":true,"type":"string"},{"metadata":{},"name":"sla_due","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_document","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_esignature_configuration","nullable":true,"type":"string"},{"metadata":{},"name":"state","nullable":true,"type":"string"},{"metadata":{},"name":"sys_class_name","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_on","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain_path","nullable":true,"type":"string"},{"metadata":{},"name":"sys_id","nullable":true,"type":"string"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"string"},{"metadata":{},"name":"sys_tags","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_on","nullable":true,"type":"string"},{"metadata":{},"name":"task_effective_number","nullable":true,"type":"string"},{"metadata":{},"name":"time_worked","nullable":true,"type":"string"},{"metadata":{},"name":"u_all_classes_configuration_items","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_escalate","nullable":true,"type":"string"},{"metadata":{},"name":"u_estimated_delivery_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_catalog_item","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_customer","nullable":true,"type":"string"},{"metadata":{},"name":"u_manual_routing","nullable":true,"type":"string"},{"metadata":{},"name":"u_new_hire","nullable":true,"type":"string"},{"metadata":{},"name":"u_start_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_support_tier","nullable":true,"type":"string"},{"metadata":{},"name":"universal_request","nullable":true,"type":"string"},{"metadata":{},"name":"upon_approval","nullable":true,"type":"string"},{"metadata":{},"name":"upon_reject","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"user_input","nullable":true,"type":"string"},{"metadata":{},"name":"watch_list","nullable":true,"type":"string"},{"metadata":{},"name":"work_end","nullable":true,"type":"string"},{"metadata":{},"name":"work_notes","nullable":true,"type":"string"},{"metadata":{},"name":"work_start","nullable":true,"type":"string"}],"type":"struct"},"type":"array"}}],"type":"struct"}'
#schemaFromJson = StructType.fromJson(json.loads(JSON_FROM_SCHEMA))

# functions

#Flatten array of structs and structs
job.logger().info(f, f'###################_TASK-2_UDF_###################')

def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df

def union_unmatched_columns(df1, df2) :
    for column in [column for column in df2.columns if column not in df1.columns]:
        df1 = df1.withColumn(column, F.lit(None))

    for column in [column for column in df1.columns if column not in df2.columns]:
        df2 = df2.withColumn(column, F.lit(None))
    
    return df1.unionByName(df2)

#%% last curated ingest date
job.logger().info(f, f'###################_TASK-3_LAST_STRUCTURED/CURATED_INGEST_DATE_###################')

try :
    df_structured = spark.sql(f'''
        select 
            * 
        from 
            {structured_db}.{tbl}
        where
            False
        ''')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = True
    job.logger().info(f, f'structured : {structured}')
    
    # Dropping "job_id", "source", "ingest_date"
    job.logger().info(f, f'Dropping Columns : "job_id", "source", "ingest_date"')
    df_structured = df_structured.drop("job_id", "source", "ingest_date")

except Exception as e:
    job.logger().info(f, e)
    df_structured = spark.createDataFrame([], '')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = False
    job.logger().info(f, f'structured : {structured}')


# raise Exception('Forced Exception')

#%%------------------------------------------Job Start------------------------------------------
try :
    # All files for a date partition that haven't been processed are 
    # returned, so be cognizent of the size of this dataframe.
    job.logger().info(f, f'###################_TASK-4_JOB_START_READ_DATAFRAME_###################')
    # raw file df
    # df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=JSON_FROM_SCHEMA)
    df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, file_format='json')
except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%%------------------------------------------Job Process------------------------------------------

try:    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-5_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])
        
        job.logger().info(f, f'raw_dataframe')
        df.printSchema()
        df.show()

        # flattening df
        projects = list(df.columns)
        for project in projects :
            if df.select(df[project]['value']).collect()[0][0] != [] :
                df_flatten = flatten(df.select(df[project]['value']).withColumnRenamed(f'{project}.value', project)).select(lit(f'{project}').alias("project_name"), "*")
                #df_structured.show(truncate=15)
                df_flatten = df_flatten.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_flatten.columns]).drop('empty_string_value')
                df_structured = union_unmatched_columns(df_structured, df_flatten)

        #df_str = df_structured.select([col(c).cast("string") for c in df_structured.columns])
        #df_str = df_structured.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_structured.columns])
        
        #df_structured = df_str
        
        job.logger().info(f, f'df_structured schema : after union_unmatched_columns')
        df_structured.printSchema()
        df_structured.show()
        
        df_structured.createOrReplaceTempView('raw_data')
        df_structured = spark.sql(f'''
            select
                '{JOB_ID}'                                                                                job_id,
                '{SOURCE}'                                                                                source,                                                          
                *
            from raw_data
        ''')
		
        df_transformed = df_structured

        job.logger().info(f, f'df_transformed')
        df_transformed.printSchema()
        df_transformed.show()
        job.logger().info(f, f'df_transformed.count() : {df_transformed.count()}')

        job.logger().info(f, f'###################_TASK-6_COMMIT_FILE_###################')
        # Commit files
        if not df_transformed.rdd.isEmpty() :
            job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))
        
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-7_REFRESH_PARTITION/RUN_CRAWLER_###################')
        df_table = spark.sql(f'''show tables in {structured_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
                job.logger().info(f, f"partion {date_partition.split('=')[1]} added to {tbl}")
            except Exception as e:
                job.logger().info(f, e)
        else :
            job.logger().info(f, f'initiating {crawler} run for first time')
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler {crawler} is {state.lower()}.")
                    state_previous = state
        
        
        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        job.logger().info(f, f'###################_TASK-8_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")
#%%------------------------------------------Job End------------------------------------------

job.runtime().end()

# %%
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Projects.Ingest"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Projects.Ingest"
$replacementEntityName = "PROJECTS"
$replacementEntityNameLower = "projects"
$replacementEntityNamePascal = "Projects"
$replacementEntityNameUpper = "PROJECTS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsprojects"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Projects.Ingest
============

## Introduction 

This solutions is reponsible for ingesting the data.  It has python scripts that are scheduled and ran with Glue to. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in ingest.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## ingest.py

- Writes data to the **bucket_target**/**prefix_target**/ingest_date=yyyy-MM-dd partition.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe ingest.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagetarget
  displayName: Target Stage
  default: Target ingest name, such as raw

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'projects' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Ingest.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Ingest.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Ingest deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  IngestStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  IngestJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/ingest.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  IngestCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "Standard"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--encryption-type": "sse-s3",
        "--prefix_target" : !Ref Entity,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain,
        "--History" : "False",
        "--max_record_per_request" : "500"
      }

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Jobs\ingest.py<content=>
#%%------------------------------------------ingest------------------------------------------

import os
import sys
import argparse
import requests
import json
import boto3
import botocore
from datetime import datetime
from datetime import timedelta
import math
import concurrent.futures as cf
import logging

# Timer
start_time = datetime.utcnow()
ingest_start_time = f'''{(start_time - timedelta(days=1)).strftime("%Y-%m-%d")}'''
ingest_end_time = f'''{start_time.strftime("%Y-%m-%d")}'''

# file
f = os.path.basename(__file__)

# secret
ado_user = ''
ado_personal_access_token = 'lxturceksjcxfqgqli5qjm57hit7xnuykkf7ig7kqswoz272okpq'

#%%------------------------------------------Init------------------------------------------

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
parser.add_argument('--max_record_per_request')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--History')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target
#bucket_target = "kbxt-dl-analytics-ado-workitems-raw-dev" #remove
prefix_target = args.prefix_target
#prefix_target = 'workitems' #remove
max_record_per_request = int(args.max_record_per_request)
#max_record_per_request = 1000 #remove
history = args.History
#history = 'True'
ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'workitems'
DOMAIN = args.Domain
#DOMAIN = 'ado'
OFFSET = 0
JOB_NAME = args.JOB_NAME

# KbxtDlPy logger
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

crawler = bucket_target.replace('-', '_')+'-crawler'

table = prefix_target

base_api_url = 'https://analytics.dev.azure.com'
organization = 'kbxltrans'
headers = {"Content-Type":"application/json","Accept":"application/json"}

if history == 'True' :
    job.logger().info(f, f"ingesting historic data")
    ingest_date = "1800-01-01"

else :
    job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")
    ingest_date = start_time.strftime('%Y-%m-%d')
    job.logger().info(f, f'ingest_start_time : {ingest_start_time}')
    job.logger().info(f, f'ingest_end_time : {ingest_end_time}')


job.logger().info(f, f'servicenow table to injest : {table}')
#job.logger().info(f, f'base_api_url : {base_api_url}')
job.logger().info(f, f'sysparm_limit : {max_record_per_request}')
#job.logger().info(f, f'params : {params}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'prefix_target : {prefix_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'crawler : {crawler}')

#variables

#determine s3 key
File = f.split('.')[0]
devtemplateprojectname = f"kbx.{PRODUCT}.dl.{DOMAIN}.{ENTITY}.{File}"

extract_date = f'ingest_date={ingest_date}'
extract_datetime = start_time.strftime("%Y%m%d%H%M%S%f")
filepath = f"{prefix_target}/{extract_date}/{devtemplateprojectname}+py+{extract_datetime}.json"
job.logger().info(f, f"object key={filepath}")

job.logger().info(f, f'###################_TASK-1_DEF_UDF_###################')
# User Defined Functions

def uploadFile(inputStream, filePath, bucketName):
    s3_resource = boto3.resource('s3')
    s3_client = boto3.client('s3')

    def isBucketExists():
        try:
            s3_resource.meta.client.head_bucket(Bucket=bucketName)
        except botocore.exceptions.ClientError as e:
            return False
        else :
            return True
    #logger  
    if (not isBucketExists()):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = s3_resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream)
    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
        job.logger().info(f, f"File uploaded at {filePath}")
        return True
    else :
        job.logger().info(f, f"Upload failed with HTTPStatusCode {res.get('HTTPStatusCode')}")
        return False

class Ingest() :
    def __init__(self, base_url, organization, project, table, max_record_per_request, userid, password, headers) -> None:
        self.userid = userid
        self.password = password
        self.url_count = '/'.join((base_url, organization, project, '_odata', 'v3.0', table))
        self.url_data = '/'.join((base_url, organization, project, '_odata', 'v3.0', table))
        self.max_record_per_request = max_record_per_request
        self.headers = headers

    def count_api_response(self, params) :
        response =  requests.get(url=self.url_count, auth=(self.userid, self.password), params=params, headers=self.headers)
        if response.json()['value'] != [] :
            countapiresponse = response.json()['value'][0]['Count']
        else :
            countapiresponse = 0
        self.countapiresponse = countapiresponse
        return countapiresponse
    
    def number_of_request(self, count_params) :
        numberofrequest = math.ceil(self.count_api_response(count_params)/self.max_record_per_request)
        if numberofrequest == 0 :
            numberofrequest = 1
        self.numberofrequest = numberofrequest
        return numberofrequest

    def get_data(self, data_params, count_params) :
        def send_request(params) :
            response = requests.get(url=self.url_data, params=params, auth=(self.userid, self.password), headers=self.headers)
            if response.status_code != 200 :
                job.logger().critical(f, f"api response status code : {response.status_code}")
                raise Exception(f"api response status code : {response.status_code}")
            else :
                job.logger().info(f, f"DATA COUNT {len(response.json()['value'])}")
                job.logger().info(f, f"api headers {response.headers}")
                return response

        params_params = []
        co = self.number_of_request(count_params)
        for n in range(co) :
            parameters = data_params.copy()
            parameters['$skip'] = int(self.max_record_per_request)*n
            params_params.append(parameters)

        with cf.ThreadPoolExecutor() as executor :
            results = executor.map(send_request, params_params)
        
        job.logger().info(f, f'##### All requests Response Received #####')

        job.logger().info(f, f'##### Validate initial_total_count of each response and append data #####')
        response = next(results)
        job.logger().info(f, f'requested_url : {response.url}')
        data = response.json()
        initial_total_count = self.countapiresponse
        job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['value'])}")
        job.logger().info(f, f"initial_total_count from parallel response: {initial_total_count}")
        job.logger().info(f, f"Total Data Appended : {len(data['value'])}")        

        for response in results :
            job.logger().info(f, f'requested_url : {response.url}')
            data['value'].extend(response.json()['value'])
            job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['value'])}")
            job.logger().info(f, f"initial_total_count from parallel response: {initial_total_count}")
            job.logger().info(f, f"Total Data Appended : {len(data['value'])}")
        
        if len(data['value']) != int(initial_total_count) :
            job.logger().info(f, f"SOME DATA MISSED IN SOME RESPONSE")
            raise Exception(f"ALL DATA NOT RECEIVED")
        else :
            job.logger().info(f, f"ALL DATA RECEIVED")

        self.data = data
        return data

#%%------------------------------------------Job Process ------------------------------------------
job.logger().info(f, f'###################_TASK-2_START_INGESTION_###################')

try:
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
    
    url_project = 'https://analytics.dev.azure.com/kbxltrans/_odata/v3.0/Projects?$Select=ProjectName'
    response =  requests.get(url=url_project, auth=(ado_user, ado_personal_access_token), headers=headers)
    projects = [project['ProjectName'] for project in response.json()['value']]

    if history == 'True' :
        data_params = {
            "$top" : f'{max_record_per_request}',
            "$skip" : OFFSET,
            "$orderby" : "AnalyticsUpdatedDate asc",
            "$count" : "true"
        }
        count_params={
            '$apply' : 'aggregate($count as Count)'
        }
    else :
        data_params = {
            "$filter" : f"date(AnalyticsUpdatedDate) ge {ingest_start_time} and date(AnalyticsUpdatedDate) lt {ingest_end_time}",
            "$top" : f'{max_record_per_request}',
            "$skip" : OFFSET,
            "$orderby" : "AnalyticsUpdatedDate asc",
            "$count" : "true"
        }

        count_params={
            '$apply' : f'filter(date(AnalyticsUpdatedDate) ge {ingest_start_time} and date(AnalyticsUpdatedDate) lt {ingest_end_time})/aggregate($count as Count)'
            }
    
    obj = {}
    data = {}
    for project in projects :
        print (project)
        o = Ingest(base_url=base_api_url, organization=organization, project=project, table=table, max_record_per_request=max_record_per_request, userid=ado_user, password=ado_personal_access_token, headers = headers)
        obj[project] = {'object' : o, 'data' : o.get_data(data_params=data_params, count_params=count_params)}
        data[project] = o.data

    #raise Exception("Force Exception")
    
    input_stream = bytes(json.dumps(data).encode('UTF-8'))
    if uploadFile(input_stream, filepath, bucket_target) :
        job.logger().info(f, f"File s3a://{bucket_target}/{filepath} uploaded successfully")
        job.logger().info(f, f'###################_TASK-3_JOB_RUN_SUCCESSFULL_###################')
    else :
        raise Exception(f"Upload Failed")

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().info(f, f'###################_TASK-3_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

# %%<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Workitems.Ingest"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Workitems.Ingest"
$replacementEntityName = "WORKITEMS"
$replacementEntityNameLower = "workitems"
$replacementEntityNamePascal = "Workitems"
$replacementEntityNameUpper = "WORKITEMS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsworkitems"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Workitems.Ingest
============

## Introduction 

This solutions is reponsible for ingesting the data.  It has python scripts that are scheduled and ran with Glue to. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in ingest.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## ingest.py

- Writes data to the **bucket_target**/**prefix_target**/ingest_date=yyyy-MM-dd partition.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe ingest.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagetarget
  displayName: Target Stage
  default: Target ingest name, such as raw

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'workitems' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Ingest.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Ingest.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Ingest deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  IngestStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  IngestJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/ingest.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  IngestCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "Standard"
      NumberOfWorkers: 15
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--encryption-type": "sse-s3",
        "--prefix_target" : !Ref Entity,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain,
        "--History" : "False",
        "--max_record_per_request" : "500"
      }

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\Ingestion\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Jobs\ingest.py<content=>
#%%------------------------------------------ingest------------------------------------------

import os
import sys
import argparse
import requests
import json
import boto3
import botocore
from datetime import datetime
from datetime import timedelta
import math
import concurrent.futures as cf
import logging

# Timer
start_time = datetime.utcnow()
ingest_start_time = f'''{(start_time - timedelta(days=1)).strftime("%Y-%m-%d")}'''
ingest_end_time = f'''{start_time.strftime("%Y-%m-%d")}'''

# file
f = os.path.basename(__file__)

# secret
ado_user = ''
ado_personal_access_token = 'lxturceksjcxfqgqli5qjm57hit7xnuykkf7ig7kqswoz272okpq'

#%%------------------------------------------Init------------------------------------------

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
parser.add_argument('--max_record_per_request')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--History')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target
#bucket_target = "kbxt-dl-analytics-ado-workitems-raw-dev" #remove
prefix_target = args.prefix_target
#prefix_target = 'workitems' #remove
max_record_per_request = int(args.max_record_per_request)
#max_record_per_request = 1000 #remove
history = args.History
#history = 'True'
ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'workitems'
DOMAIN = args.Domain
#DOMAIN = 'ado'
OFFSET = 0
JOB_NAME = args.JOB_NAME

# KbxtDlPy logger
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

crawler = bucket_target.replace('-', '_')+'-crawler'

table = prefix_target

base_api_url = 'https://analytics.dev.azure.com'
organization = 'kbxltrans'
headers = {"Content-Type":"application/json","Accept":"application/json"}

if history == 'True' :
    job.logger().info(f, f"ingesting historic data")
    ingest_date = "1800-01-01"

else :
    job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")
    ingest_date = start_time.strftime('%Y-%m-%d')
    job.logger().info(f, f'ingest_start_time : {ingest_start_time}')
    job.logger().info(f, f'ingest_end_time : {ingest_end_time}')


job.logger().info(f, f'servicenow table to injest : {table}')
#job.logger().info(f, f'base_api_url : {base_api_url}')
job.logger().info(f, f'sysparm_limit : {max_record_per_request}')
#job.logger().info(f, f'params : {params}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'prefix_target : {prefix_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'crawler : {crawler}')

#variables

#determine s3 key
File = f.split('.')[0]
devtemplateprojectname = f"kbx.{PRODUCT}.dl.{DOMAIN}.{ENTITY}.{File}"

extract_date = f'ingest_date={ingest_date}'
extract_datetime = start_time.strftime("%Y%m%d%H%M%S%f")
filepath = f"{prefix_target}/{extract_date}/{devtemplateprojectname}+py+{extract_datetime}.json"
job.logger().info(f, f"object key={filepath}")

job.logger().info(f, f'###################_TASK-1_DEF_UDF_###################')
# User Defined Functions

def uploadFile(inputStream, filePath, bucketName):
    s3_resource = boto3.resource('s3')
    s3_client = boto3.client('s3')

    def isBucketExists():
        try:
            s3_resource.meta.client.head_bucket(Bucket=bucketName)
        except botocore.exceptions.ClientError as e:
            return False
        else :
            return True
    #logger  
    if (not isBucketExists()):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = s3_resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream)
    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
        job.logger().info(f, f"File uploaded at {filePath}")
        return True
    else :
        job.logger().info(f, f"Upload failed with HTTPStatusCode {res.get('HTTPStatusCode')}")
        return False

class Ingest() :
    def __init__(self, base_url, organization, project, table, max_record_per_request, userid, password, headers) -> None:
        self.userid = userid
        self.password = password
        self.url_count = '/'.join((base_url, organization, project, '_odata', 'v3.0', table))
        self.url_data = '/'.join((base_url, organization, project, '_odata', 'v3.0', table))
        self.max_record_per_request = max_record_per_request
        self.headers = headers

    def count_api_response(self, params) :
        response =  requests.get(url=self.url_count, auth=(self.userid, self.password), params=params, headers=self.headers)
        if response.json()['value'] != [] :
            countapiresponse = response.json()['value'][0]['Count']
        else :
            countapiresponse = 0
        self.countapiresponse = countapiresponse
        return countapiresponse
    
    def number_of_request(self, count_params) :
        numberofrequest = math.ceil(self.count_api_response(count_params)/self.max_record_per_request)
        if numberofrequest == 0 :
            numberofrequest = 1
        self.numberofrequest = numberofrequest
        return numberofrequest

    def get_data(self, data_params, count_params) :
        def send_request(params) :
            response = requests.get(url=self.url_data, params=params, auth=(self.userid, self.password), headers=self.headers)
            if response.status_code != 200 :
                job.logger().critical(f, f"api response status code : {response.status_code}")
                raise Exception(f"api response status code : {response.status_code}")
            else :
                job.logger().info(f, f"DATA COUNT {len(response.json()['value'])}")
                job.logger().info(f, f"api headers {response.headers}")
                return response

        params_params = []
        co = self.number_of_request(count_params)
        for n in range(co) :
            parameters = data_params.copy()
            parameters['$skip'] = int(self.max_record_per_request)*n
            params_params.append(parameters)

        with cf.ThreadPoolExecutor() as executor :
            results = executor.map(send_request, params_params)
        
        job.logger().info(f, f'##### All requests Response Received #####')

        job.logger().info(f, f'##### Validate initial_total_count of each response and append data #####')
        response = next(results)
        job.logger().info(f, f'requested_url : {response.url}')
        data = response.json()
        initial_total_count = self.countapiresponse
        job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['value'])}")
        job.logger().info(f, f"initial_total_count from parallel response: {initial_total_count}")
        job.logger().info(f, f"Total Data Appended : {len(data['value'])}")        

        for response in results :
            job.logger().info(f, f'requested_url : {response.url}')
            data['value'].extend(response.json()['value'])
            job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['value'])}")
            job.logger().info(f, f"initial_total_count from parallel response: {initial_total_count}")
            job.logger().info(f, f"Total Data Appended : {len(data['value'])}")
        
        if len(data['value']) != int(initial_total_count) :
            job.logger().info(f, f"SOME DATA MISSED IN SOME RESPONSE")
            raise Exception(f"ALL DATA NOT RECEIVED")
        else :
            job.logger().info(f, f"ALL DATA RECEIVED")

        self.data = data
        return data

#%%------------------------------------------Job Process ------------------------------------------
job.logger().info(f, f'###################_TASK-2_START_INGESTION_###################')

try:
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
    
    url_project = 'https://analytics.dev.azure.com/kbxltrans/_odata/v3.0/Projects?$Select=ProjectName'
    response =  requests.get(url=url_project, auth=(ado_user, ado_personal_access_token), headers=headers)
    projects = [project['ProjectName'] for project in response.json()['value']]

    if history == 'True' :
        data_params = {
            "$top" : f'{max_record_per_request}',
            "$skip" : OFFSET,
            "$orderby" : "AnalyticsUpdatedDate asc",
            "$count" : "true"
        }
        count_params={
            '$apply' : 'aggregate($count as Count)'
        }
    else :
        data_params = {
            "$filter" : f"date(AnalyticsUpdatedDate) ge {ingest_start_time} and date(AnalyticsUpdatedDate) lt {ingest_end_time}",
            "$top" : f'{max_record_per_request}',
            "$skip" : OFFSET,
            "$orderby" : "AnalyticsUpdatedDate asc",
            "$count" : "true"
        }

        count_params={
            '$apply' : f'filter(date(AnalyticsUpdatedDate) ge {ingest_start_time} and date(AnalyticsUpdatedDate) lt {ingest_end_time})/aggregate($count as Count)'
            }
    
    obj = {}
    data = {}
    for project in projects :
        print (project)
        o = Ingest(base_url=base_api_url, organization=organization, project=project, table=table, max_record_per_request=max_record_per_request, userid=ado_user, password=ado_personal_access_token, headers = headers)
        obj[project] = {'object' : o, 'data' : o.get_data(data_params=data_params, count_params=count_params)}
        data[project] = o.data

    #raise Exception("Force Exception")
    
    input_stream = bytes(json.dumps(data).encode('UTF-8'))
    if uploadFile(input_stream, filepath, bucket_target) :
        job.logger().info(f, f"File s3a://{bucket_target}/{filepath} uploaded successfully")
        job.logger().info(f, f'###################_TASK-3_JOB_RUN_SUCCESSFULL_###################')
    else :
        raise Exception(f"Upload Failed")

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().info(f, f'###################_TASK-3_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

# %%<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\.dockerignore<content=>
.git
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-airflow-$(environment)' 
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: 'deployments/$(Build.Repository.Name)/'
 
stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**/!(*_example_*)'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Copy
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)dags'
        globExpressions: '**/!(*_example_*)'
        targetFolder: 'dags'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'


<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\docker-compose.yml<content=>
version: '3.7'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"

    webserver:
        image: puckel/docker-airflow:latest
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            # Encryption
            # Can be static, it's only used for local development
            - FERNET_KEY=lUDB1r1SNvYy4kZAhA-4z8PqC0x8UQfYmo4uFP8UFcg=
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ~/.aws:/root/.aws:ro
            - ./dags:/usr/local/airflow/dags
            - ./plugins:/usr/local/airflow/plugins
            - ./requirements.txt:/requirements.txt
        ports:
            - "8080:8080"
        build: .
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\Dockerfile<content=>
# VERSION 1.10.9
# AUTHOR: Matthieu "Puckel_" Roisil
# DESCRIPTION: Basic Airflow container
# BUILD: docker build --rm -t puckel/docker-airflow .
# SOURCE: https://github.com/puckel/docker-airflow

FROM python:3.7-slim-buster
LABEL maintainer="KBX"

# Never prompt the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux

# Airflow
ARG AIRFLOW_VERSION=1.10.9
ARG AIRFLOW_USER_HOME=/usr/local/airflow
ARG AIRFLOW_DEPS=""
ARG PYTHON_DEPS=""
ENV AIRFLOW_HOME=${AIRFLOW_USER_HOME}

# Define en_US.
ENV LANGUAGE en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LC_ALL en_US.UTF-8
ENV LC_CTYPE en_US.UTF-8
ENV LC_MESSAGES en_US.UTF-8

# Disable noisy "Handling signal" log messages:
# ENV GUNICORN_CMD_ARGS --log-level WARNING

RUN set -ex \
    && buildDeps=' \
        freetds-dev \
        libkrb5-dev \
        libsasl2-dev \
        libssl-dev \
        libffi-dev \
        libpq-dev \
        git \
    ' \
    && apt-get update -yqq \
    && apt-get upgrade -yqq \
    && apt-get install -yqq --no-install-recommends \
        $buildDeps \
        freetds-bin \
        build-essential \
        default-libmysqlclient-dev \
        apt-utils \
        curl \
        rsync \
        netcat \
        locales \
    && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \
    && locale-gen \
    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \
    && useradd -ms /bin/bash -d ${AIRFLOW_USER_HOME} airflow \
    && pip install -U pip setuptools wheel \
    && pip install pytz \
    && pip install pyOpenSSL \
    && pip install ndg-httpsclient \
    && pip install pyasn1 \
    && pip install apache-airflow[crypto,celery,postgres,hive,jdbc,mysql,ssh${AIRFLOW_DEPS:+,}${AIRFLOW_DEPS}]==${AIRFLOW_VERSION} \
    && pip install 'redis==3.2' \
    && if [ -n "${PYTHON_DEPS}" ]; then pip install ${PYTHON_DEPS}; fi \
    && apt-get purge --auto-remove -yqq $buildDeps \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf \
        /var/lib/apt/lists/* \
        /tmp/* \
        /var/tmp/* \
        /usr/share/man \
        /usr/share/doc \
        /usr/share/doc-base

COPY script/entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_USER_HOME}/airflow.cfg

RUN chown -R airflow: ${AIRFLOW_USER_HOME}

EXPOSE 8080 5555 8793

RUN pip install apache-airflow-backport-providers-amazon \
 && pip install apache-airflow-backport-providers-apache-spark \
 && pip install apache-airflow-backport-providers-jdbc \
 && pip install apache-airflow-backport-providers-datadog \
 && pip install apache-airflow-backport-providers-postgres \
 && pip install sqlalchemy==1.3.13

USER airflow
WORKDIR ${AIRFLOW_USER_HOME}
ENTRYPOINT ["/entrypoint.sh"]
CMD ["webserver"]
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\LICENSE<content=>
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2017 Matthieu "Puckel_" Roisil

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\package-lock.json<content=>
{
  "name": "KBX.DL.CodeTemplates.Airflow",
  "lockfileVersion": 2,
  "requires": true,
  "packages": {}
}
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\package.json<content=>
{}
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\README.md<content=>
# docker-airflow

This repository contains **Dockerfile** of [apache-airflow](https://github.com/apache/incubator-airflow) for [Docker](https://www.docker.com/)'s [automated build](https://registry.hub.docker.com/u/puckel/docker-airflow/) published to the public [Docker Hub Registry](https://registry.hub.docker.com/).

## Information

* Based on Python (3.7-slim-buster) official Image [python:3.7-slim-buster](https://hub.docker.com/_/python/) and uses the official [Postgres](https://hub.docker.com/_/postgres/) as backend and [Redis](https://hub.docker.com/_/redis/) as queue
* Install [Docker](https://www.docker.com/)
* Install [Docker Compose](https://docs.docker.com/compose/install/)
* This follows the Airflow release from this page [Python Package Index](https://pypi.python.org/pypi/apache-airflow)

## Installation

This solution has been created by a code template.

## Build

Set any necessary environment variables in the docker-compose.yml file

* LOAD_EX can be set to "n" if you do not want to deploy the sample DAGs
* AWS credentials should be picked up by the mounted profile drive

Load AWS Credentials
``` Powershell
 kochid aws refresh
```

Compose and start a container from the docker-compose.yml file:

* docker-compose -f docker-compose.yml up

A docker container will now be visible and manageable in Docker Desktop.
The Airflow Web UI can be accessed at: [localhost:8080](http://localhost:8080/)

## Configuration

* Use the variable template DAG to set Airflow variables which your other DAGs require.  Please refer to the [yourprojectname]_variables.py template DAG
* Use the connection template DAG to set Airflow connections; connections should be named [yourprojectname]_[resource]; for example, kbxdlcodetemplatesworkflow_aws for a project named KBX.DL.CodeTemplates.Workflow.  Please refer to the [yourprojectname]_connections.py template DAG

## Management

Stop and remove the container along with any associated resources created via the aforementioned "up" command:

* docker-compose -f docker-compose.yml down

List running containers:

* docker ps

Access the shell of a running container as the default user:

* docker exec -ti "container name" bash

Access the shell of a running container as root:

* docker exec -u 0 -ti "container name" bash

## Pitfalls

* **Connections and Variables** are not persisted between containers unless you include them as environment variables in the docker-compose YAML file.
* Manually refreshing Airflow Web UI pages excessively will cause the instance to become temporarily unresponsive
* Ensure that the "dag" parameter is set for all operators to avoid vague error messages
* A DAG must be in the "On" state to be triggered by schedule or manually
* DAGs which leverage dynamic operator generation based on the results of a query will execute that query every time the "DAG bag" is filled or the DAG is accessed if the query exists within the same DAG.  The query which the dynamic operator generation relies on should exist in a separate DAG (scheduled as per requirements) where the results are written to an Airflow variable; this variable will then be referenced in the DAG containing dynamic operator generation
* Implementation of sub-DAGs requires explicitly setting the "schedule_interval" and "start_date" parameters of the parent DAG instead of passing them as part of the "default_args" parameter to avoid vague error messages
* Provider packages expose operators, hooks, and sensors but only the operators provide "out-of-the-box" functionality; resort to wiring up the hook and sensor only if an operator is unavailable or cannot meet a specific requirement
* For an unscheduled DAG either do not reference the "schedule_interval" property or set it to None (the type, not a string); for example, schedule_interval=None
* Setting the "start_date" parameter of any DAG to the current or a future date/time will cause the scheduler to fail; jobs will look as though they have started but not execute any tasks and be stuck in a "running" state
* **start_date** is counterintuitive but by design; for example, a job scheduled hourly and starting at 1400h will actually execute at 1500h.  All times in airflow are UTC and so don't execute at your local time.  Avoid using datetime.now() to offset this. Please see https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag and https://marclamberti.com/blog/apache-airflow-best-practices-1/ for further information

## References

* puckel Airflow Docker image documentation - https://hub.docker.com/r/puckel/docker-airflow
* Airflow core concepts - https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html
* Airflow best practices - https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html
* Airflow lesser-known tips and tricks - https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\requirements.txt<content=>
# KBX does not support the automation of this file, create a help request
wtforms==2.3.3
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\config\airflow.cfg<content=>
[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Users must supply an Airflow connection id that provides access to the storage
# location.
remote_log_conn_id =
remote_base_log_folder =
encrypt_s3_logs = False

# Logging level
logging_level = INFO

# Logging level for Flask-appbuilder UI
fab_logging_level = WARN

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
# Example: logging_config_class = my.path.default_local_settings.LOGGING_CONFIG
logging_config_class =

# Flag to enable/disable Colored logs in Console
# Colour the logs when the controlling terminal is a TTY.
colored_console_log = True

# Log format for when Colored logs is enabled
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d}} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Format of Log line
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Name of handler to read task instance logs.
# Default to use task handler.
task_log_reader = task

# Hostname by providing a path to a callable, which will resolve the hostname.
# The format is "package:function".
#
# For example, default value "socket:getfqdn" means that result from getfqdn() of "socket"
# package will be used as hostname.
#
# No argument should be required in the function specified.
# If using IP address as hostname is preferred, use value ``airflow.utils.net:get_host_ip_address``
hostname_callable = socket:getfqdn

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = utc

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
# sql_alchemy_conn = sqlite:////tmp/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool. 0 indicates no limit.
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
# When the number of checked-out connections reaches the size set in pool_size,
# additional connections will be returned up to this limit.
# When those additional connections are returned to the pool, they are disconnected and discarded.
# It follows then that the total number of simultaneous connections the pool will allow
# is pool_size + max_overflow,
# and the total number of "sleeping" connections the pool will allow is pool_size.
# max_overflow can be set to -1 to indicate no overflow limit;
# no limit will be placed on the total number of concurrent connections. Defaults to 10.
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite. If the number of DB connections is ever exceeded,
# a lower config value will allow the system to recover faster.
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout.
# Typically, this is a simple statement like "SELECT 1".
# More information here:
# https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic
sql_alchemy_pool_pre_ping = True

# The schema to use for the metadata database.
# SqlAlchemy supports databases with the concept of multiple schemas.
sql_alchemy_schema =

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = True

# Where your Airflow plugins are stored
plugins_folder = /usr/local/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = $FERNET_KEY

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a ``run_as_user`` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
default_impersonation =

# What security module to use (for example kerberos)
security =

# If set to False enables some unsecure features like Charts and Ad Hoc Queries.
# In 2.0 will default to True.
secure_mode = False

# Turn unit test mode on (overwrites many configuration options with test
# values at runtime)
unit_test_mode = False

# Whether to enable pickling for xcom (note that this is insecure and allows for
# RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
enable_xcom_pickling = True

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

# Whether to override params with dag_run.conf. If you pass some key-value pairs
# through ``airflow dags backfill -c`` or
# ``airflow dags trigger -c``, the key-value pairs will override the existing ones in params.
dag_run_conf_overrides_params = False

# Worker initialisation check to validate Metadata Database connection
worker_precheck = False

# When discovering DAGs, ignore any files that don't contain the strings ``DAG`` and ``airflow``.
dag_discovery_safe_mode = True

# The number of retries each task is going to have by default. Can be overridden at dag or task level.
default_task_retries = 0

# Whether to serialises DAGs and persist them in DB.
# If set to True, Webserver reads from DB instead of parsing DAG files
# More details: https://airflow.apache.org/docs/stable/dag-serialization.html
store_serialized_dags = False

# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate.
min_serialized_dag_update_interval = 30

# On each dagrun check against defined SLAs
check_slas = True

[cli]
# In what way should the cli access the API. The LocalClient will use the
# database directly, while the json_client will use the api running on the
# webserver
api_client = airflow.api.client.local_client

# If you set web_server_url_prefix, do NOT forget to append it here, ex:
# ``endpoint_url = http://localhost:8080/myroot``
# So api will look like: ``http://localhost:8080/myroot/api/experimental/...``
endpoint_url = http://localhost:8080

[debug]
# Used only with DebugExecutor. If set to True DAG will fail with first
# failed task. Helpful for debugging purposes.
fail_fast = False

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[lineage]
# what lineage backend to use
backend =

[atlas]
sasl_enabled = False
host =
port = 21000
username =
password =

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via ``default_args``
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[hive]
# Default mapreduce queue for HiveOperator tasks
default_hive_mapred_queue =

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_key =

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# Secret key used to run your flask app
# It should be as random as possible
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use. Choices include
# sync (default), eventlet, gevent
worker_class = sync

# Log files for the gunicorn webserver. '-' means log to stderr.
access_logfile = -

# Log files for the gunicorn webserver. '-' means log to stderr.
error_logfile = -

# Expose the configuration file in the web server
expose_config = True

# Expose hostname in the web server
expose_hostname = True

# Expose stacktrace in the web server
expose_stacktrace = True

# Set to true to turn on authentication:
# https://airflow.apache.org/security.html#web-authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode. Choices include user (default) and ldapgroup.
# Ldap group filtering requires using the ldap backend
#
# Note that the ldap server needs the "memberOf" overlay to be set up
# in order to user the ldapgroup mode.
owner_mode = user

# Default DAG view. Valid values are:
# tree, graph, duration, gantt, landing_times
dag_default_view = tree

# "Default DAG orientation. Valid values are:"
# LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Puts the webserver in demonstration mode; blurs the names of Operators for
# privacy.
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
# while fetching logs from other worker machine
log_fetch_timeout_sec = 5

# Time interval (in secs) to wait before next log fetching.
log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing.
log_auto_tailing_offset = 30

# Animation speed for auto tailing log display.
log_animation_speed = 1000

# By default, the webserver shows paused DAGs. Flip this to hide paused
# DAGs by default
hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
page_size = 100

# Use FAB-based webserver with RBAC feature
rbac = False

# Define the color of navigation bar
navbar_color = #007A87

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
enable_proxy_fix = False

# Number of values to trust for ``X-Forwarded-For``.
# More info: https://werkzeug.palletsprojects.com/en/0.16.x/middleware/proxy_fix/
proxy_fix_x_for = 1

# Number of values to trust for ``X-Forwarded-Proto``
proxy_fix_x_proto = 1

# Number of values to trust for ``X-Forwarded-Host``
proxy_fix_x_host = 1

# Number of values to trust for ``X-Forwarded-Port``
proxy_fix_x_port = 1

# Number of values to trust for ``X-Forwarded-Prefix``
proxy_fix_x_prefix = 1

# Set secure flag on session cookie
cookie_secure = False

# Set samesite policy on session cookie
cookie_samesite =

# Default setting for wrap toggle on DAG code and TI log views.
default_wrap = False

# Allow the UI to be rendered in a frame
x_frame_enabled = True

# Send anonymous user activity to your analytics tool
# choose from google_analytics, segment, or metarouter
# analytics_tool =

# Unique ID of your account in the analytics tool
# analytics_id =

# Update FAB permissions and sync security manager roles
# on webserver startup
update_fab_perms = True

# Minutes of non-activity before logged out from UI
# 0 means never get forcibly logged out
force_log_out_after = 0

# The UI cookie lifetime in days
session_lifetime_days = 30

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]

# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
# smtp_user =
# Example: smtp_password = airflow
# smtp_password =
smtp_port = 25
smtp_mail_from = airflow@example.com

[sentry]

# Sentry (https://docs.sentry.io) integration
sentry_dsn =

[celery]

# This section only applies if you are using the CeleryExecutor in
# ``[core]`` section above
# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# The maximum and minimum concurrency that will be used when starting workers with the
# ``airflow celery worker`` command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Example: worker_autoscale = 16,12
worker_autoscale = 16,12

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://redis:6379/1

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it ``airflow flower``. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# The root URL for Flower
# Example: flower_url_prefix = /flower
flower_url_prefix =

# This defines the port that Celery Flower runs on
flower_port = 5555

# Securing Flower with Basic Authentication
# Accepts user:password pairs separated by a comma
# Example: flower_basic_auth = user1:password1,user2:password2
flower_basic_auth =

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default

# How many processes CeleryExecutor uses to sync task state.
# 0 means to use max(1, number of cores - 1) processes.
sync_parallelism = 0

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# In case of using SSL
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

# Celery Pool implementation.
# Choices include: prefork (default), eventlet, gevent or solo.
# See:
# https://docs.celeryproject.org/en/latest/userguide/workers.html#concurrency
# https://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html
pool = prefork

# The number of seconds to wait before timing out ``send_task_to_executor`` or
# ``fetch_celery_task_state`` operations.
operation_timeout = 2

[celery_broker_transport_options]

# This section is for specifying options which can be passed to the
# underlying celery broker transport. See:
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_transport_options
# The visibility timeout defines the number of seconds to wait for the worker
# to acknowledge the task before the message is redelivered to another worker.
# Make sure to increase the visibility timeout to match the time of the longest
# ETA you're planning to use.
# visibility_timeout is only supported for Redis and SQS celery brokers.
# See:
# http://docs.celeryproject.org/en/master/userguide/configuration.html#std:setting-broker_transport_options
# Example: visibility_timeout = 21600
# visibility_timeout =

[dask]

# This section only applies if you are using the DaskExecutor in
# [core] section above
# The IP address and port of the Dask cluster's scheduler.
cluster_address = 127.0.0.1:8786

# TLS/ SSL settings to access a secured Dask scheduler.
tls_ca =
tls_cert =
tls_key =

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# After how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# The number of times to try to schedule each DAG file
# -1 indicates unlimited number
num_runs = -1

# The number of seconds to wait between consecutive DAG file processing
processor_poll_interval = 1

# after how much time (seconds) a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
dag_dir_list_interval = 300

# How often should stats be printed to the logs. Setting to 0 will disable printing stats
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
# This is used by the health check in the "/health" endpoint
scheduler_health_check_threshold = 30
child_process_log_directory = /usr/local/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is False,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
catchup_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# If this is too high, SQL query performance may be impacted by one
# or more of the following:
# - reversion to full table scan
# - complexity of query predicate
# - excessive locking
# Additionally, you may hit the maximum allowable query length for your db.
# Set this to 0 for no limit (not advised)
max_tis_per_query = 512

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# If you want to avoid send all the available metrics to StatsD,
# you can configure an allow list of prefixes to send only the metrics that
# start with the elements of the list (e.g: scheduler,executor,dagrun)
statsd_allow_list =

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
max_threads = 2
authenticate = False

# Turn off scheduler use of cron intervals by setting this to False.
# DAGs submitted manually in the web UI or with trigger_dag will still run.
use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
# Only has effect if schedule_interval is set to None in DAG
allow_trigger_in_future = False

[ldap]
# set this to ldaps://<your.ldap.server>:<port>
uri =
user_filter = objectClass=*
user_name_attr = uid
group_member_attr = memberOf
superuser_filter =
data_profiler_filter =
bind_user = cn=Manager,dc=example,dc=com
bind_password = insecure
basedn = dc=example,dc=com
cacert = /etc/ca/ldap_ca.crt
search_scope = LEVEL

# This setting allows the use of LDAP servers that either return a
# broken schema, or do not return a schema.
ignore_malformed_schema = False

[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

# Number of cpu cores required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_cpu = 1

# Memory in MB required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_memory = 256

# Enable framework checkpointing for mesos
# See http://mesos.apache.org/documentation/latest/slave-recovery/
checkpoint = False

# Failover timeout in milliseconds.
# When checkpointing is enabled and this option is set, Mesos waits
# until the configured timeout for
# the MesosExecutor framework to re-register after a failover. Mesos
# shuts down running tasks if the
# MesosExecutor framework fails to re-register within this timeframe.
# Example: failover_timeout = 604800
# failover_timeout =

# Enable framework authentication for mesos
# See http://mesos.apache.org/documentation/latest/configuration/
authenticate = False

# Mesos credentials, if authentication is enabled
# Example: default_principal = admin
# default_principal =
# Example: default_secret = admin
# default_secret =

# Optional Docker Image to run on slave before running the command
# This image should be accessible from mesos slave i.e mesos slave
# should be able to pull this docker image before executing the command.
# Example: docker_image_slave = puckel/docker-airflow
# docker_image_slave =

[kerberos]
ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
api_rev = v3

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

[elasticsearch]
# Elasticsearch host
host =

# Format of the log_id, which is used to query for a given tasks logs
log_id_template = {{dag_id}}-{{task_id}}-{{execution_date}}-{{try_number}}

# Used to mark the end of a log stream for a task
end_of_log_mark = end_of_log

# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
# Code will construct log_id using the log_id template from the argument above.
# NOTE: The code will prefix the https:// automatically, don't include that here.
frontend =

# Write the task logs to the stdout of the worker, rather than the default files
write_stdout = False

# Instead of the default log formatter, write the log lines as JSON
json_format = False

# Log fields to also attach to the json output, if enabled
json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
use_ssl = False
verify_certs = True

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository =
worker_container_tag =
worker_container_image_pull_policy = IfNotPresent

# If True (default), worker pods will be deleted upon termination
delete_worker_pods = True

# Number of Kubernetes Worker Pod creation calls per scheduler loop
worker_pods_creation_batch_size = 1

# The Kubernetes namespace where airflow workers should be created. Defaults to ``default``
namespace = default

# The name of the Kubernetes ConfigMap containing the Airflow Configuration (this file)
# Example: airflow_configmap = airflow-configmap
airflow_configmap =

# The name of the Kubernetes ConfigMap containing ``airflow_local_settings.py`` file.
#
# For example:
#
# ``airflow_local_settings_configmap = "airflow-configmap"`` if you have the following ConfigMap.
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     airflow_local_settings.py: |
#         def pod_mutation_hook(pod):
#             ...
#     airflow.cfg: |
#         ...
# Example: airflow_local_settings_configmap = airflow-configmap
airflow_local_settings_configmap =

# For docker image already contains DAGs, this is set to ``True``, and the worker will
# search for dags in dags_folder,
# otherwise use git sync or dags volume claim to mount DAGs
dags_in_image = False

# For either git sync or volume mounted DAGs, the worker will look in this subpath for DAGs
dags_volume_subpath =

# For DAGs mounted via a volume claim (mutually exclusive with git-sync and host path)
dags_volume_claim =

# For volume mounted logs, the worker will look in this subpath for logs
logs_volume_subpath =

# A shared volume claim for the logs
logs_volume_claim =

# For DAGs mounted via a hostPath volume (mutually exclusive with volume claim and git-sync)
# Useful in local environment, discouraged in production
dags_volume_host =

# A hostPath volume for the logs
# Useful in local environment, discouraged in production
logs_volume_host =

# A list of configMapsRefs to envFrom. If more than one configMap is
# specified, provide a comma separated list: configmap_a,configmap_b
env_from_configmap_ref =

# A list of secretRefs to envFrom. If more than one secret is
# specified, provide a comma separated list: secret_a,secret_b
env_from_secret_ref =

# Git credentials and repository for DAGs mounted via Git (mutually exclusive with volume claim)
git_repo =
git_branch =
git_subpath =

# The specific rev or hash the git_sync init container will checkout
# This becomes GIT_SYNC_REV environment variable in the git_sync init container for worker pods
git_sync_rev =

# Use git_user and git_password for user authentication or git_ssh_key_secret_name
# and git_ssh_key_secret_key for SSH authentication
git_user =
git_password =
git_sync_root = /git
git_sync_dest = repo

# Mount point of the volume if git-sync is being used.
# i.e. /usr/local/airflow/dags
git_dags_folder_mount_point =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-secrets.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: airflow-secrets
#   data:
#     # key needs to be gitSshKey
#     gitSshKey: <base64_encoded_data>
# Example: git_ssh_key_secret_name = airflow-secrets
git_ssh_key_secret_name =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     known_hosts: |
#         github.com ssh-rsa <...>
#     airflow.cfg: |
#         ...
# Example: git_ssh_known_hosts_configmap_name = airflow-configmap
git_ssh_known_hosts_configmap_name =

# To give the git_sync init container credentials via a secret, create a secret
# with two fields: GIT_SYNC_USERNAME and GIT_SYNC_PASSWORD (example below) and
# add ``git_sync_credentials_secret = <secret_name>`` to your airflow config under the
# ``kubernetes`` section
#
# Secret Example:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: git-credentials
#   data:
#     GIT_SYNC_USERNAME: <base64_encoded_git_username>
#     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
git_sync_credentials_secret =

# For cloning DAGs from git repositories into volumes: https://github.com/kubernetes/git-sync
git_sync_container_repository = k8s.gcr.io/git-sync
git_sync_container_tag = v3.1.1
git_sync_init_container_name = git-sync-clone
git_sync_run_as_user = 65533

# The name of the Kubernetes service account to be associated with airflow workers, if any.
# Service accounts are required for workers that require access to secrets or cluster resources.
# See the Kubernetes RBAC documentation for more:
# https://kubernetes.io/docs/admin/authorization/rbac/
worker_service_account_name =

# Any image pull secrets to be given to worker pods, If more than one secret is
# required, provide a comma separated list: secret_a,secret_b
image_pull_secrets =

# GCP Service Account Keys to be provided to tasks run on Kubernetes Executors
# Should be supplied in the format: key-name-1:key-path-1,key-name-2:key-path-2
gcp_service_account_keys =

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has.
# cluster_context =
# config_file =

# Affinity configuration as a single line formatted JSON object.
# See the affinity model for top-level key names (e.g. ``nodeAffinity``, etc.):
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#affinity-v1-core
affinity =

# A list of toleration objects as a single line formatted JSON array
# See:
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#toleration-v1-core
tolerations =

# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis.
# See:
# https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
# Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely
# for kubernetes api responses, which will cause the scheduler to hang.
# The timeout is specified as [connect timeout, read timeout]
kube_client_request_args = {{"_request_timeout" : [60,60] }}

# Specifies the uid to run the first process of the worker pods containers as
run_as_user =

# Specifies a gid to associate with all containers in the worker pods
# if using a git_ssh_key_secret_name use an fs_group
# that allows for the key to be read, e.g. 65533
fs_group =

[kubernetes_node_selectors]

# The Key-value pairs to be given to worker pods.
# The worker pods will be scheduled to the nodes of the specified key-value pairs.
# Should be supplied in the format: key = value

[kubernetes_annotations]

# The Key-value annotations pairs to be given to worker pods.
# Should be supplied in the format: key = value

[kubernetes_environment_variables]

# The scheduler sets the following environment variables into your workers. You may define as
# many environment variables as needed and the kubernetes launcher will set them in the launched workers.
# Environment variables in this section are defined as follows
# ``<environment_variable_key> = <environment_variable_value>``
#
# For example if you wanted to set an environment variable with value `prod` and key
# ``ENVIRONMENT`` you would follow the following format:
# ENVIRONMENT = prod
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_secrets]

# The scheduler mounts the following secrets into your workers as they are launched by the
# scheduler. You may define as many secrets as needed and the kubernetes launcher will parse the
# defined secrets and mount them as secret environment variables in the launched workers.
# Secrets in this section are defined as follows
# ``<environment_variable_mount> = <kubernetes_secret_object>=<kubernetes_secret_key>``
#
# For example if you wanted to mount a kubernetes secret key named ``postgres_password`` from the
# kubernetes secret object ``airflow-secret`` as the environment variable ``POSTGRES_PASSWORD`` into
# your workers you would follow the following format:
# ``POSTGRES_PASSWORD = airflow-secret=postgres_credentials``
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_labels]

# The Key-value pairs to be given to worker pods.
# The worker pods will be given these static labels, as well as some additional dynamic labels
# to identify the task.
# Should be supplied in the format: ``key = value``
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\dags\kbxanalyticsdlazuredevops_airflow.py<content=>
from os import getenv
# from tkinter import Variable
import airflow
from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.operators.glue import AwsGlueJobOperator
from ast import literal_eval
from airflow.models import Variable

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals

# Same as File Name
DAG_ID = 'kbxanalyticsdlazuredevops_airflow'
# When it should be turned on, not execution date.
START_DATE = airflow.utils.dates.days_ago(1)
# How often to Run. @daily - Once a day at Midnight
SCHEDULE_INTERVAL = '30 06 * * *'
# Who is listed as the owner of this DAG in the Airflow Web Server
DAG_OWNER_NAME = "KBX"
# List of email address to send email alerts to if this job fails

AWS_CONN_ID = 'kbxanalyticsdlazuredevops_aws'
ENV = Variable.get('kbxanalyticsdlazuredevops_environment')

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': START_DATE,
    'catchup': False
}

with DAG(
    DAG_ID, 
    schedule_interval=SCHEDULE_INTERVAL,
    default_args=default_args
    ) as dag:

    po_workitems_raw_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-workitems-raw-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-workitems-raw-job-{ENV}",
        dag=dag,
        retries = 1,
        aws_conn_id=AWS_CONN_ID
    )

    po_workitems_structured_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-workitems-structured-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-workitems-structured-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    # po_workitems_curated_job = AwsGlueJobOperator(
        # task_id=f"execute_kbxt-dl-analytics-azuredevops-workitems-curated-job-{ENV}", 
        # job_name=f"kbxt-dl-analytics-azuredevops-workitems-curated-job-{ENV}",
        # dag=dag,
        # retries = 0,
        # aws_conn_id=AWS_CONN_ID
    # )

    po_projects_raw_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-projects-raw-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-projects-raw-job-{ENV}",
        dag=dag,
        retries = 1,
        aws_conn_id=AWS_CONN_ID
    )

    po_projects_structured_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-projects-structured-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-projects-structured-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    # po_projects_curated_job = AwsGlueJobOperator(
        # task_id=f"execute_kbxt-dl-analytics-azuredevops-projects-curated-job-{ENV}", 
        # job_name=f"kbxt-dl-analytics-azuredevops-projects-curated-job-{ENV}",
        # dag=dag,
        # retries = 0,
        # aws_conn_id=AWS_CONN_ID
    # )

    po_workitems_raw_job >> po_workitems_structured_job

    po_projects_raw_job >> po_projects_structured_job
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\dags\kbxanalyticsdlazuredevops_connections.py<content=>
from os import getenv
from airflow import DAG, settings
from airflow.models import Connection
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator

def ListConnections():
    return settings.Session().query(Connection)

def CreateConnections():
    try:
        # Build a connection object:
        conn = Connection(
            # conn_id: Name of the connection as displayed in the Airflow UI.
			# Snake-case; prefix with the product name.
            conn_id="kbxanalyticsdlazuredevops_aws",
            # conn_type: The type of connection to create.
            # Valid conn_type values are: "azure_cosmos", "azure_data_lake", "cassandra", "cloudant", 
            # "docker", "gcpcloudsql", "google_cloud_platform", "grpc", "hive_cli", "hiveserver2", 
            # "jdbc", "jira", "mongo", "mssql", "mysql", "oracle", "pig_cli", "postgres", "presto",
            # "redis", "sqlite", "vertica", "wasb".
            conn_type="", 
            # host: Endpoint at which the resource exists; URL, IP address, etc.
            host="", 
            login="", 
            # Leave the password property value as-is; this will be updated via the Airflow UI.
            password="ChangeMeLater",
            # port: The port to use when creating a database type connection.
            # port=1234,
            # schema: The schema to use when creating a database type connection.
            # schema="schema"
            # extra: Used to specify additional connection type specific settings.
            # Refer to https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html for more details.
            # extra="json-formatted string"
        )

        # Get the current Airflow session:
        session = settings.Session()
        # Add the connection to the session if it doesn't already exist:
        if(conn.conn_id not in ListConnections()):
            session.add(conn)
            # Commit the newly-created connection:
            session.commit()
            return True
            
        return False
    except:
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_connections', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_connections",
        python_callable=CreateConnections
    )

    po
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\dags\kbxanalyticsdlazuredevops_variables.py<content=>
from os import getenv
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator
from airflow.models import Variable

def SetVariables():
    try:
        # Multiple variables may be set via multiple Variable.set() calls.
        # This variale DAG can be run once manually for static values or leverage
        # schedules to set variable values based on the results of other operations.
        Variable.set("kbxanalyticsdlazuredevops_environment", 'value/values')
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_variables', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_variables",
        python_callable=SetVariables
    )

    po
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_get_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable
from ast import literal_eval

def GetVariable():
    print("Performing Variable.get() action...")
    keys=literal_eval(Variable.get("kbxanalyticsdlazuredevops_variable"))
    print(keys)

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
    #'schedule_interval': '0/3 * * * ?'
}

with DAG("kbxanalyticsdlazuredevops_example_get_variable", default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="get_variable",
        provide_context=False,
        python_callable=GetVariable
    )

    po
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_glue_test.py<content=>
from os import getenv

from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator

def ListGlueJobs():
    try:
        gh = AwsGlueJobHook(aws_conn_id="aws_lg_nonprod")
        print(gh.list_jobs())
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_example_glue_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_glue_jobs",
        provide_context=False,
        python_callable=ListGlueJobs
    )

    po
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_s3_conn_test.py<content=>
"""
S3 Connection Test
"""
from airflow import DAG
from airflow.hooks.base_hook import BaseHook
from airflow.operators.python_operator import PythonOperator
import boto3
from datetime import *

def ListBuckets():
    try:
        s3Conn=boto3.client("s3")
        res = s3Conn.list_buckets()
        for bkt in res["Buckets"]:
            print(bkt["Name"])

        if(len(res["Buckets"]) > 0):
            return True
        return False
    except :
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_example_s3_conn_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_s3_buckets",
        provide_context=False,
        python_callable=ListBuckets
    )

    po
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_set_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable

def SetVariable():
    keys=["kbxanalyticsdlazuredevops_0","1","2","3","4","5"]
    print("Performing Variable.get() action...")
    Variable.set("kbxanalyticsdlazuredevops_variable", keys)
    
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG("kbxanalyticsdlazuredevops_example_set_variable", default_args=default_args) as dag:
    po = PythonOperator(task_id="set_variable",
        provide_context=False,
        python_callable=SetVariable
    )

    po
<path=>ADO\Development\Orchestration\KBX.Analytics.DL.AzureDevOps.Workflow\script\entrypoint.sh<content=>
#!/usr/bin/env bash

# User-provided configuration must always be respected.
#
# Therefore, this script must only derives Airflow AIRFLOW__ variables from other variables
# when the user did not provide their own configuration.

TRY_LOOP="20"

# Global defaults and back-compat
: "${AIRFLOW_HOME:="/usr/local/airflow"}"
: "${AIRFLOW__CORE__FERNET_KEY:=${FERNET_KEY:=$(python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)")}}"
: "${AIRFLOW__CORE__EXECUTOR:=${EXECUTOR:-Sequential}Executor}"

# Load DAGs examples (default: Yes)
if [[ -z "$AIRFLOW__CORE__LOAD_EXAMPLES" && "${LOAD_EX:=n}" == n ]]; then
  AIRFLOW__CORE__LOAD_EXAMPLES=False
fi

export \
  AIRFLOW_HOME \
  AIRFLOW__CORE__EXECUTOR \
  AIRFLOW__CORE__FERNET_KEY \
  AIRFLOW__CORE__LOAD_EXAMPLES \

# Install custom python package if requirements.txt is present
if [ -e "/requirements.txt" ]; then
    $(command -v pip) install --user -r /requirements.txt
fi

wait_for_port() {
  local name="$1" host="$2" port="$3"
  local j=0
  while ! nc -z "$host" "$port" >/dev/null 2>&1 < /dev/null; do
    j=$((j+1))
    if [ $j -ge $TRY_LOOP ]; then
      echo >&2 "$(date) - $host:$port still not reachable, giving up"
      exit 1
    fi
    echo "$(date) - waiting for $name... $j/$TRY_LOOP"
    sleep 5
  done
}

# Other executors than SequentialExecutor drive the need for an SQL database, here PostgreSQL is used
if [ "$AIRFLOW__CORE__EXECUTOR" != "SequentialExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the database
  if [ -z "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" ]; then
    # Default values corresponding to the default compose files
    : "${POSTGRES_HOST:="postgres"}"
    : "${POSTGRES_PORT:="5432"}"
    : "${POSTGRES_USER:="airflow"}"
    : "${POSTGRES_PASSWORD:="airflow"}"
    : "${POSTGRES_DB:="airflow"}"
    : "${POSTGRES_EXTRAS:-""}"

    AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
    export AIRFLOW__CORE__SQL_ALCHEMY_CONN

    # Check if the user has provided explicit Airflow configuration for the broker's connection to the database
    if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
      AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
      export AIRFLOW__CELERY__RESULT_BACKEND
    fi
  else
    if [[ "$AIRFLOW__CORE__EXECUTOR" == "CeleryExecutor" && -z "$AIRFLOW__CELERY__RESULT_BACKEND" ]]; then
      >&2 printf '%s\n' "FATAL: if you set AIRFLOW__CORE__SQL_ALCHEMY_CONN manually with CeleryExecutor you must also set AIRFLOW__CELERY__RESULT_BACKEND"
      exit 1
    fi

    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    POSTGRES_ENDPOINT=$(echo -n "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" | cut -d '/' -f3 | sed -e 's,.*@,,')
    POSTGRES_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    POSTGRES_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Postgres" "$POSTGRES_HOST" "$POSTGRES_PORT"
fi

# CeleryExecutor drives the need for a Celery broker, here Redis is used
if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the broker
  if [ -z "$AIRFLOW__CELERY__BROKER_URL" ]; then
    # Default values corresponding to the default compose files
    : "${REDIS_PROTO:="redis://"}"
    : "${REDIS_HOST:="redis"}"
    : "${REDIS_PORT:="6379"}"
    : "${REDIS_PASSWORD:=""}"
    : "${REDIS_DBNUM:="1"}"

    # When Redis is secured by basic auth, it does not handle the username part of basic auth, only a token
    if [ -n "$REDIS_PASSWORD" ]; then
      REDIS_PREFIX=":${REDIS_PASSWORD}@"
    else
      REDIS_PREFIX=
    fi

    AIRFLOW__CELERY__BROKER_URL="${REDIS_PROTO}${REDIS_PREFIX}${REDIS_HOST}:${REDIS_PORT}/${REDIS_DBNUM}"
    export AIRFLOW__CELERY__BROKER_URL
  else
    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    REDIS_ENDPOINT=$(echo -n "$AIRFLOW__CELERY__BROKER_URL" | cut -d '/' -f3 | sed -e 's,.*@,,')
    REDIS_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    REDIS_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Redis" "$REDIS_HOST" "$REDIS_PORT"
fi

case "$1" in
  webserver)
    airflow initdb
    if [ "$AIRFLOW__CORE__EXECUTOR" = "LocalExecutor" ] || [ "$AIRFLOW__CORE__EXECUTOR" = "SequentialExecutor" ]; then
      # With the "Local" and "Sequential" executors it should all run in one container.
      airflow scheduler &
    fi
    exec airflow webserver
    ;;
  worker|scheduler)
    # Give the webserver time to run initdb.
    sleep 10
    exec airflow "$@"
    ;;
  flower)
    sleep 10
    exec airflow "$@"
    ;;
  version)
    exec airflow "$@"
    ;;
  *)
    # The command is something like bash, not an airflow subcommand. Just run it in the right environment.
    exec "$@"
    ;;
esac
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Projects.Structured"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Projects.Structured"
$replacementEntityName = "PROJECTS"
$replacementEntityNameLower = "projects"
$replacementEntityNamePascal = "Projects"
$replacementEntityNameUpper = "PROJECTS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsprojects"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Projects.Structured
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'projects' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix


  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3",
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain
      }

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Jobs\transform.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Task.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1800-01-01"
#date_partition_override = "ingest_date=1800-01-01"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-azuredevops-projects-raw-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-azuredevops-projects-structured-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'projects' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'projects'
DOMAIN = args.Domain
#DOMAIN = 'azuredevops'
JOB_NAME = args.JOB_NAME
#JOB_NAME = 'kbxt-dl-analytics-azuredevops-projects-structured-job-dev'


# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATE_DATE_PARTITION_TO_PROCESS_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition : {date_partition}')
job.logger().info(f, f'is_replay : {is_replay}')

#JSON_FROM_SCHEMA = '{"fields":[{"metadata":{},"name":"result","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"active","nullable":true,"type":"string"},{"metadata":{},"name":"activity_due","nullable":true,"type":"string"},{"metadata":{},"name":"additional_assignee_list","nullable":true,"type":"string"},{"metadata":{},"name":"agile_story","nullable":true,"type":"string"},{"metadata":{},"name":"approval","nullable":true,"type":"string"},{"metadata":{},"name":"approval_history","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"business_duration","nullable":true,"type":"string"},{"metadata":{},"name":"business_service","nullable":true,"type":"string"},{"metadata":{},"name":"calendar_duration","nullable":true,"type":"string"},{"metadata":{},"name":"close_notes","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_by","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci_business_app","nullable":true,"type":"string"},{"metadata":{},"name":"comments","nullable":true,"type":"string"},{"metadata":{},"name":"company","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"contract","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_display","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_id","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"due_date","nullable":true,"type":"string"},{"metadata":{},"name":"escalation","nullable":true,"type":"string"},{"metadata":{},"name":"expected_start","nullable":true,"type":"string"},{"metadata":{},"name":"follow_up","nullable":true,"type":"string"},{"metadata":{},"name":"group_list","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"string"},{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"order","nullable":true,"type":"string"},{"metadata":{},"name":"parent","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"string"},{"metadata":{},"name":"route_reason","nullable":true,"type":"string"},{"metadata":{},"name":"short_description","nullable":true,"type":"string"},{"metadata":{},"name":"skills","nullable":true,"type":"string"},{"metadata":{},"name":"sla_due","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_document","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_esignature_configuration","nullable":true,"type":"string"},{"metadata":{},"name":"state","nullable":true,"type":"string"},{"metadata":{},"name":"sys_class_name","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_on","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain_path","nullable":true,"type":"string"},{"metadata":{},"name":"sys_id","nullable":true,"type":"string"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"string"},{"metadata":{},"name":"sys_tags","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_on","nullable":true,"type":"string"},{"metadata":{},"name":"task_effective_number","nullable":true,"type":"string"},{"metadata":{},"name":"time_worked","nullable":true,"type":"string"},{"metadata":{},"name":"u_all_classes_configuration_items","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_escalate","nullable":true,"type":"string"},{"metadata":{},"name":"u_estimated_delivery_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_catalog_item","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_customer","nullable":true,"type":"string"},{"metadata":{},"name":"u_manual_routing","nullable":true,"type":"string"},{"metadata":{},"name":"u_new_hire","nullable":true,"type":"string"},{"metadata":{},"name":"u_start_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_support_tier","nullable":true,"type":"string"},{"metadata":{},"name":"universal_request","nullable":true,"type":"string"},{"metadata":{},"name":"upon_approval","nullable":true,"type":"string"},{"metadata":{},"name":"upon_reject","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"user_input","nullable":true,"type":"string"},{"metadata":{},"name":"watch_list","nullable":true,"type":"string"},{"metadata":{},"name":"work_end","nullable":true,"type":"string"},{"metadata":{},"name":"work_notes","nullable":true,"type":"string"},{"metadata":{},"name":"work_start","nullable":true,"type":"string"}],"type":"struct"},"type":"array"}}],"type":"struct"}'
#schemaFromJson = StructType.fromJson(json.loads(JSON_FROM_SCHEMA))

# functions

#Flatten array of structs and structs
job.logger().info(f, f'###################_TASK-2_UDF_###################')

def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df

def union_unmatched_columns(df1, df2) :
    for column in [column for column in df2.columns if column not in df1.columns]:
        df1 = df1.withColumn(column, F.lit(None))

    for column in [column for column in df1.columns if column not in df2.columns]:
        df2 = df2.withColumn(column, F.lit(None))
    
    return df1.unionByName(df2)

#%% last curated ingest date
job.logger().info(f, f'###################_TASK-3_LAST_STRUCTURED/CURATED_INGEST_DATE_###################')

try :
    df_structured = spark.sql(f'''
        select 
            * 
        from 
            {structured_db}.{tbl}
        where
            False
        ''')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = True
    job.logger().info(f, f'structured : {structured}')
    
    # Dropping "job_id", "source", "ingest_date"
    job.logger().info(f, f'Dropping Columns : "job_id", "source", "ingest_date"')
    df_structured = df_structured.drop("job_id", "source", "ingest_date")

except Exception as e:
    job.logger().info(f, e)
    df_structured = spark.createDataFrame([], '')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = False
    job.logger().info(f, f'structured : {structured}')


# raise Exception('Forced Exception')

#%%------------------------------------------Job Start------------------------------------------
try :
    # All files for a date partition that haven't been processed are 
    # returned, so be cognizent of the size of this dataframe.
    job.logger().info(f, f'###################_TASK-4_JOB_START_READ_DATAFRAME_###################')
    # raw file df
    # df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=JSON_FROM_SCHEMA)
    df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, file_format='json')
except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%%------------------------------------------Job Process------------------------------------------

try:    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-5_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])
        
        job.logger().info(f, f'raw_dataframe')
        df.printSchema()
        df.show()

        # flattening df
        projects = list(df.columns)
        for project in projects :
            if df.select(df[project]['value']).collect()[0][0] != [] :
                df_flatten = flatten(df.select(df[project]['value']).withColumnRenamed(f'{project}.value', project)).select(lit(f'{project}').alias("project_name"), "*")
                #df_structured.show(truncate=15)
                df_flatten = df_flatten.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_flatten.columns]).drop('empty_string_value')
                df_structured = union_unmatched_columns(df_structured, df_flatten)

        #df_str = df_structured.select([col(c).cast("string") for c in df_structured.columns])
        #df_str = df_structured.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_structured.columns])
        
        #df_structured = df_str
        
        job.logger().info(f, f'df_structured schema : after union_unmatched_columns')
        df_structured.printSchema()
        df_structured.show()
        
        df_structured.createOrReplaceTempView('raw_data')
        df_structured = spark.sql(f'''
            select
                '{JOB_ID}'                                                                                job_id,
                '{SOURCE}'                                                                                source,                                                          
                *
            from raw_data
        ''')
		
        df_transformed = df_structured

        job.logger().info(f, f'df_transformed')
        df_transformed.printSchema()
        df_transformed.show()
        job.logger().info(f, f'df_transformed.count() : {df_transformed.count()}')

        job.logger().info(f, f'###################_TASK-6_COMMIT_FILE_###################')
        # Commit files
        if not df_transformed.rdd.isEmpty() :
            job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))
        
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-7_REFRESH_PARTITION/RUN_CRAWLER_###################')
        df_table = spark.sql(f'''show tables in {structured_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
                job.logger().info(f, f"partion {date_partition.split('=')[1]} added to {tbl}")
            except Exception as e:
                job.logger().info(f, e)
        else :
            job.logger().info(f, f'initiating {crawler} run for first time')
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler {crawler} is {state.lower()}.")
                    state_previous = state
        
        
        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        job.logger().info(f, f'###################_TASK-8_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")
#%%------------------------------------------Job End------------------------------------------

job.runtime().end()

# %%
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Workitems.Structured"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Workitems.Structured"
$replacementEntityName = "WORKITEMS"
$replacementEntityNameLower = "workitems"
$replacementEntityNamePascal = "Workitems"
$replacementEntityNameUpper = "WORKITEMS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsworkitems"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Workitems.Structured
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'workitems' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Ref AdditionalPythonModules,
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3",
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain
      }

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Jobs\transform.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Task.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1800-01-01"
#date_partition_override = "ingest_date=1800-01-01"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-azuredevops-workitems-raw-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-azuredevops-workitems-structured-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'workitems' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'workitems'
DOMAIN = args.Domain
#DOMAIN = 'azuredevops'
JOB_NAME = args.JOB_NAME
#JOB_NAME = 'kbxt-dl-analytics-azuredevops-workitems-structured-job-dev'


# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATE_DATE_PARTITION_TO_PROCESS_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition : {date_partition}')
job.logger().info(f, f'is_replay : {is_replay}')

#JSON_FROM_SCHEMA = '{"fields":[{"metadata":{},"name":"result","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"active","nullable":true,"type":"string"},{"metadata":{},"name":"activity_due","nullable":true,"type":"string"},{"metadata":{},"name":"additional_assignee_list","nullable":true,"type":"string"},{"metadata":{},"name":"agile_story","nullable":true,"type":"string"},{"metadata":{},"name":"approval","nullable":true,"type":"string"},{"metadata":{},"name":"approval_history","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"business_duration","nullable":true,"type":"string"},{"metadata":{},"name":"business_service","nullable":true,"type":"string"},{"metadata":{},"name":"calendar_duration","nullable":true,"type":"string"},{"metadata":{},"name":"close_notes","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_by","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci_business_app","nullable":true,"type":"string"},{"metadata":{},"name":"comments","nullable":true,"type":"string"},{"metadata":{},"name":"company","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"contract","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_display","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_id","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"due_date","nullable":true,"type":"string"},{"metadata":{},"name":"escalation","nullable":true,"type":"string"},{"metadata":{},"name":"expected_start","nullable":true,"type":"string"},{"metadata":{},"name":"follow_up","nullable":true,"type":"string"},{"metadata":{},"name":"group_list","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"string"},{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"order","nullable":true,"type":"string"},{"metadata":{},"name":"parent","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"string"},{"metadata":{},"name":"route_reason","nullable":true,"type":"string"},{"metadata":{},"name":"short_description","nullable":true,"type":"string"},{"metadata":{},"name":"skills","nullable":true,"type":"string"},{"metadata":{},"name":"sla_due","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_document","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_esignature_configuration","nullable":true,"type":"string"},{"metadata":{},"name":"state","nullable":true,"type":"string"},{"metadata":{},"name":"sys_class_name","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_on","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain_path","nullable":true,"type":"string"},{"metadata":{},"name":"sys_id","nullable":true,"type":"string"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"string"},{"metadata":{},"name":"sys_tags","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_on","nullable":true,"type":"string"},{"metadata":{},"name":"task_effective_number","nullable":true,"type":"string"},{"metadata":{},"name":"time_worked","nullable":true,"type":"string"},{"metadata":{},"name":"u_all_classes_configuration_items","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_escalate","nullable":true,"type":"string"},{"metadata":{},"name":"u_estimated_delivery_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_catalog_item","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_customer","nullable":true,"type":"string"},{"metadata":{},"name":"u_manual_routing","nullable":true,"type":"string"},{"metadata":{},"name":"u_new_hire","nullable":true,"type":"string"},{"metadata":{},"name":"u_start_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_support_tier","nullable":true,"type":"string"},{"metadata":{},"name":"universal_request","nullable":true,"type":"string"},{"metadata":{},"name":"upon_approval","nullable":true,"type":"string"},{"metadata":{},"name":"upon_reject","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"user_input","nullable":true,"type":"string"},{"metadata":{},"name":"watch_list","nullable":true,"type":"string"},{"metadata":{},"name":"work_end","nullable":true,"type":"string"},{"metadata":{},"name":"work_notes","nullable":true,"type":"string"},{"metadata":{},"name":"work_start","nullable":true,"type":"string"}],"type":"struct"},"type":"array"}}],"type":"struct"}'
#schemaFromJson = StructType.fromJson(json.loads(JSON_FROM_SCHEMA))

# functions

#Flatten array of structs and structs
job.logger().info(f, f'###################_TASK-2_UDF_###################')

def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df

def union_unmatched_columns(df1, df2) :
    for column in [column for column in df2.columns if column not in df1.columns]:
        df1 = df1.withColumn(column, F.lit(''))

    for column in [column for column in df1.columns if column not in df2.columns]:
        df2 = df2.withColumn(column, F.lit(''))
    
    return df1.unionByName(df2)

#%% last curated ingest date
job.logger().info(f, f'###################_TASK-3_LAST_STRUCTURED/CURATED_INGEST_DATE_###################')

try :
    df_structured = spark.sql(f'''
        select 
            * 
        from 
            {structured_db}.{tbl}
        where
            False
        ''')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = True
    job.logger().info(f, f'structured : {structured}')
    
    # Dropping "job_id", "source", "ingest_date"
    job.logger().info(f, f'Dropping Columns : "job_id", "source", "ingest_date"')
    df_structured = df_structured.drop("job_id", "source", "ingest_date")

except Exception as e:
    job.logger().info(f, e)
    df_structured = spark.createDataFrame([], '')
    job.logger().info(f, f'last_structured_schema')
    df_structured.printSchema()
    job.logger().info(f, f'number of columns in df_structured : {len(df_structured.columns)}')
    structured = False
    job.logger().info(f, f'structured : {structured}')


# raise Exception('Forced Exception')

#%%------------------------------------------Job Start------------------------------------------
try :
    # All files for a date partition that haven't been processed are 
    # returned, so be cognizent of the size of this dataframe.
    job.logger().info(f, f'###################_TASK-4_JOB_START_READ_DATAFRAME_###################')
    # raw file df
    # df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=JSON_FROM_SCHEMA)
    df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, file_format='json')
except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%%------------------------------------------Job Process------------------------------------------

try:    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-5_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])
        
        job.logger().info(f, f'raw_dataframe')
        df.printSchema()
        df.show()

        # flattening df
        projects = list(df.columns)
        for project in projects :
            if df.select(df[project]['value']).collect()[0][0] != [] :
                df_flatten = flatten(df.select(df[project]['value']).withColumnRenamed(f'{project}.value', project)).select(lit(f'{project}').alias("project_name"), "*")
                #df_structured.show(truncate=15)
                df_flatten = df_flatten.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_flatten.columns]).drop('empty_string_value')
                df_structured = union_unmatched_columns(df_structured, df_flatten)

        #df_str = df_structured.select([col(c).cast("string") for c in df_structured.columns])
        #df_str = df_structured.withColumn('empty_string_value', F.lit('')).selectExpr([f"coalesce(trim(cast({cl} as string)), empty_string_value) as {cl}" for cl in df_structured.columns])
        
        #df_structured = df_str
        
        job.logger().info(f, f'df_structured schema : after union_unmatched_columns')
        df_structured.printSchema()
        df_structured.show()
        
        df_structured.createOrReplaceTempView('raw_data')
        df_structured = spark.sql(f'''
            select
                '{JOB_ID}'                                                                                job_id,
                '{SOURCE}'                                                                                source,                                                          
                *
            from raw_data
        ''')
		
        df_transformed = df_structured

        job.logger().info(f, f'df_transformed')
        df_transformed.printSchema()
        df_transformed.show()
        job.logger().info(f, f'df_transformed.count() : {df_transformed.count()}')

        job.logger().info(f, f'###################_TASK-6_COMMIT_FILE_###################')
        # Commit files
        if not df_transformed.rdd.isEmpty() :
            job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))
        
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-7_REFRESH_PARTITION/RUN_CRAWLER_###################')
        df_table = spark.sql(f'''show tables in {structured_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
                job.logger().info(f, f"partion {date_partition.split('=')[1]} added to {tbl}")
            except Exception as e:
                job.logger().info(f, e)
        else :
            job.logger().info(f, f'initiating {crawler} run for first time')
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler {crawler} is {state.lower()}.")
                    state_previous = state
        
        
        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        job.logger().info(f, f'###################_TASK-8_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")
#%%------------------------------------------Job End------------------------------------------

job.runtime().end()

# %%
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Projects.Ingest"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Projects.Ingest"
$replacementEntityName = "PROJECTS"
$replacementEntityNameLower = "projects"
$replacementEntityNamePascal = "Projects"
$replacementEntityNameUpper = "PROJECTS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsprojects"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Projects.Ingest
============

## Introduction 

This solutions is reponsible for ingesting the data.  It has python scripts that are scheduled and ran with Glue to. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in ingest.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## ingest.py

- Writes data to the **bucket_target**/**prefix_target**/ingest_date=yyyy-MM-dd partition.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe ingest.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagetarget
  displayName: Target Stage
  default: Target ingest name, such as raw

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'projects' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Ingest.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Ingest.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Ingest deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Ingest/KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  IngestStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  IngestJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/ingest.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        PythonLibPath: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity

  TransformTriggerScheduledStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/TriggerScheduled.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        Schedule: "cron(0 */4 * * ? *)"   

  IngestCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 3
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Ref AdditionalPythonModules,
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--prefix_target" : !Ref Entity,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3"
      }

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Ingest\KBX.Analytics.DL.AzureDevOps.Projects.Ingest.Jobs\ingest.py<content=>
#%% ingest
import os
import sys
from datetime import datetime
import argparse

# Timer
start_time = datetime.now()

# file
f = os.path.basename(__file__)

#%% Init
from KbxtDlPy.Harness import Job
job = Job(name="ingest", level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-raw-dev"
prefix_target = args.prefix_target # ex:"projects"

# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)

#%% Job Process 
try:    
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))

    ##### YOUR CODE START #####

    # Write JSON data to "[bucket_target_path]/[prefix_target]/ingest_date=yyyy-MM-dd/kbxanalyticsdlazuredevopsprojects.ingest+py+yyyyMMddHHmmssfffffff.json partition format with sequential file names.
    # Partition string format: datetime.now().strftime("ingest_date=%Y-%m-%d")
    # File name string format: datetime.now().strftime("%Y%m%d%H%M%S%f")
    # ex: s3://kbxt-dl-analytics-servicenow-task-raw-dev/task/ingest_date=2022-06-22/kbxanalyticsdlservicenowtask.ingest+py+202203231800278283056.json

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

# %%
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Projects.Structured"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Projects.Structured"
$replacementEntityName = "PROJECTS"
$replacementEntityNameLower = "projects"
$replacementEntityNamePascal = "Projects"
$replacementEntityNameUpper = "PROJECTS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsprojects"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Projects.Structured
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'projects' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Projects.Structured/KBX.Analytics.DL.AzureDevOps.Projects.Structured.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity

  TransformTriggerScheduledStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/TriggerScheduled.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        Schedule: "cron(*/10 * * * ? *)"   

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 3
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Ref AdditionalPythonModules,
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        #"--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3"
      }

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Projects.Structured\KBX.Analytics.DL.AzureDevOps.Projects.Structured.Jobs\transform.py<content=>
#%% transform
import os
import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import argparse

# Timer
start_time = datetime.now()

# file
f = os.path.basename(__file__)

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.AzureDevOps.Projects.Structured") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%% Init
from KbxtDlPy.Harness import Job #For local testing, add lib. here and in harness.py as a prefix
job = Job(name="transform", level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--schema_json', nargs='?', const='', type=str, default='')
parser.add_argument('--file_format', nargs='?', const='', type=str, default='json')
parser.add_argument('--filename_prefix', nargs='?', const='', type=str, default='')
parser.add_argument('--includes_header', dest="includesHeader", action="store_true")
parser.set_defaults(includesHeader=False)

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-raw-dev"
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-structured-dev"
prefix_source = args.prefix_source # ex:"deventity"
schema_json = args.schema_json
file_format = args.file_format # ex:"parquet","csv","json"
filename_prefix = args.filename_prefix # ex:the prefix you want to provide to the part file name. Should be all lowercase and without any spaces or special characters.
includes_header = args.includesHeader

# Prefix of files to process, in case files need to be excluded
file_prefix = "" # ex:"part-"

# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")    
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

#%% Job Start
# All files for a date partition that haven't been processed are 
# returned, so be cognizent of the size of this dataframe.
# json_schema parameter is optional; the dataframe schema is inferred when this parameter is not supplied.
df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, file_format=file_format, path=prefix_source, schema_json=schema_json, includes_header=includes_header)

#%% Job Process 
try:    
    if (df is not None):
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])  


        ##### YOUR CODE START #####

        df_transformed = df # Dataframe to write

        ##### YOUR CODE END #####


        # Commit files
        job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition), filename_prefix)

        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df.count()))

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

#%% Job End
job.runtime().end()

# %%
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\.dockerignore<content=>
.git
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\.gitignore<content=>
### Vim ###
[._]*.s[a-w][a-z]
[._]s[a-w][a-z]
*.un~
Session.vim
.netrwhist
*~

### SublimeText ###
# cache files for sublime text
*.tmlanguage.cache
*.tmPreferences.cache
*.stTheme.cache

# workspace files are user-specific
*.sublime-workspace

# project files should be checked into the repository, unless a significant
# proportion of contributors will probably not be using SublimeText
# *.sublime-project

# sftp configuration file
sftp-config.json

# Python
__pycache__
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'productnametemplatereplace' # This can be hard-coded since the solution is named per product
  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: ''$(prefix)-$(product)-airflow-$(environment)' 
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: 'deployments/$(Build.Repository.Name)/'
 
stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**/!(*_example_*)'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Copy
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)dags'
        globExpressions: '**/!(*_example_*)'
        targetFolder: 'dags'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'


<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\docker-compose.yml<content=>
version: '3.7'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"

    webserver:
        image: puckel/docker-airflow:latest
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            # Encryption
            # Can be static, it's only used for local development
            - FERNET_KEY=lUDB1r1SNvYy4kZAhA-4z8PqC0x8UQfYmo4uFP8UFcg=
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ~/.aws:/root/.aws:ro
            - ./dags:/usr/local/airflow/dags
            - ./plugins:/usr/local/airflow/plugins
            - ./requirements.txt:/requirements.txt
        ports:
            - "8080:8080"
        build: .
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\Dockerfile<content=>
# VERSION 1.10.9
# AUTHOR: Matthieu "Puckel_" Roisil
# DESCRIPTION: Basic Airflow container
# BUILD: docker build --rm -t puckel/docker-airflow .
# SOURCE: https://github.com/puckel/docker-airflow

FROM python:3.7-slim-buster
LABEL maintainer="KBX"

# Never prompt the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux

# Airflow
ARG AIRFLOW_VERSION=1.10.9
ARG AIRFLOW_USER_HOME=/usr/local/airflow
ARG AIRFLOW_DEPS=""
ARG PYTHON_DEPS=""
ENV AIRFLOW_HOME=${AIRFLOW_USER_HOME}

# Define en_US.
ENV LANGUAGE en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LC_ALL en_US.UTF-8
ENV LC_CTYPE en_US.UTF-8
ENV LC_MESSAGES en_US.UTF-8

# Disable noisy "Handling signal" log messages:
# ENV GUNICORN_CMD_ARGS --log-level WARNING

RUN set -ex \
    && buildDeps=' \
        freetds-dev \
        libkrb5-dev \
        libsasl2-dev \
        libssl-dev \
        libffi-dev \
        libpq-dev \
        git \
    ' \
    && apt-get update -yqq \
    && apt-get upgrade -yqq \
    && apt-get install -yqq --no-install-recommends \
        $buildDeps \
        freetds-bin \
        build-essential \
        default-libmysqlclient-dev \
        apt-utils \
        curl \
        rsync \
        netcat \
        locales \
    && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \
    && locale-gen \
    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \
    && useradd -ms /bin/bash -d ${AIRFLOW_USER_HOME} airflow \
    && pip install -U pip setuptools wheel \
    && pip install pytz \
    && pip install pyOpenSSL \
    && pip install ndg-httpsclient \
    && pip install pyasn1 \
    && pip install apache-airflow[crypto,celery,postgres,hive,jdbc,mysql,ssh${AIRFLOW_DEPS:+,}${AIRFLOW_DEPS}]==${AIRFLOW_VERSION} \
    && pip install 'redis==3.2' \
    && if [ -n "${PYTHON_DEPS}" ]; then pip install ${PYTHON_DEPS}; fi \
    && apt-get purge --auto-remove -yqq $buildDeps \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf \
        /var/lib/apt/lists/* \
        /tmp/* \
        /var/tmp/* \
        /usr/share/man \
        /usr/share/doc \
        /usr/share/doc-base

COPY script/entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_USER_HOME}/airflow.cfg

RUN chown -R airflow: ${AIRFLOW_USER_HOME}

EXPOSE 8080 5555 8793

RUN pip install apache-airflow-backport-providers-amazon \
 && pip install apache-airflow-backport-providers-apache-spark \
 && pip install apache-airflow-backport-providers-jdbc \
 && pip install apache-airflow-backport-providers-datadog \
 && pip install apache-airflow-backport-providers-postgres \
 && pip install sqlalchemy==1.3.13

USER airflow
WORKDIR ${AIRFLOW_USER_HOME}
ENTRYPOINT ["/entrypoint.sh"]
CMD ["webserver"]
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\LICENSE<content=>
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2017 Matthieu "Puckel_" Roisil

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\package-lock.json<content=>
{
  "name": "KBX.DL.CodeTemplates.Airflow",
  "lockfileVersion": 2,
  "requires": true,
  "packages": {}
}
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\package.json<content=>
{}
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\README.md<content=>
# docker-airflow

This repository contains **Dockerfile** of [apache-airflow](https://github.com/apache/incubator-airflow) for [Docker](https://www.docker.com/)'s [automated build](https://registry.hub.docker.com/u/puckel/docker-airflow/) published to the public [Docker Hub Registry](https://registry.hub.docker.com/).

## Information

* Based on Python (3.7-slim-buster) official Image [python:3.7-slim-buster](https://hub.docker.com/_/python/) and uses the official [Postgres](https://hub.docker.com/_/postgres/) as backend and [Redis](https://hub.docker.com/_/redis/) as queue
* Install [Docker](https://www.docker.com/)
* Install [Docker Compose](https://docs.docker.com/compose/install/)
* This follows the Airflow release from this page [Python Package Index](https://pypi.python.org/pypi/apache-airflow)

## Installation

This solution has been created by a code template.

## Build

Set any necessary environment variables in the docker-compose.yml file

* LOAD_EX can be set to "n" if you do not want to deploy the sample DAGs
* AWS credentials should be picked up by the mounted profile drive

Load AWS Credentials
``` Powershell
 kochid aws refresh
```

Compose and start a container from the docker-compose.yml file:

* docker-compose -f docker-compose.yml up

A docker container will now be visible and manageable in Docker Desktop.
The Airflow Web UI can be accessed at: [localhost:8080](http://localhost:8080/)

## Configuration

* Use the variable template DAG to set Airflow variables which your other DAGs require.  Please refer to the [yourprojectname]_variables.py template DAG
* Use the connection template DAG to set Airflow connections; connections should be named [yourprojectname]_[resource]; for example, kbxdlcodetemplatesworkflow_aws for a project named KBX.DL.CodeTemplates.Workflow.  Please refer to the [yourprojectname]_connections.py template DAG

## Management

Stop and remove the container along with any associated resources created via the aforementioned "up" command:

* docker-compose -f docker-compose.yml down

List running containers:

* docker ps

Access the shell of a running container as the default user:

* docker exec -ti "container name" bash

Access the shell of a running container as root:

* docker exec -u 0 -ti "container name" bash

## Pitfalls

* **Connections and Variables** are not persisted between containers unless you include them as environment variables in the docker-compose YAML file.
* Manually refreshing Airflow Web UI pages excessively will cause the instance to become temporarily unresponsive
* Ensure that the "dag" parameter is set for all operators to avoid vague error messages
* A DAG must be in the "On" state to be triggered by schedule or manually
* DAGs which leverage dynamic operator generation based on the results of a query will execute that query every time the "DAG bag" is filled or the DAG is accessed if the query exists within the same DAG.  The query which the dynamic operator generation relies on should exist in a separate DAG (scheduled as per requirements) where the results are written to an Airflow variable; this variable will then be referenced in the DAG containing dynamic operator generation
* Implementation of sub-DAGs requires explicitly setting the "schedule_interval" and "start_date" parameters of the parent DAG instead of passing them as part of the "default_args" parameter to avoid vague error messages
* Provider packages expose operators, hooks, and sensors but only the operators provide "out-of-the-box" functionality; resort to wiring up the hook and sensor only if an operator is unavailable or cannot meet a specific requirement
* For an unscheduled DAG either do not reference the "schedule_interval" property or set it to None (the type, not a string); for example, schedule_interval=None
* Setting the "start_date" parameter of any DAG to the current or a future date/time will cause the scheduler to fail; jobs will look as though they have started but not execute any tasks and be stuck in a "running" state
* **start_date** is counterintuitive but by design; for example, a job scheduled hourly and starting at 1400h will actually execute at 1500h.  All times in airflow are UTC and so don't execute at your local time.  Avoid using datetime.now() to offset this. Please see https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag and https://marclamberti.com/blog/apache-airflow-best-practices-1/ for further information

## References

* puckel Airflow Docker image documentation - https://hub.docker.com/r/puckel/docker-airflow
* Airflow core concepts - https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html
* Airflow best practices - https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html
* Airflow lesser-known tips and tricks - https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\requirements.txt<content=>
# KBX does not support the automation of this file, create a help request
wtforms==2.3.3
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\config\airflow.cfg<content=>
[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Users must supply an Airflow connection id that provides access to the storage
# location.
remote_log_conn_id =
remote_base_log_folder =
encrypt_s3_logs = False

# Logging level
logging_level = INFO

# Logging level for Flask-appbuilder UI
fab_logging_level = WARN

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
# Example: logging_config_class = my.path.default_local_settings.LOGGING_CONFIG
logging_config_class =

# Flag to enable/disable Colored logs in Console
# Colour the logs when the controlling terminal is a TTY.
colored_console_log = True

# Log format for when Colored logs is enabled
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d}} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Format of Log line
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Name of handler to read task instance logs.
# Default to use task handler.
task_log_reader = task

# Hostname by providing a path to a callable, which will resolve the hostname.
# The format is "package:function".
#
# For example, default value "socket:getfqdn" means that result from getfqdn() of "socket"
# package will be used as hostname.
#
# No argument should be required in the function specified.
# If using IP address as hostname is preferred, use value ``airflow.utils.net:get_host_ip_address``
hostname_callable = socket:getfqdn

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = utc

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
# sql_alchemy_conn = sqlite:////tmp/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool. 0 indicates no limit.
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
# When the number of checked-out connections reaches the size set in pool_size,
# additional connections will be returned up to this limit.
# When those additional connections are returned to the pool, they are disconnected and discarded.
# It follows then that the total number of simultaneous connections the pool will allow
# is pool_size + max_overflow,
# and the total number of "sleeping" connections the pool will allow is pool_size.
# max_overflow can be set to -1 to indicate no overflow limit;
# no limit will be placed on the total number of concurrent connections. Defaults to 10.
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite. If the number of DB connections is ever exceeded,
# a lower config value will allow the system to recover faster.
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout.
# Typically, this is a simple statement like "SELECT 1".
# More information here:
# https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic
sql_alchemy_pool_pre_ping = True

# The schema to use for the metadata database.
# SqlAlchemy supports databases with the concept of multiple schemas.
sql_alchemy_schema =

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = True

# Where your Airflow plugins are stored
plugins_folder = /usr/local/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = $FERNET_KEY

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a ``run_as_user`` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
default_impersonation =

# What security module to use (for example kerberos)
security =

# If set to False enables some unsecure features like Charts and Ad Hoc Queries.
# In 2.0 will default to True.
secure_mode = False

# Turn unit test mode on (overwrites many configuration options with test
# values at runtime)
unit_test_mode = False

# Whether to enable pickling for xcom (note that this is insecure and allows for
# RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
enable_xcom_pickling = True

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

# Whether to override params with dag_run.conf. If you pass some key-value pairs
# through ``airflow dags backfill -c`` or
# ``airflow dags trigger -c``, the key-value pairs will override the existing ones in params.
dag_run_conf_overrides_params = False

# Worker initialisation check to validate Metadata Database connection
worker_precheck = False

# When discovering DAGs, ignore any files that don't contain the strings ``DAG`` and ``airflow``.
dag_discovery_safe_mode = True

# The number of retries each task is going to have by default. Can be overridden at dag or task level.
default_task_retries = 0

# Whether to serialises DAGs and persist them in DB.
# If set to True, Webserver reads from DB instead of parsing DAG files
# More details: https://airflow.apache.org/docs/stable/dag-serialization.html
store_serialized_dags = False

# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate.
min_serialized_dag_update_interval = 30

# On each dagrun check against defined SLAs
check_slas = True

[cli]
# In what way should the cli access the API. The LocalClient will use the
# database directly, while the json_client will use the api running on the
# webserver
api_client = airflow.api.client.local_client

# If you set web_server_url_prefix, do NOT forget to append it here, ex:
# ``endpoint_url = http://localhost:8080/myroot``
# So api will look like: ``http://localhost:8080/myroot/api/experimental/...``
endpoint_url = http://localhost:8080

[debug]
# Used only with DebugExecutor. If set to True DAG will fail with first
# failed task. Helpful for debugging purposes.
fail_fast = False

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[lineage]
# what lineage backend to use
backend =

[atlas]
sasl_enabled = False
host =
port = 21000
username =
password =

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via ``default_args``
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[hive]
# Default mapreduce queue for HiveOperator tasks
default_hive_mapred_queue =

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_key =

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# Secret key used to run your flask app
# It should be as random as possible
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use. Choices include
# sync (default), eventlet, gevent
worker_class = sync

# Log files for the gunicorn webserver. '-' means log to stderr.
access_logfile = -

# Log files for the gunicorn webserver. '-' means log to stderr.
error_logfile = -

# Expose the configuration file in the web server
expose_config = True

# Expose hostname in the web server
expose_hostname = True

# Expose stacktrace in the web server
expose_stacktrace = True

# Set to true to turn on authentication:
# https://airflow.apache.org/security.html#web-authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode. Choices include user (default) and ldapgroup.
# Ldap group filtering requires using the ldap backend
#
# Note that the ldap server needs the "memberOf" overlay to be set up
# in order to user the ldapgroup mode.
owner_mode = user

# Default DAG view. Valid values are:
# tree, graph, duration, gantt, landing_times
dag_default_view = tree

# "Default DAG orientation. Valid values are:"
# LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Puts the webserver in demonstration mode; blurs the names of Operators for
# privacy.
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
# while fetching logs from other worker machine
log_fetch_timeout_sec = 5

# Time interval (in secs) to wait before next log fetching.
log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing.
log_auto_tailing_offset = 30

# Animation speed for auto tailing log display.
log_animation_speed = 1000

# By default, the webserver shows paused DAGs. Flip this to hide paused
# DAGs by default
hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
page_size = 100

# Use FAB-based webserver with RBAC feature
rbac = False

# Define the color of navigation bar
navbar_color = #007A87

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
enable_proxy_fix = False

# Number of values to trust for ``X-Forwarded-For``.
# More info: https://werkzeug.palletsprojects.com/en/0.16.x/middleware/proxy_fix/
proxy_fix_x_for = 1

# Number of values to trust for ``X-Forwarded-Proto``
proxy_fix_x_proto = 1

# Number of values to trust for ``X-Forwarded-Host``
proxy_fix_x_host = 1

# Number of values to trust for ``X-Forwarded-Port``
proxy_fix_x_port = 1

# Number of values to trust for ``X-Forwarded-Prefix``
proxy_fix_x_prefix = 1

# Set secure flag on session cookie
cookie_secure = False

# Set samesite policy on session cookie
cookie_samesite =

# Default setting for wrap toggle on DAG code and TI log views.
default_wrap = False

# Allow the UI to be rendered in a frame
x_frame_enabled = True

# Send anonymous user activity to your analytics tool
# choose from google_analytics, segment, or metarouter
# analytics_tool =

# Unique ID of your account in the analytics tool
# analytics_id =

# Update FAB permissions and sync security manager roles
# on webserver startup
update_fab_perms = True

# Minutes of non-activity before logged out from UI
# 0 means never get forcibly logged out
force_log_out_after = 0

# The UI cookie lifetime in days
session_lifetime_days = 30

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]

# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
# smtp_user =
# Example: smtp_password = airflow
# smtp_password =
smtp_port = 25
smtp_mail_from = airflow@example.com

[sentry]

# Sentry (https://docs.sentry.io) integration
sentry_dsn =

[celery]

# This section only applies if you are using the CeleryExecutor in
# ``[core]`` section above
# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# The maximum and minimum concurrency that will be used when starting workers with the
# ``airflow celery worker`` command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Example: worker_autoscale = 16,12
worker_autoscale = 16,12

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://redis:6379/1

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it ``airflow flower``. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# The root URL for Flower
# Example: flower_url_prefix = /flower
flower_url_prefix =

# This defines the port that Celery Flower runs on
flower_port = 5555

# Securing Flower with Basic Authentication
# Accepts user:password pairs separated by a comma
# Example: flower_basic_auth = user1:password1,user2:password2
flower_basic_auth =

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default

# How many processes CeleryExecutor uses to sync task state.
# 0 means to use max(1, number of cores - 1) processes.
sync_parallelism = 0

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# In case of using SSL
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

# Celery Pool implementation.
# Choices include: prefork (default), eventlet, gevent or solo.
# See:
# https://docs.celeryproject.org/en/latest/userguide/workers.html#concurrency
# https://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html
pool = prefork

# The number of seconds to wait before timing out ``send_task_to_executor`` or
# ``fetch_celery_task_state`` operations.
operation_timeout = 2

[celery_broker_transport_options]

# This section is for specifying options which can be passed to the
# underlying celery broker transport. See:
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_transport_options
# The visibility timeout defines the number of seconds to wait for the worker
# to acknowledge the task before the message is redelivered to another worker.
# Make sure to increase the visibility timeout to match the time of the longest
# ETA you're planning to use.
# visibility_timeout is only supported for Redis and SQS celery brokers.
# See:
# http://docs.celeryproject.org/en/master/userguide/configuration.html#std:setting-broker_transport_options
# Example: visibility_timeout = 21600
# visibility_timeout =

[dask]

# This section only applies if you are using the DaskExecutor in
# [core] section above
# The IP address and port of the Dask cluster's scheduler.
cluster_address = 127.0.0.1:8786

# TLS/ SSL settings to access a secured Dask scheduler.
tls_ca =
tls_cert =
tls_key =

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# After how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# The number of times to try to schedule each DAG file
# -1 indicates unlimited number
num_runs = -1

# The number of seconds to wait between consecutive DAG file processing
processor_poll_interval = 1

# after how much time (seconds) a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
dag_dir_list_interval = 300

# How often should stats be printed to the logs. Setting to 0 will disable printing stats
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
# This is used by the health check in the "/health" endpoint
scheduler_health_check_threshold = 30
child_process_log_directory = /usr/local/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is False,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
catchup_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# If this is too high, SQL query performance may be impacted by one
# or more of the following:
# - reversion to full table scan
# - complexity of query predicate
# - excessive locking
# Additionally, you may hit the maximum allowable query length for your db.
# Set this to 0 for no limit (not advised)
max_tis_per_query = 512

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# If you want to avoid send all the available metrics to StatsD,
# you can configure an allow list of prefixes to send only the metrics that
# start with the elements of the list (e.g: scheduler,executor,dagrun)
statsd_allow_list =

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
max_threads = 2
authenticate = False

# Turn off scheduler use of cron intervals by setting this to False.
# DAGs submitted manually in the web UI or with trigger_dag will still run.
use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
# Only has effect if schedule_interval is set to None in DAG
allow_trigger_in_future = False

[ldap]
# set this to ldaps://<your.ldap.server>:<port>
uri =
user_filter = objectClass=*
user_name_attr = uid
group_member_attr = memberOf
superuser_filter =
data_profiler_filter =
bind_user = cn=Manager,dc=example,dc=com
bind_password = insecure
basedn = dc=example,dc=com
cacert = /etc/ca/ldap_ca.crt
search_scope = LEVEL

# This setting allows the use of LDAP servers that either return a
# broken schema, or do not return a schema.
ignore_malformed_schema = False

[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

# Number of cpu cores required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_cpu = 1

# Memory in MB required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_memory = 256

# Enable framework checkpointing for mesos
# See http://mesos.apache.org/documentation/latest/slave-recovery/
checkpoint = False

# Failover timeout in milliseconds.
# When checkpointing is enabled and this option is set, Mesos waits
# until the configured timeout for
# the MesosExecutor framework to re-register after a failover. Mesos
# shuts down running tasks if the
# MesosExecutor framework fails to re-register within this timeframe.
# Example: failover_timeout = 604800
# failover_timeout =

# Enable framework authentication for mesos
# See http://mesos.apache.org/documentation/latest/configuration/
authenticate = False

# Mesos credentials, if authentication is enabled
# Example: default_principal = admin
# default_principal =
# Example: default_secret = admin
# default_secret =

# Optional Docker Image to run on slave before running the command
# This image should be accessible from mesos slave i.e mesos slave
# should be able to pull this docker image before executing the command.
# Example: docker_image_slave = puckel/docker-airflow
# docker_image_slave =

[kerberos]
ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
api_rev = v3

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

[elasticsearch]
# Elasticsearch host
host =

# Format of the log_id, which is used to query for a given tasks logs
log_id_template = {{dag_id}}-{{task_id}}-{{execution_date}}-{{try_number}}

# Used to mark the end of a log stream for a task
end_of_log_mark = end_of_log

# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
# Code will construct log_id using the log_id template from the argument above.
# NOTE: The code will prefix the https:// automatically, don't include that here.
frontend =

# Write the task logs to the stdout of the worker, rather than the default files
write_stdout = False

# Instead of the default log formatter, write the log lines as JSON
json_format = False

# Log fields to also attach to the json output, if enabled
json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
use_ssl = False
verify_certs = True

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository =
worker_container_tag =
worker_container_image_pull_policy = IfNotPresent

# If True (default), worker pods will be deleted upon termination
delete_worker_pods = True

# Number of Kubernetes Worker Pod creation calls per scheduler loop
worker_pods_creation_batch_size = 1

# The Kubernetes namespace where airflow workers should be created. Defaults to ``default``
namespace = default

# The name of the Kubernetes ConfigMap containing the Airflow Configuration (this file)
# Example: airflow_configmap = airflow-configmap
airflow_configmap =

# The name of the Kubernetes ConfigMap containing ``airflow_local_settings.py`` file.
#
# For example:
#
# ``airflow_local_settings_configmap = "airflow-configmap"`` if you have the following ConfigMap.
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     airflow_local_settings.py: |
#         def pod_mutation_hook(pod):
#             ...
#     airflow.cfg: |
#         ...
# Example: airflow_local_settings_configmap = airflow-configmap
airflow_local_settings_configmap =

# For docker image already contains DAGs, this is set to ``True``, and the worker will
# search for dags in dags_folder,
# otherwise use git sync or dags volume claim to mount DAGs
dags_in_image = False

# For either git sync or volume mounted DAGs, the worker will look in this subpath for DAGs
dags_volume_subpath =

# For DAGs mounted via a volume claim (mutually exclusive with git-sync and host path)
dags_volume_claim =

# For volume mounted logs, the worker will look in this subpath for logs
logs_volume_subpath =

# A shared volume claim for the logs
logs_volume_claim =

# For DAGs mounted via a hostPath volume (mutually exclusive with volume claim and git-sync)
# Useful in local environment, discouraged in production
dags_volume_host =

# A hostPath volume for the logs
# Useful in local environment, discouraged in production
logs_volume_host =

# A list of configMapsRefs to envFrom. If more than one configMap is
# specified, provide a comma separated list: configmap_a,configmap_b
env_from_configmap_ref =

# A list of secretRefs to envFrom. If more than one secret is
# specified, provide a comma separated list: secret_a,secret_b
env_from_secret_ref =

# Git credentials and repository for DAGs mounted via Git (mutually exclusive with volume claim)
git_repo =
git_branch =
git_subpath =

# The specific rev or hash the git_sync init container will checkout
# This becomes GIT_SYNC_REV environment variable in the git_sync init container for worker pods
git_sync_rev =

# Use git_user and git_password for user authentication or git_ssh_key_secret_name
# and git_ssh_key_secret_key for SSH authentication
git_user =
git_password =
git_sync_root = /git
git_sync_dest = repo

# Mount point of the volume if git-sync is being used.
# i.e. /usr/local/airflow/dags
git_dags_folder_mount_point =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-secrets.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: airflow-secrets
#   data:
#     # key needs to be gitSshKey
#     gitSshKey: <base64_encoded_data>
# Example: git_ssh_key_secret_name = airflow-secrets
git_ssh_key_secret_name =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     known_hosts: |
#         github.com ssh-rsa <...>
#     airflow.cfg: |
#         ...
# Example: git_ssh_known_hosts_configmap_name = airflow-configmap
git_ssh_known_hosts_configmap_name =

# To give the git_sync init container credentials via a secret, create a secret
# with two fields: GIT_SYNC_USERNAME and GIT_SYNC_PASSWORD (example below) and
# add ``git_sync_credentials_secret = <secret_name>`` to your airflow config under the
# ``kubernetes`` section
#
# Secret Example:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: git-credentials
#   data:
#     GIT_SYNC_USERNAME: <base64_encoded_git_username>
#     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
git_sync_credentials_secret =

# For cloning DAGs from git repositories into volumes: https://github.com/kubernetes/git-sync
git_sync_container_repository = k8s.gcr.io/git-sync
git_sync_container_tag = v3.1.1
git_sync_init_container_name = git-sync-clone
git_sync_run_as_user = 65533

# The name of the Kubernetes service account to be associated with airflow workers, if any.
# Service accounts are required for workers that require access to secrets or cluster resources.
# See the Kubernetes RBAC documentation for more:
# https://kubernetes.io/docs/admin/authorization/rbac/
worker_service_account_name =

# Any image pull secrets to be given to worker pods, If more than one secret is
# required, provide a comma separated list: secret_a,secret_b
image_pull_secrets =

# GCP Service Account Keys to be provided to tasks run on Kubernetes Executors
# Should be supplied in the format: key-name-1:key-path-1,key-name-2:key-path-2
gcp_service_account_keys =

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has.
# cluster_context =
# config_file =

# Affinity configuration as a single line formatted JSON object.
# See the affinity model for top-level key names (e.g. ``nodeAffinity``, etc.):
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#affinity-v1-core
affinity =

# A list of toleration objects as a single line formatted JSON array
# See:
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#toleration-v1-core
tolerations =

# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis.
# See:
# https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
# Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely
# for kubernetes api responses, which will cause the scheduler to hang.
# The timeout is specified as [connect timeout, read timeout]
kube_client_request_args = {{"_request_timeout" : [60,60] }}

# Specifies the uid to run the first process of the worker pods containers as
run_as_user =

# Specifies a gid to associate with all containers in the worker pods
# if using a git_ssh_key_secret_name use an fs_group
# that allows for the key to be read, e.g. 65533
fs_group =

[kubernetes_node_selectors]

# The Key-value pairs to be given to worker pods.
# The worker pods will be scheduled to the nodes of the specified key-value pairs.
# Should be supplied in the format: key = value

[kubernetes_annotations]

# The Key-value annotations pairs to be given to worker pods.
# Should be supplied in the format: key = value

[kubernetes_environment_variables]

# The scheduler sets the following environment variables into your workers. You may define as
# many environment variables as needed and the kubernetes launcher will set them in the launched workers.
# Environment variables in this section are defined as follows
# ``<environment_variable_key> = <environment_variable_value>``
#
# For example if you wanted to set an environment variable with value `prod` and key
# ``ENVIRONMENT`` you would follow the following format:
# ENVIRONMENT = prod
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_secrets]

# The scheduler mounts the following secrets into your workers as they are launched by the
# scheduler. You may define as many secrets as needed and the kubernetes launcher will parse the
# defined secrets and mount them as secret environment variables in the launched workers.
# Secrets in this section are defined as follows
# ``<environment_variable_mount> = <kubernetes_secret_object>=<kubernetes_secret_key>``
#
# For example if you wanted to mount a kubernetes secret key named ``postgres_password`` from the
# kubernetes secret object ``airflow-secret`` as the environment variable ``POSTGRES_PASSWORD`` into
# your workers you would follow the following format:
# ``POSTGRES_PASSWORD = airflow-secret=postgres_credentials``
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_labels]

# The Key-value pairs to be given to worker pods.
# The worker pods will be given these static labels, as well as some additional dynamic labels
# to identify the task.
# Should be supplied in the format: ``key = value``
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\dags\kbxanalyticsdlazuredevops_airflow.py<content=>
from os import getenv
# from tkinter import Variable
import airflow
from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.operators.glue import AwsGlueJobOperator
from ast import literal_eval
from airflow.models import Variable

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals

# Same as File Name
DAG_ID = 'kbxanalyticsdlazuredevops_airflow'
# When it should be turned on, not execution date.
START_DATE = airflow.utils.dates.days_ago(1)
# How often to Run. @daily - Once a day at Midnight
SCHEDULE_INTERVAL = '30 06 * * *'
# Who is listed as the owner of this DAG in the Airflow Web Server
DAG_OWNER_NAME = "KBX"
# List of email address to send email alerts to if this job fails

AWS_CONN_ID = 'kbxanalyticsdlazuredevops_aws'
ENV = Variable.get('kbxanalyticsdlazuredevops_environment')

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': START_DATE,
    'catchup': False
}

with DAG(
    DAG_ID, 
    schedule_interval=SCHEDULE_INTERVAL,
    default_args=default_args
    ) as dag:

    po_workitems_raw_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-workitems-raw-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-workitems-raw-job-{ENV}",
        dag=dag,
        retries = 1,
        aws_conn_id=AWS_CONN_ID
    )

    po_workitems_structured_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-workitems-structured-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-workitems-structured-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    # po_workitems_curated_job = AwsGlueJobOperator(
        # task_id=f"execute_kbxt-dl-analytics-azuredevops-workitems-curated-job-{ENV}", 
        # job_name=f"kbxt-dl-analytics-azuredevops-workitems-curated-job-{ENV}",
        # dag=dag,
        # retries = 0,
        # aws_conn_id=AWS_CONN_ID
    # )

    po_projects_raw_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-projects-raw-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-projects-raw-job-{ENV}",
        dag=dag,
        retries = 1,
        aws_conn_id=AWS_CONN_ID
    )

    po_projects_structured_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-azuredevops-projects-structured-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-azuredevops-projects-structured-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    # po_projects_curated_job = AwsGlueJobOperator(
        # task_id=f"execute_kbxt-dl-analytics-azuredevops-projects-curated-job-{ENV}", 
        # job_name=f"kbxt-dl-analytics-azuredevops-projects-curated-job-{ENV}",
        # dag=dag,
        # retries = 0,
        # aws_conn_id=AWS_CONN_ID
    # )

    po_workitems_raw_job >> po_workitems_structured_job

    po_projects_raw_job >> po_projects_structured_job
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\dags\kbxanalyticsdlazuredevops_connections.py<content=>
from os import getenv
from airflow import DAG, settings
from airflow.models import Connection
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator

def ListConnections():
    return settings.Session().query(Connection)

def CreateConnections():
    try:
        # Build a connection object:
        conn = Connection(
            # conn_id: Name of the connection as displayed in the Airflow UI.
			# Snake-case; prefix with the product name.
            conn_id="kbxanalyticsdlazuredevops_connDescription",
            # conn_type: The type of connection to create.
            # Valid conn_type values are: "azure_cosmos", "azure_data_lake", "cassandra", "cloudant", 
            # "docker", "gcpcloudsql", "google_cloud_platform", "grpc", "hive_cli", "hiveserver2", 
            # "jdbc", "jira", "mongo", "mssql", "mysql", "oracle", "pig_cli", "postgres", "presto",
            # "redis", "sqlite", "vertica", "wasb".
            conn_type="", 
            # host: Endpoint at which the resource exists; URL, IP address, etc.
            host="", 
            login="", 
            # Leave the password property value as-is; this will be updated via the Airflow UI.
            password="ChangeMeLater",
            # port: The port to use when creating a database type connection.
            # port=1234,
            # schema: The schema to use when creating a database type connection.
            # schema="schema"
            # extra: Used to specify additional connection type specific settings.
            # Refer to https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html for more details.
            # extra="json-formatted string"
        )

        # Get the current Airflow session:
        session = settings.Session()
        # Add the connection to the session if it doesn't already exist:
        if(conn.conn_id not in ListConnections()):
            session.add(conn)
            # Commit the newly-created connection:
            session.commit()
            return True
            
        return False
    except:
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_connections', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_connections",
        python_callable=CreateConnections
    )

    po
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\dags\kbxanalyticsdlazuredevops_variables.py<content=>
from os import getenv
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator
from airflow.models import Variable

def SetVariables():
    try:
        # Multiple variables may be set via multiple Variable.set() calls.
        # This variale DAG can be run once manually for static values or leverage
        # schedules to set variable values based on the results of other operations.
        Variable.set("kbxanalyticsdlazuredevops_variableDescription", 'value/values')
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_variables', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_variables",
        python_callable=SetVariables
    )

    po
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_get_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable
from ast import literal_eval

def GetVariable():
    print("Performing Variable.get() action...")
    keys=literal_eval(Variable.get("kbxanalyticsdlazuredevops_variable"))
    print(keys)

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
    #'schedule_interval': '0/3 * * * ?'
}

with DAG("kbxanalyticsdlazuredevops_example_get_variable", default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="get_variable",
        provide_context=False,
        python_callable=GetVariable
    )

    po
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_glue_test.py<content=>
from os import getenv

from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator

def ListGlueJobs():
    try:
        gh = AwsGlueJobHook(aws_conn_id="aws_lg_nonprod")
        print(gh.list_jobs())
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_example_glue_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_glue_jobs",
        provide_context=False,
        python_callable=ListGlueJobs
    )

    po
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_s3_conn_test.py<content=>
"""
S3 Connection Test
"""
from airflow import DAG
from airflow.hooks.base_hook import BaseHook
from airflow.operators.python_operator import PythonOperator
import boto3
from datetime import *

def ListBuckets():
    try:
        s3Conn=boto3.client("s3")
        res = s3Conn.list_buckets()
        for bkt in res["Buckets"]:
            print(bkt["Name"])

        if(len(res["Buckets"]) > 0):
            return True
        return False
    except :
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlazuredevops_example_s3_conn_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_s3_buckets",
        provide_context=False,
        python_callable=ListBuckets
    )

    po
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\examples\kbxanalyticsdlazuredevops_example_set_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable

def SetVariable():
    keys=["kbxanalyticsdlazuredevops_0","1","2","3","4","5"]
    print("Performing Variable.get() action...")
    Variable.set("kbxanalyticsdlazuredevops_variable", keys)
    
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG("kbxanalyticsdlazuredevops_example_set_variable", default_args=default_args) as dag:
    po = PythonOperator(task_id="set_variable",
        provide_context=False,
        python_callable=SetVariable
    )

    po
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workflow\script\entrypoint.sh<content=>
#!/usr/bin/env bash

# User-provided configuration must always be respected.
#
# Therefore, this script must only derives Airflow AIRFLOW__ variables from other variables
# when the user did not provide their own configuration.

TRY_LOOP="20"

# Global defaults and back-compat
: "${AIRFLOW_HOME:="/usr/local/airflow"}"
: "${AIRFLOW__CORE__FERNET_KEY:=${FERNET_KEY:=$(python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)")}}"
: "${AIRFLOW__CORE__EXECUTOR:=${EXECUTOR:-Sequential}Executor}"

# Load DAGs examples (default: Yes)
if [[ -z "$AIRFLOW__CORE__LOAD_EXAMPLES" && "${LOAD_EX:=n}" == n ]]; then
  AIRFLOW__CORE__LOAD_EXAMPLES=False
fi

export \
  AIRFLOW_HOME \
  AIRFLOW__CORE__EXECUTOR \
  AIRFLOW__CORE__FERNET_KEY \
  AIRFLOW__CORE__LOAD_EXAMPLES \

# Install custom python package if requirements.txt is present
if [ -e "/requirements.txt" ]; then
    $(command -v pip) install --user -r /requirements.txt
fi

wait_for_port() {
  local name="$1" host="$2" port="$3"
  local j=0
  while ! nc -z "$host" "$port" >/dev/null 2>&1 < /dev/null; do
    j=$((j+1))
    if [ $j -ge $TRY_LOOP ]; then
      echo >&2 "$(date) - $host:$port still not reachable, giving up"
      exit 1
    fi
    echo "$(date) - waiting for $name... $j/$TRY_LOOP"
    sleep 5
  done
}

# Other executors than SequentialExecutor drive the need for an SQL database, here PostgreSQL is used
if [ "$AIRFLOW__CORE__EXECUTOR" != "SequentialExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the database
  if [ -z "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" ]; then
    # Default values corresponding to the default compose files
    : "${POSTGRES_HOST:="postgres"}"
    : "${POSTGRES_PORT:="5432"}"
    : "${POSTGRES_USER:="airflow"}"
    : "${POSTGRES_PASSWORD:="airflow"}"
    : "${POSTGRES_DB:="airflow"}"
    : "${POSTGRES_EXTRAS:-""}"

    AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
    export AIRFLOW__CORE__SQL_ALCHEMY_CONN

    # Check if the user has provided explicit Airflow configuration for the broker's connection to the database
    if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
      AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
      export AIRFLOW__CELERY__RESULT_BACKEND
    fi
  else
    if [[ "$AIRFLOW__CORE__EXECUTOR" == "CeleryExecutor" && -z "$AIRFLOW__CELERY__RESULT_BACKEND" ]]; then
      >&2 printf '%s\n' "FATAL: if you set AIRFLOW__CORE__SQL_ALCHEMY_CONN manually with CeleryExecutor you must also set AIRFLOW__CELERY__RESULT_BACKEND"
      exit 1
    fi

    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    POSTGRES_ENDPOINT=$(echo -n "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" | cut -d '/' -f3 | sed -e 's,.*@,,')
    POSTGRES_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    POSTGRES_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Postgres" "$POSTGRES_HOST" "$POSTGRES_PORT"
fi

# CeleryExecutor drives the need for a Celery broker, here Redis is used
if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the broker
  if [ -z "$AIRFLOW__CELERY__BROKER_URL" ]; then
    # Default values corresponding to the default compose files
    : "${REDIS_PROTO:="redis://"}"
    : "${REDIS_HOST:="redis"}"
    : "${REDIS_PORT:="6379"}"
    : "${REDIS_PASSWORD:=""}"
    : "${REDIS_DBNUM:="1"}"

    # When Redis is secured by basic auth, it does not handle the username part of basic auth, only a token
    if [ -n "$REDIS_PASSWORD" ]; then
      REDIS_PREFIX=":${REDIS_PASSWORD}@"
    else
      REDIS_PREFIX=
    fi

    AIRFLOW__CELERY__BROKER_URL="${REDIS_PROTO}${REDIS_PREFIX}${REDIS_HOST}:${REDIS_PORT}/${REDIS_DBNUM}"
    export AIRFLOW__CELERY__BROKER_URL
  else
    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    REDIS_ENDPOINT=$(echo -n "$AIRFLOW__CELERY__BROKER_URL" | cut -d '/' -f3 | sed -e 's,.*@,,')
    REDIS_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    REDIS_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Redis" "$REDIS_HOST" "$REDIS_PORT"
fi

case "$1" in
  webserver)
    airflow initdb
    if [ "$AIRFLOW__CORE__EXECUTOR" = "LocalExecutor" ] || [ "$AIRFLOW__CORE__EXECUTOR" = "SequentialExecutor" ]; then
      # With the "Local" and "Sequential" executors it should all run in one container.
      airflow scheduler &
    fi
    exec airflow webserver
    ;;
  worker|scheduler)
    # Give the webserver time to run initdb.
    sleep 10
    exec airflow "$@"
    ;;
  flower)
    sleep 10
    exec airflow "$@"
    ;;
  version)
    exec airflow "$@"
    ;;
  *)
    # The command is something like bash, not an airflow subcommand. Just run it in the right environment.
    exec "$@"
    ;;
esac
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Workitems.Ingest"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Workitems.Ingest"
$replacementEntityName = "WORKITEMS"
$replacementEntityNameLower = "workitems"
$replacementEntityNamePascal = "Workitems"
$replacementEntityNameUpper = "WORKITEMS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsworkitems"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Workitems.Ingest
============

## Introduction 

This solutions is reponsible for ingesting the data.  It has python scripts that are scheduled and ran with Glue to. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in ingest.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## ingest.py

- Writes data to the **bucket_target**/**prefix_target**/ingest_date=yyyy-MM-dd partition.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe ingest.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagetarget
  displayName: Target Stage
  default: Target ingest name, such as raw

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'workitems' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Ingest.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Ingest.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Ingest deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Ingest/KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  IngestStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  IngestJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/ingest.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        PythonLibPath: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity

  TransformTriggerScheduledStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/TriggerScheduled.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        Schedule: "cron(0 */4 * * ? *)"   

  IngestCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 3
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Ref AdditionalPythonModules,
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--prefix_target" : !Ref Entity,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3"
      }

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest\KBX.Analytics.DL.AzureDevOps.Workitems.Ingest.Jobs\ingest.py<content=>
#%% ingest
import os
import sys
from datetime import datetime
import argparse

# Timer
start_time = datetime.now()

# file
f = os.path.basename(__file__)

#%% Init
from KbxtDlPy.Harness import Job
job = Job(name="ingest", level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-raw-dev"
prefix_target = args.prefix_target # ex:"workitems"

# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)

#%% Job Process 
try:    
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))

    ##### YOUR CODE START #####

    # Write JSON data to "[bucket_target_path]/[prefix_target]/ingest_date=yyyy-MM-dd/kbxanalyticsdlazuredevopsworkitems.ingest+py+yyyyMMddHHmmssfffffff.json partition format with sequential file names.
    # Partition string format: datetime.now().strftime("ingest_date=%Y-%m-%d")
    # File name string format: datetime.now().strftime("%Y%m%d%H%M%S%f")
    # ex: s3://kbxt-dl-analytics-servicenow-task-raw-dev/task/ingest_date=2022-06-22/kbxanalyticsdlservicenowtask.ingest+py+202203231800278283056.json

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

# %%
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.AzureDevOps.Workitems.Structured"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.AzureDevOps.Workitems.Structured"
$replacementEntityName = "WORKITEMS"
$replacementEntityNameLower = "workitems"
$replacementEntityNamePascal = "Workitems"
$replacementEntityNameUpper = "WORKITEMS"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlazuredevopsworkitems"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\README.md<content=>
KBX.Analytics.DL.AzureDevOps.Workitems.Structured
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'workitems' # Determined by CodeTemplate ProjectName parameter.
  domain: 'azuredevops'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Jobs
    AllowedValues:
      - KBX.Analytics.DL.AzureDevOps.Workitems.Structured/KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity

  TransformTriggerScheduledStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/TriggerScheduled.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        Schedule: "cron(*/10 * * * ? *)"   

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 3
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Ref AdditionalPythonModules,
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        #"--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3"
      }

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\template\KBX.Analytics.DL.AzureDevOps.Workitems.Structured\KBX.Analytics.DL.AzureDevOps.Workitems.Structured.Jobs\transform.py<content=>
#%% transform
import os
import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import argparse

# Timer
start_time = datetime.now()

# file
f = os.path.basename(__file__)

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.AzureDevOps.Workitems.Structured") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%% Init
from KbxtDlPy.Harness import Job #For local testing, add lib. here and in harness.py as a prefix
job = Job(name="transform", level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--schema_json', nargs='?', const='', type=str, default='')
parser.add_argument('--file_format', nargs='?', const='', type=str, default='json')
parser.add_argument('--filename_prefix', nargs='?', const='', type=str, default='')
parser.add_argument('--includes_header', dest="includesHeader", action="store_true")
parser.set_defaults(includesHeader=False)

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-raw-dev"
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-structured-dev"
prefix_source = args.prefix_source # ex:"deventity"
schema_json = args.schema_json
file_format = args.file_format # ex:"parquet","csv","json"
filename_prefix = args.filename_prefix # ex:the prefix you want to provide to the part file name. Should be all lowercase and without any spaces or special characters.
includes_header = args.includesHeader

# Prefix of files to process, in case files need to be excluded
file_prefix = "" # ex:"part-"

# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")    
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

#%% Job Start
# All files for a date partition that haven't been processed are 
# returned, so be cognizent of the size of this dataframe.
# json_schema parameter is optional; the dataframe schema is inferred when this parameter is not supplied.
df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, file_format=file_format, path=prefix_source, schema_json=schema_json, includes_header=includes_header)

#%% Job Process 
try:    
    if (df is not None):
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])  


        ##### YOUR CODE START #####

        df_transformed = df # Dataframe to write

        ##### YOUR CODE END #####


        # Commit files
        job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition), filename_prefix)

        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df.count()))

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

#%% Job End
job.runtime().end()

# %%
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.DL.CodeTemplates.Ingest"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.DL.CodeTemplates.Ingest"
$replacementEntityName = "DEVTEMPLATE"
$replacementEntityNameLower = "devtemplate"
$replacementEntityNamePascal = "DevTemplate"
$replacementEntityNameUpper = "DEVTEMPLATE"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "devtemplateprojectname"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "productnametemplatereplace"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\README.md<content=>
KBX.DL.CodeTemplates.Ingest
============

## Introduction 

This solutions is reponsible for ingesting the data.  It has python scripts that are scheduled and ran with Glue to. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in ingest.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## ingest.py

- Writes data to the **bucket_target**/**prefix_target**/ingest_date=yyyy-MM-dd partition.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe ingest.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagetarget
  displayName: Target Stage
  default: Target ingest name, such as raw

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'productnametemplatereplace' # This can be hard-coded since the solution is named per product
  entity: 'devtemplate' # Determined by CodeTemplate ProjectName parameter.
  domain: '[domain]'
  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Ingest.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Infrastructure\Ingest.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Ingest deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.DL.CodeTemplates.Ingest/KBX.DL.CodeTemplates.Ingest.Infrastructure
    AllowedValues:
      - KBX.DL.CodeTemplates.Ingest/KBX.DL.CodeTemplates.Ingest.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.DL.CodeTemplates.Ingest/KBX.DL.CodeTemplates.Ingest.Jobs
    AllowedValues:
      - KBX.DL.CodeTemplates.Ingest/KBX.DL.CodeTemplates.Ingest.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-productnametemplatereplace-service-role
    AllowedValues:
      - kbxt-dl-productnametemplatereplace-service-role

Resources:
  IngestStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  IngestJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/ingest.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        PythonLibPath: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity

  TransformTriggerScheduledStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/TriggerScheduled.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        Schedule: "cron(0 */4 * * ? *)"   

  IngestCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 3
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Ref AdditionalPythonModules,
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--prefix_target" : !Ref Entity,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3"
      }

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  <path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Ingest\KBX.DL.CodeTemplates.Ingest.Jobs\ingest.py<content=>
#%% ingest
import os
import sys
from datetime import datetime
import argparse

# Timer
start_time = datetime.now()

# file
f = os.path.basename(__file__)

#%% Init
from KbxtDlPy.Harness import Job
job = Job(name="ingest", level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-raw-dev"
prefix_target = args.prefix_target # ex:"devtemplate"

# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)

#%% Job Process 
try:    
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))

    ##### YOUR CODE START #####

    # Write JSON data to "[bucket_target_path]/[prefix_target]/ingest_date=yyyy-MM-dd/devtemplateprojectname.ingest+py+yyyyMMddHHmmssfffffff.json partition format with sequential file names.
    # Partition string format: datetime.now().strftime("ingest_date=%Y-%m-%d")
    # File name string format: datetime.now().strftime("%Y%m%d%H%M%S%f")
    # ex: s3://kbxt-dl-analytics-servicenow-task-raw-dev/task/ingest_date=2022-06-22/kbxanalyticsdlservicenowtask.ingest+py+202203231800278283056.json

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

# %%
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.DL.CodeTemplates.Transform"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.DL.CodeTemplates.Transform"
$replacementEntityName = "DEVTEMPLATE"
$replacementEntityNameLower = "devtemplate"
$replacementEntityNamePascal = "DevTemplate"
$replacementEntityNameUpper = "DEVTEMPLATE"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "devtemplateprojectname"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "productnametemplatereplace"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\README.md<content=>
KBX.DL.CodeTemplates.Transform
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'productnametemplatereplace' # This can be hard-coded since the solution is named per product
  entity: 'devtemplate' # Determined by CodeTemplate ProjectName parameter.
  domain: '[domain]'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.DL.CodeTemplates.Transform/KBX.DL.CodeTemplates.Transform.Infrastructure
    AllowedValues:
      - KBX.DL.CodeTemplates.Transform/KBX.DL.CodeTemplates.Transform.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.DL.CodeTemplates.Transform/KBX.DL.CodeTemplates.Transform.Jobs
    AllowedValues:
      - KBX.DL.CodeTemplates.Transform/KBX.DL.CodeTemplates.Transform.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-productnametemplatereplace-service-role
    AllowedValues:
      - kbxt-dl-productnametemplatereplace-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity

  TransformTriggerScheduledStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/TriggerScheduled.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        Schedule: "cron(*/10 * * * ? *)"   

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Schedule:
        ScheduleExpression: !Ref Schedule
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 3
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Ref AdditionalPythonModules,
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        #"--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--encryption-type": "sse-s3"
      }

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  <path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Transform\KBX.DL.CodeTemplates.Transform.Jobs\transform.py<content=>
#%% transform
import os
import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import argparse

# Timer
start_time = datetime.now()

# file
f = os.path.basename(__file__)

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.DL.CodeTemplates.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%% Init
from KbxtDlPy.Harness import Job #For local testing, add lib. here and in harness.py as a prefix
job = Job(name="transform", level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--schema_json', nargs='?', const='', type=str, default='')
parser.add_argument('--file_format', nargs='?', const='', type=str, default='json')
parser.add_argument('--filename_prefix', nargs='?', const='', type=str, default='')
parser.add_argument('--includes_header', dest="includesHeader", action="store_true")
parser.set_defaults(includesHeader=False)

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-raw-dev"
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-structured-dev"
prefix_source = args.prefix_source # ex:"deventity"
schema_json = args.schema_json
file_format = args.file_format # ex:"parquet","csv","json"
filename_prefix = args.filename_prefix # ex:the prefix you want to provide to the part file name. Should be all lowercase and without any spaces or special characters.
includes_header = args.includesHeader

# Prefix of files to process, in case files need to be excluded
file_prefix = "" # ex:"part-"

# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")    
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

#%% Job Start
# All files for a date partition that haven't been processed are 
# returned, so be cognizent of the size of this dataframe.
# json_schema parameter is optional; the dataframe schema is inferred when this parameter is not supplied.
df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, file_format=file_format, path=prefix_source, schema_json=schema_json, includes_header=includes_header)

#%% Job Process 
try:    
    if (df is not None):
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])  


        ##### YOUR CODE START #####

        df_transformed = df # Dataframe to write

        ##### YOUR CODE END #####


        # Commit files
        job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition), filename_prefix)

        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df.count()))

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

#%% Job End
job.runtime().end()

# %%
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\.dockerignore<content=>
.git
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\.gitignore<content=>
### Vim ###
[._]*.s[a-w][a-z]
[._]s[a-w][a-z]
*.un~
Session.vim
.netrwhist
*~

### SublimeText ###
# cache files for sublime text
*.tmlanguage.cache
*.tmPreferences.cache
*.stTheme.cache

# workspace files are user-specific
*.sublime-workspace

# project files should be checked into the repository, unless a significant
# proportion of contributors will probably not be using SublimeText
# *.sublime-project

# sftp configuration file
sftp-config.json

# Python
__pycache__
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'productnametemplatereplace' # This can be hard-coded since the solution is named per product
  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: ''$(prefix)-$(product)-airflow-$(environment)' 
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: 'deployments/$(Build.Repository.Name)/'
 
stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**/!(*_example_*)'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Copy
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)dags'
        globExpressions: '**/!(*_example_*)'
        targetFolder: 'dags'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'


<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Run powershell as administrator

- Clone this repo locally
    ```POWERSHELL
    git clone https://kbxltrans@dev.azure.com/kbxltrans/Platform-Data/_git/KBX.DL.CodeTemplates.Workflow
    ```

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1** and install wtforms.

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[DomainName] -LOAD_EX 
    ```

    ```POWERSHELL
    pip3 install wtforms==2.3.3
    ```

- Navigate to the new solutions directory

    ```POWERSHELL
    cd ..\KBX.[ProductName].DL.[DomainName]
    ```

**NOTE**: When the script is downloaded you may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project Setup
- Update the docker-compose.yaml replace the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_DEFAULT_REGION with your values, if your job needs to access AWS.

## Project ReadMe Files
- Review the ReadMe.md file

## Run Solution
- Bring up containers
```POWERSHELL
  docker-compose -f docker-compose.yml up
```
- Navigate to the Administration at http://localhost:8080/
- Bring down containers "CTRL +c"
```POWERSHELL
  docker-compose -f docker-compose.yml down
```

## Project Pipeline Setup
This will deploy the "dags" folder to an Airflow environment, which must exist prior to the pipeline deployment.
- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Congrats
- You have completed setup of your solution.  Please remove this file.<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. This takes on the naming convention of KBX.[Product].DL.[Domain]  ex) KBX.eDock.DL.Legacy

.PARAMETER LOAD_EX
Load examples. OPTIONS: y, n

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Legacy -LOAD_EX y

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format: KBX.[Product].[Domain]:")]
	[String]
	$ProjectName,

	[Parameter(Mandatory = $true, HelpMessage = "Do you want to load examples (ex. y):")]
	[String]
	$LOAD_EX #OPTIONS: y, n
)

$TemplateProject = "KBX.DL.CodeTemplates.Workflow"
$EntityName = ""

#Change these to accomidate new templates
$oldProjectName = "KBX.DL.DEV.TEMPLATE"
$replacementEntityName = "DEVTEMPLATE"
$replacementEntityNameLower = "devtemplate"
$replacementEntityNamePascal = "DevTemplate"
$replacementEntityNameUpper = "DEVTEMPLATE"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementLoadExLower = "devtemplateloadex"
$replacementProjectAliasLower = "projectnametemplatereplace"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productName = $ProjectName.Split(".")[1]
$productNameLower = $productName.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")
$templateFiles = @("CodeTemplateDatalakeReadme.md", "CreateNewDatalakeProjectFromCodeTemplate.ps1")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		Select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
Select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $ProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementProjectAliasLower)", $newProjectAliasLower } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $ProjectName -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementLoadExLower, $LOAD_EX -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed

Write-Host "Removing template PowerShell script and readme..."
$templateFiles | ForEach-Object -Process {
	Remove-Item "$Destination\$_" -Force
}

If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\docker-compose.yml<content=>
version: '3.7'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"

    webserver:
        image: puckel/docker-airflow:latest
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=devtemplateloadex
            - EXECUTOR=Local
            # Encryption
            # Can be static, it's only used for local development
            - FERNET_KEY=lUDB1r1SNvYy4kZAhA-4z8PqC0x8UQfYmo4uFP8UFcg=
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ~/.aws:/root/.aws:ro
            - ./dags:/usr/local/airflow/dags
            - ./plugins:/usr/local/airflow/plugins
            - ./requirements.txt:/requirements.txt
        ports:
            - "8080:8080"
        build: .
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\Dockerfile<content=>
# VERSION 1.10.9
# AUTHOR: Matthieu "Puckel_" Roisil
# DESCRIPTION: Basic Airflow container
# BUILD: docker build --rm -t puckel/docker-airflow .
# SOURCE: https://github.com/puckel/docker-airflow

FROM python:3.7-slim-buster
LABEL maintainer="KBX"

# Never prompt the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux

# Airflow
ARG AIRFLOW_VERSION=1.10.9
ARG AIRFLOW_USER_HOME=/usr/local/airflow
ARG AIRFLOW_DEPS=""
ARG PYTHON_DEPS=""
ENV AIRFLOW_HOME=${AIRFLOW_USER_HOME}

# Define en_US.
ENV LANGUAGE en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LC_ALL en_US.UTF-8
ENV LC_CTYPE en_US.UTF-8
ENV LC_MESSAGES en_US.UTF-8

# Disable noisy "Handling signal" log messages:
# ENV GUNICORN_CMD_ARGS --log-level WARNING

RUN set -ex \
    && buildDeps=' \
        freetds-dev \
        libkrb5-dev \
        libsasl2-dev \
        libssl-dev \
        libffi-dev \
        libpq-dev \
        git \
    ' \
    && apt-get update -yqq \
    && apt-get upgrade -yqq \
    && apt-get install -yqq --no-install-recommends \
        $buildDeps \
        freetds-bin \
        build-essential \
        default-libmysqlclient-dev \
        apt-utils \
        curl \
        rsync \
        netcat \
        locales \
    && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \
    && locale-gen \
    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \
    && useradd -ms /bin/bash -d ${AIRFLOW_USER_HOME} airflow \
    && pip install -U pip setuptools wheel \
    && pip install pytz \
    && pip install pyOpenSSL \
    && pip install ndg-httpsclient \
    && pip install pyasn1 \
    && pip install apache-airflow[crypto,celery,postgres,hive,jdbc,mysql,ssh${AIRFLOW_DEPS:+,}${AIRFLOW_DEPS}]==${AIRFLOW_VERSION} \
    && pip install 'redis==3.2' \
    && if [ -n "${PYTHON_DEPS}" ]; then pip install ${PYTHON_DEPS}; fi \
    && apt-get purge --auto-remove -yqq $buildDeps \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf \
        /var/lib/apt/lists/* \
        /tmp/* \
        /var/tmp/* \
        /usr/share/man \
        /usr/share/doc \
        /usr/share/doc-base

COPY script/entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_USER_HOME}/airflow.cfg

RUN chown -R airflow: ${AIRFLOW_USER_HOME}

EXPOSE 8080 5555 8793

RUN pip install apache-airflow-backport-providers-amazon \
 && pip install apache-airflow-backport-providers-apache-spark \
 && pip install apache-airflow-backport-providers-jdbc \
 && pip install apache-airflow-backport-providers-datadog \
 && pip install apache-airflow-backport-providers-postgres \
 && pip install sqlalchemy==1.3.13

USER airflow
WORKDIR ${AIRFLOW_USER_HOME}
ENTRYPOINT ["/entrypoint.sh"]
CMD ["webserver"]
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\LICENSE<content=>
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2017 Matthieu "Puckel_" Roisil

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\package-lock.json<content=>
{
  "name": "KBX.DL.CodeTemplates.Airflow",
  "lockfileVersion": 2,
  "requires": true,
  "packages": {}
}
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\package.json<content=>
{}
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\README.md<content=>
# docker-airflow

This repository contains **Dockerfile** of [apache-airflow](https://github.com/apache/incubator-airflow) for [Docker](https://www.docker.com/)'s [automated build](https://registry.hub.docker.com/u/puckel/docker-airflow/) published to the public [Docker Hub Registry](https://registry.hub.docker.com/).

## Information

* Based on Python (3.7-slim-buster) official Image [python:3.7-slim-buster](https://hub.docker.com/_/python/) and uses the official [Postgres](https://hub.docker.com/_/postgres/) as backend and [Redis](https://hub.docker.com/_/redis/) as queue
* Install [Docker](https://www.docker.com/)
* Install [Docker Compose](https://docs.docker.com/compose/install/)
* This follows the Airflow release from this page [Python Package Index](https://pypi.python.org/pypi/apache-airflow)

## Installation

This solution has been created by a code template.

## Build

Set any necessary environment variables in the docker-compose.yml file

* LOAD_EX can be set to "n" if you do not want to deploy the sample DAGs
* AWS credentials should be picked up by the mounted profile drive

Load AWS Credentials
``` Powershell
 kochid aws refresh
```

Compose and start a container from the docker-compose.yml file:

* docker-compose -f docker-compose.yml up

A docker container will now be visible and manageable in Docker Desktop.
The Airflow Web UI can be accessed at: [localhost:8080](http://localhost:8080/)

## Configuration

* Use the variable template DAG to set Airflow variables which your other DAGs require.  Please refer to the [yourprojectname]_variables.py template DAG
* Use the connection template DAG to set Airflow connections; connections should be named [yourprojectname]_[resource]; for example, kbxdlcodetemplatesworkflow_aws for a project named KBX.DL.CodeTemplates.Workflow.  Please refer to the [yourprojectname]_connections.py template DAG

## Management

Stop and remove the container along with any associated resources created via the aforementioned "up" command:

* docker-compose -f docker-compose.yml down

List running containers:

* docker ps

Access the shell of a running container as the default user:

* docker exec -ti "container name" bash

Access the shell of a running container as root:

* docker exec -u 0 -ti "container name" bash

## Pitfalls

* **Connections and Variables** are not persisted between containers unless you include them as environment variables in the docker-compose YAML file.
* Manually refreshing Airflow Web UI pages excessively will cause the instance to become temporarily unresponsive
* Ensure that the "dag" parameter is set for all operators to avoid vague error messages
* A DAG must be in the "On" state to be triggered by schedule or manually
* DAGs which leverage dynamic operator generation based on the results of a query will execute that query every time the "DAG bag" is filled or the DAG is accessed if the query exists within the same DAG.  The query which the dynamic operator generation relies on should exist in a separate DAG (scheduled as per requirements) where the results are written to an Airflow variable; this variable will then be referenced in the DAG containing dynamic operator generation
* Implementation of sub-DAGs requires explicitly setting the "schedule_interval" and "start_date" parameters of the parent DAG instead of passing them as part of the "default_args" parameter to avoid vague error messages
* Provider packages expose operators, hooks, and sensors but only the operators provide "out-of-the-box" functionality; resort to wiring up the hook and sensor only if an operator is unavailable or cannot meet a specific requirement
* For an unscheduled DAG either do not reference the "schedule_interval" property or set it to None (the type, not a string); for example, schedule_interval=None
* Setting the "start_date" parameter of any DAG to the current or a future date/time will cause the scheduler to fail; jobs will look as though they have started but not execute any tasks and be stuck in a "running" state
* **start_date** is counterintuitive but by design; for example, a job scheduled hourly and starting at 1400h will actually execute at 1500h.  All times in airflow are UTC and so don't execute at your local time.  Avoid using datetime.now() to offset this. Please see https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag and https://marclamberti.com/blog/apache-airflow-best-practices-1/ for further information

## References

* puckel Airflow Docker image documentation - https://hub.docker.com/r/puckel/docker-airflow
* Airflow core concepts - https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html
* Airflow best practices - https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html
* Airflow lesser-known tips and tricks - https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\requirements.txt<content=>
# KBX does not support the automation of this file, create a help request
wtforms==2.3.3
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\config\airflow.cfg<content=>
[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Users must supply an Airflow connection id that provides access to the storage
# location.
remote_log_conn_id =
remote_base_log_folder =
encrypt_s3_logs = False

# Logging level
logging_level = INFO

# Logging level for Flask-appbuilder UI
fab_logging_level = WARN

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
# Example: logging_config_class = my.path.default_local_settings.LOGGING_CONFIG
logging_config_class =

# Flag to enable/disable Colored logs in Console
# Colour the logs when the controlling terminal is a TTY.
colored_console_log = True

# Log format for when Colored logs is enabled
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d}} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Format of Log line
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Name of handler to read task instance logs.
# Default to use task handler.
task_log_reader = task

# Hostname by providing a path to a callable, which will resolve the hostname.
# The format is "package:function".
#
# For example, default value "socket:getfqdn" means that result from getfqdn() of "socket"
# package will be used as hostname.
#
# No argument should be required in the function specified.
# If using IP address as hostname is preferred, use value ``airflow.utils.net:get_host_ip_address``
hostname_callable = socket:getfqdn

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = utc

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
# sql_alchemy_conn = sqlite:////tmp/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool. 0 indicates no limit.
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
# When the number of checked-out connections reaches the size set in pool_size,
# additional connections will be returned up to this limit.
# When those additional connections are returned to the pool, they are disconnected and discarded.
# It follows then that the total number of simultaneous connections the pool will allow
# is pool_size + max_overflow,
# and the total number of "sleeping" connections the pool will allow is pool_size.
# max_overflow can be set to -1 to indicate no overflow limit;
# no limit will be placed on the total number of concurrent connections. Defaults to 10.
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite. If the number of DB connections is ever exceeded,
# a lower config value will allow the system to recover faster.
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout.
# Typically, this is a simple statement like "SELECT 1".
# More information here:
# https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic
sql_alchemy_pool_pre_ping = True

# The schema to use for the metadata database.
# SqlAlchemy supports databases with the concept of multiple schemas.
sql_alchemy_schema =

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = True

# Where your Airflow plugins are stored
plugins_folder = /usr/local/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = $FERNET_KEY

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a ``run_as_user`` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
default_impersonation =

# What security module to use (for example kerberos)
security =

# If set to False enables some unsecure features like Charts and Ad Hoc Queries.
# In 2.0 will default to True.
secure_mode = False

# Turn unit test mode on (overwrites many configuration options with test
# values at runtime)
unit_test_mode = False

# Whether to enable pickling for xcom (note that this is insecure and allows for
# RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
enable_xcom_pickling = True

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

# Whether to override params with dag_run.conf. If you pass some key-value pairs
# through ``airflow dags backfill -c`` or
# ``airflow dags trigger -c``, the key-value pairs will override the existing ones in params.
dag_run_conf_overrides_params = False

# Worker initialisation check to validate Metadata Database connection
worker_precheck = False

# When discovering DAGs, ignore any files that don't contain the strings ``DAG`` and ``airflow``.
dag_discovery_safe_mode = True

# The number of retries each task is going to have by default. Can be overridden at dag or task level.
default_task_retries = 0

# Whether to serialises DAGs and persist them in DB.
# If set to True, Webserver reads from DB instead of parsing DAG files
# More details: https://airflow.apache.org/docs/stable/dag-serialization.html
store_serialized_dags = False

# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate.
min_serialized_dag_update_interval = 30

# On each dagrun check against defined SLAs
check_slas = True

[cli]
# In what way should the cli access the API. The LocalClient will use the
# database directly, while the json_client will use the api running on the
# webserver
api_client = airflow.api.client.local_client

# If you set web_server_url_prefix, do NOT forget to append it here, ex:
# ``endpoint_url = http://localhost:8080/myroot``
# So api will look like: ``http://localhost:8080/myroot/api/experimental/...``
endpoint_url = http://localhost:8080

[debug]
# Used only with DebugExecutor. If set to True DAG will fail with first
# failed task. Helpful for debugging purposes.
fail_fast = False

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[lineage]
# what lineage backend to use
backend =

[atlas]
sasl_enabled = False
host =
port = 21000
username =
password =

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via ``default_args``
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[hive]
# Default mapreduce queue for HiveOperator tasks
default_hive_mapred_queue =

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_key =

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# Secret key used to run your flask app
# It should be as random as possible
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use. Choices include
# sync (default), eventlet, gevent
worker_class = sync

# Log files for the gunicorn webserver. '-' means log to stderr.
access_logfile = -

# Log files for the gunicorn webserver. '-' means log to stderr.
error_logfile = -

# Expose the configuration file in the web server
expose_config = True

# Expose hostname in the web server
expose_hostname = True

# Expose stacktrace in the web server
expose_stacktrace = True

# Set to true to turn on authentication:
# https://airflow.apache.org/security.html#web-authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode. Choices include user (default) and ldapgroup.
# Ldap group filtering requires using the ldap backend
#
# Note that the ldap server needs the "memberOf" overlay to be set up
# in order to user the ldapgroup mode.
owner_mode = user

# Default DAG view. Valid values are:
# tree, graph, duration, gantt, landing_times
dag_default_view = tree

# "Default DAG orientation. Valid values are:"
# LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Puts the webserver in demonstration mode; blurs the names of Operators for
# privacy.
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
# while fetching logs from other worker machine
log_fetch_timeout_sec = 5

# Time interval (in secs) to wait before next log fetching.
log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing.
log_auto_tailing_offset = 30

# Animation speed for auto tailing log display.
log_animation_speed = 1000

# By default, the webserver shows paused DAGs. Flip this to hide paused
# DAGs by default
hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
page_size = 100

# Use FAB-based webserver with RBAC feature
rbac = False

# Define the color of navigation bar
navbar_color = #007A87

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
enable_proxy_fix = False

# Number of values to trust for ``X-Forwarded-For``.
# More info: https://werkzeug.palletsprojects.com/en/0.16.x/middleware/proxy_fix/
proxy_fix_x_for = 1

# Number of values to trust for ``X-Forwarded-Proto``
proxy_fix_x_proto = 1

# Number of values to trust for ``X-Forwarded-Host``
proxy_fix_x_host = 1

# Number of values to trust for ``X-Forwarded-Port``
proxy_fix_x_port = 1

# Number of values to trust for ``X-Forwarded-Prefix``
proxy_fix_x_prefix = 1

# Set secure flag on session cookie
cookie_secure = False

# Set samesite policy on session cookie
cookie_samesite =

# Default setting for wrap toggle on DAG code and TI log views.
default_wrap = False

# Allow the UI to be rendered in a frame
x_frame_enabled = True

# Send anonymous user activity to your analytics tool
# choose from google_analytics, segment, or metarouter
# analytics_tool =

# Unique ID of your account in the analytics tool
# analytics_id =

# Update FAB permissions and sync security manager roles
# on webserver startup
update_fab_perms = True

# Minutes of non-activity before logged out from UI
# 0 means never get forcibly logged out
force_log_out_after = 0

# The UI cookie lifetime in days
session_lifetime_days = 30

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]

# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
# smtp_user =
# Example: smtp_password = airflow
# smtp_password =
smtp_port = 25
smtp_mail_from = airflow@example.com

[sentry]

# Sentry (https://docs.sentry.io) integration
sentry_dsn =

[celery]

# This section only applies if you are using the CeleryExecutor in
# ``[core]`` section above
# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# The maximum and minimum concurrency that will be used when starting workers with the
# ``airflow celery worker`` command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Example: worker_autoscale = 16,12
worker_autoscale = 16,12

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://redis:6379/1

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it ``airflow flower``. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# The root URL for Flower
# Example: flower_url_prefix = /flower
flower_url_prefix =

# This defines the port that Celery Flower runs on
flower_port = 5555

# Securing Flower with Basic Authentication
# Accepts user:password pairs separated by a comma
# Example: flower_basic_auth = user1:password1,user2:password2
flower_basic_auth =

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default

# How many processes CeleryExecutor uses to sync task state.
# 0 means to use max(1, number of cores - 1) processes.
sync_parallelism = 0

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# In case of using SSL
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

# Celery Pool implementation.
# Choices include: prefork (default), eventlet, gevent or solo.
# See:
# https://docs.celeryproject.org/en/latest/userguide/workers.html#concurrency
# https://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html
pool = prefork

# The number of seconds to wait before timing out ``send_task_to_executor`` or
# ``fetch_celery_task_state`` operations.
operation_timeout = 2

[celery_broker_transport_options]

# This section is for specifying options which can be passed to the
# underlying celery broker transport. See:
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_transport_options
# The visibility timeout defines the number of seconds to wait for the worker
# to acknowledge the task before the message is redelivered to another worker.
# Make sure to increase the visibility timeout to match the time of the longest
# ETA you're planning to use.
# visibility_timeout is only supported for Redis and SQS celery brokers.
# See:
# http://docs.celeryproject.org/en/master/userguide/configuration.html#std:setting-broker_transport_options
# Example: visibility_timeout = 21600
# visibility_timeout =

[dask]

# This section only applies if you are using the DaskExecutor in
# [core] section above
# The IP address and port of the Dask cluster's scheduler.
cluster_address = 127.0.0.1:8786

# TLS/ SSL settings to access a secured Dask scheduler.
tls_ca =
tls_cert =
tls_key =

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# After how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# The number of times to try to schedule each DAG file
# -1 indicates unlimited number
num_runs = -1

# The number of seconds to wait between consecutive DAG file processing
processor_poll_interval = 1

# after how much time (seconds) a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
dag_dir_list_interval = 300

# How often should stats be printed to the logs. Setting to 0 will disable printing stats
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
# This is used by the health check in the "/health" endpoint
scheduler_health_check_threshold = 30
child_process_log_directory = /usr/local/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is False,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
catchup_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# If this is too high, SQL query performance may be impacted by one
# or more of the following:
# - reversion to full table scan
# - complexity of query predicate
# - excessive locking
# Additionally, you may hit the maximum allowable query length for your db.
# Set this to 0 for no limit (not advised)
max_tis_per_query = 512

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# If you want to avoid send all the available metrics to StatsD,
# you can configure an allow list of prefixes to send only the metrics that
# start with the elements of the list (e.g: scheduler,executor,dagrun)
statsd_allow_list =

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
max_threads = 2
authenticate = False

# Turn off scheduler use of cron intervals by setting this to False.
# DAGs submitted manually in the web UI or with trigger_dag will still run.
use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
# Only has effect if schedule_interval is set to None in DAG
allow_trigger_in_future = False

[ldap]
# set this to ldaps://<your.ldap.server>:<port>
uri =
user_filter = objectClass=*
user_name_attr = uid
group_member_attr = memberOf
superuser_filter =
data_profiler_filter =
bind_user = cn=Manager,dc=example,dc=com
bind_password = insecure
basedn = dc=example,dc=com
cacert = /etc/ca/ldap_ca.crt
search_scope = LEVEL

# This setting allows the use of LDAP servers that either return a
# broken schema, or do not return a schema.
ignore_malformed_schema = False

[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

# Number of cpu cores required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_cpu = 1

# Memory in MB required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_memory = 256

# Enable framework checkpointing for mesos
# See http://mesos.apache.org/documentation/latest/slave-recovery/
checkpoint = False

# Failover timeout in milliseconds.
# When checkpointing is enabled and this option is set, Mesos waits
# until the configured timeout for
# the MesosExecutor framework to re-register after a failover. Mesos
# shuts down running tasks if the
# MesosExecutor framework fails to re-register within this timeframe.
# Example: failover_timeout = 604800
# failover_timeout =

# Enable framework authentication for mesos
# See http://mesos.apache.org/documentation/latest/configuration/
authenticate = False

# Mesos credentials, if authentication is enabled
# Example: default_principal = admin
# default_principal =
# Example: default_secret = admin
# default_secret =

# Optional Docker Image to run on slave before running the command
# This image should be accessible from mesos slave i.e mesos slave
# should be able to pull this docker image before executing the command.
# Example: docker_image_slave = puckel/docker-airflow
# docker_image_slave =

[kerberos]
ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
api_rev = v3

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

[elasticsearch]
# Elasticsearch host
host =

# Format of the log_id, which is used to query for a given tasks logs
log_id_template = {{dag_id}}-{{task_id}}-{{execution_date}}-{{try_number}}

# Used to mark the end of a log stream for a task
end_of_log_mark = end_of_log

# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
# Code will construct log_id using the log_id template from the argument above.
# NOTE: The code will prefix the https:// automatically, don't include that here.
frontend =

# Write the task logs to the stdout of the worker, rather than the default files
write_stdout = False

# Instead of the default log formatter, write the log lines as JSON
json_format = False

# Log fields to also attach to the json output, if enabled
json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
use_ssl = False
verify_certs = True

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository =
worker_container_tag =
worker_container_image_pull_policy = IfNotPresent

# If True (default), worker pods will be deleted upon termination
delete_worker_pods = True

# Number of Kubernetes Worker Pod creation calls per scheduler loop
worker_pods_creation_batch_size = 1

# The Kubernetes namespace where airflow workers should be created. Defaults to ``default``
namespace = default

# The name of the Kubernetes ConfigMap containing the Airflow Configuration (this file)
# Example: airflow_configmap = airflow-configmap
airflow_configmap =

# The name of the Kubernetes ConfigMap containing ``airflow_local_settings.py`` file.
#
# For example:
#
# ``airflow_local_settings_configmap = "airflow-configmap"`` if you have the following ConfigMap.
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     airflow_local_settings.py: |
#         def pod_mutation_hook(pod):
#             ...
#     airflow.cfg: |
#         ...
# Example: airflow_local_settings_configmap = airflow-configmap
airflow_local_settings_configmap =

# For docker image already contains DAGs, this is set to ``True``, and the worker will
# search for dags in dags_folder,
# otherwise use git sync or dags volume claim to mount DAGs
dags_in_image = False

# For either git sync or volume mounted DAGs, the worker will look in this subpath for DAGs
dags_volume_subpath =

# For DAGs mounted via a volume claim (mutually exclusive with git-sync and host path)
dags_volume_claim =

# For volume mounted logs, the worker will look in this subpath for logs
logs_volume_subpath =

# A shared volume claim for the logs
logs_volume_claim =

# For DAGs mounted via a hostPath volume (mutually exclusive with volume claim and git-sync)
# Useful in local environment, discouraged in production
dags_volume_host =

# A hostPath volume for the logs
# Useful in local environment, discouraged in production
logs_volume_host =

# A list of configMapsRefs to envFrom. If more than one configMap is
# specified, provide a comma separated list: configmap_a,configmap_b
env_from_configmap_ref =

# A list of secretRefs to envFrom. If more than one secret is
# specified, provide a comma separated list: secret_a,secret_b
env_from_secret_ref =

# Git credentials and repository for DAGs mounted via Git (mutually exclusive with volume claim)
git_repo =
git_branch =
git_subpath =

# The specific rev or hash the git_sync init container will checkout
# This becomes GIT_SYNC_REV environment variable in the git_sync init container for worker pods
git_sync_rev =

# Use git_user and git_password for user authentication or git_ssh_key_secret_name
# and git_ssh_key_secret_key for SSH authentication
git_user =
git_password =
git_sync_root = /git
git_sync_dest = repo

# Mount point of the volume if git-sync is being used.
# i.e. /usr/local/airflow/dags
git_dags_folder_mount_point =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-secrets.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: airflow-secrets
#   data:
#     # key needs to be gitSshKey
#     gitSshKey: <base64_encoded_data>
# Example: git_ssh_key_secret_name = airflow-secrets
git_ssh_key_secret_name =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     known_hosts: |
#         github.com ssh-rsa <...>
#     airflow.cfg: |
#         ...
# Example: git_ssh_known_hosts_configmap_name = airflow-configmap
git_ssh_known_hosts_configmap_name =

# To give the git_sync init container credentials via a secret, create a secret
# with two fields: GIT_SYNC_USERNAME and GIT_SYNC_PASSWORD (example below) and
# add ``git_sync_credentials_secret = <secret_name>`` to your airflow config under the
# ``kubernetes`` section
#
# Secret Example:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: git-credentials
#   data:
#     GIT_SYNC_USERNAME: <base64_encoded_git_username>
#     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
git_sync_credentials_secret =

# For cloning DAGs from git repositories into volumes: https://github.com/kubernetes/git-sync
git_sync_container_repository = k8s.gcr.io/git-sync
git_sync_container_tag = v3.1.1
git_sync_init_container_name = git-sync-clone
git_sync_run_as_user = 65533

# The name of the Kubernetes service account to be associated with airflow workers, if any.
# Service accounts are required for workers that require access to secrets or cluster resources.
# See the Kubernetes RBAC documentation for more:
# https://kubernetes.io/docs/admin/authorization/rbac/
worker_service_account_name =

# Any image pull secrets to be given to worker pods, If more than one secret is
# required, provide a comma separated list: secret_a,secret_b
image_pull_secrets =

# GCP Service Account Keys to be provided to tasks run on Kubernetes Executors
# Should be supplied in the format: key-name-1:key-path-1,key-name-2:key-path-2
gcp_service_account_keys =

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has.
# cluster_context =
# config_file =

# Affinity configuration as a single line formatted JSON object.
# See the affinity model for top-level key names (e.g. ``nodeAffinity``, etc.):
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#affinity-v1-core
affinity =

# A list of toleration objects as a single line formatted JSON array
# See:
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#toleration-v1-core
tolerations =

# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis.
# See:
# https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
# Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely
# for kubernetes api responses, which will cause the scheduler to hang.
# The timeout is specified as [connect timeout, read timeout]
kube_client_request_args = {{"_request_timeout" : [60,60] }}

# Specifies the uid to run the first process of the worker pods containers as
run_as_user =

# Specifies a gid to associate with all containers in the worker pods
# if using a git_ssh_key_secret_name use an fs_group
# that allows for the key to be read, e.g. 65533
fs_group =

[kubernetes_node_selectors]

# The Key-value pairs to be given to worker pods.
# The worker pods will be scheduled to the nodes of the specified key-value pairs.
# Should be supplied in the format: key = value

[kubernetes_annotations]

# The Key-value annotations pairs to be given to worker pods.
# Should be supplied in the format: key = value

[kubernetes_environment_variables]

# The scheduler sets the following environment variables into your workers. You may define as
# many environment variables as needed and the kubernetes launcher will set them in the launched workers.
# Environment variables in this section are defined as follows
# ``<environment_variable_key> = <environment_variable_value>``
#
# For example if you wanted to set an environment variable with value `prod` and key
# ``ENVIRONMENT`` you would follow the following format:
# ENVIRONMENT = prod
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_secrets]

# The scheduler mounts the following secrets into your workers as they are launched by the
# scheduler. You may define as many secrets as needed and the kubernetes launcher will parse the
# defined secrets and mount them as secret environment variables in the launched workers.
# Secrets in this section are defined as follows
# ``<environment_variable_mount> = <kubernetes_secret_object>=<kubernetes_secret_key>``
#
# For example if you wanted to mount a kubernetes secret key named ``postgres_password`` from the
# kubernetes secret object ``airflow-secret`` as the environment variable ``POSTGRES_PASSWORD`` into
# your workers you would follow the following format:
# ``POSTGRES_PASSWORD = airflow-secret=postgres_credentials``
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_labels]

# The Key-value pairs to be given to worker pods.
# The worker pods will be given these static labels, as well as some additional dynamic labels
# to identify the task.
# Should be supplied in the format: ``key = value``
<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\dags\projectnametemplatereplace_connections.py<content=>
from os import getenv
from airflow import DAG, settings
from airflow.models import Connection
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator

def ListConnections():
    return settings.Session().query(Connection)

def CreateConnections():
    try:
        # Build a connection object:
        conn = Connection(
            # conn_id: Name of the connection as displayed in the Airflow UI.
			# Snake-case; prefix with the product name.
            conn_id="projectnametemplatereplace_connDescription",
            # conn_type: The type of connection to create.
            # Valid conn_type values are: "azure_cosmos", "azure_data_lake", "cassandra", "cloudant", 
            # "docker", "gcpcloudsql", "google_cloud_platform", "grpc", "hive_cli", "hiveserver2", 
            # "jdbc", "jira", "mongo", "mssql", "mysql", "oracle", "pig_cli", "postgres", "presto",
            # "redis", "sqlite", "vertica", "wasb".
            conn_type="", 
            # host: Endpoint at which the resource exists; URL, IP address, etc.
            host="", 
            login="", 
            # Leave the password property value as-is; this will be updated via the Airflow UI.
            password="ChangeMeLater",
            # port: The port to use when creating a database type connection.
            # port=1234,
            # schema: The schema to use when creating a database type connection.
            # schema="schema"
            # extra: Used to specify additional connection type specific settings.
            # Refer to https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html for more details.
            # extra="json-formatted string"
        )

        # Get the current Airflow session:
        session = settings.Session()
        # Add the connection to the session if it doesn't already exist:
        if(conn.conn_id not in ListConnections()):
            session.add(conn)
            # Commit the newly-created connection:
            session.commit()
            return True
            
        return False
    except:
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_connections', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_connections",
        python_callable=CreateConnections
    )

    po<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\dags\projectnametemplatereplace_variables.py<content=>
from os import getenv
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator
from airflow.models import Variable

def SetVariables():
    try:
        # Multiple variables may be set via multiple Variable.set() calls.
        # This variale DAG can be run once manually for static values or leverage
        # schedules to set variable values based on the results of other operations.
        Variable.set("projectnametemplatereplace_variableDescription", 'value/values')
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_variables', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_variables",
        python_callable=SetVariables
    )

    po<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_get_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable
from ast import literal_eval

def GetVariable():
    print("Performing Variable.get() action...")
    keys=literal_eval(Variable.get("projectnametemplatereplace_variable"))
    print(keys)

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
    #'schedule_interval': '0/3 * * * ?'
}

with DAG("projectnametemplatereplace_example_get_variable", default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="get_variable",
        provide_context=False,
        python_callable=GetVariable
    )

    po<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_glue_test.py<content=>
from os import getenv

from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator

def ListGlueJobs():
    try:
        gh = AwsGlueJobHook(aws_conn_id="aws_lg_nonprod")
        print(gh.list_jobs())
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_example_glue_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_glue_jobs",
        provide_context=False,
        python_callable=ListGlueJobs
    )

    po<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_s3_conn_test.py<content=>
"""
S3 Connection Test
"""
from airflow import DAG
from airflow.hooks.base_hook import BaseHook
from airflow.operators.python_operator import PythonOperator
import boto3
from datetime import *

def ListBuckets():
    try:
        s3Conn=boto3.client("s3")
        res = s3Conn.list_buckets()
        for bkt in res["Buckets"]:
            print(bkt["Name"])

        if(len(res["Buckets"]) > 0):
            return True
        return False
    except :
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_example_s3_conn_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_s3_buckets",
        provide_context=False,
        python_callable=ListBuckets
    )

    po<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_set_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable

def SetVariable():
    keys=["projectnametemplatereplace_0","1","2","3","4","5"]
    print("Performing Variable.get() action...")
    Variable.set("projectnametemplatereplace_variable", keys)
    
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG("projectnametemplatereplace_example_set_variable", default_args=default_args) as dag:
    po = PythonOperator(task_id="set_variable",
        provide_context=False,
        python_callable=SetVariable
    )

    po<path=>ADO\Development\template\KBX.DL.CodeTemplates.Workflow\script\entrypoint.sh<content=>
#!/usr/bin/env bash

# User-provided configuration must always be respected.
#
# Therefore, this script must only derives Airflow AIRFLOW__ variables from other variables
# when the user did not provide their own configuration.

TRY_LOOP="20"

# Global defaults and back-compat
: "${AIRFLOW_HOME:="/usr/local/airflow"}"
: "${AIRFLOW__CORE__FERNET_KEY:=${FERNET_KEY:=$(python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)")}}"
: "${AIRFLOW__CORE__EXECUTOR:=${EXECUTOR:-Sequential}Executor}"

# Load DAGs examples (default: Yes)
if [[ -z "$AIRFLOW__CORE__LOAD_EXAMPLES" && "${LOAD_EX:=n}" == n ]]; then
  AIRFLOW__CORE__LOAD_EXAMPLES=False
fi

export \
  AIRFLOW_HOME \
  AIRFLOW__CORE__EXECUTOR \
  AIRFLOW__CORE__FERNET_KEY \
  AIRFLOW__CORE__LOAD_EXAMPLES \

# Install custom python package if requirements.txt is present
if [ -e "/requirements.txt" ]; then
    $(command -v pip) install --user -r /requirements.txt
fi

wait_for_port() {
  local name="$1" host="$2" port="$3"
  local j=0
  while ! nc -z "$host" "$port" >/dev/null 2>&1 < /dev/null; do
    j=$((j+1))
    if [ $j -ge $TRY_LOOP ]; then
      echo >&2 "$(date) - $host:$port still not reachable, giving up"
      exit 1
    fi
    echo "$(date) - waiting for $name... $j/$TRY_LOOP"
    sleep 5
  done
}

# Other executors than SequentialExecutor drive the need for an SQL database, here PostgreSQL is used
if [ "$AIRFLOW__CORE__EXECUTOR" != "SequentialExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the database
  if [ -z "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" ]; then
    # Default values corresponding to the default compose files
    : "${POSTGRES_HOST:="postgres"}"
    : "${POSTGRES_PORT:="5432"}"
    : "${POSTGRES_USER:="airflow"}"
    : "${POSTGRES_PASSWORD:="airflow"}"
    : "${POSTGRES_DB:="airflow"}"
    : "${POSTGRES_EXTRAS:-""}"

    AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
    export AIRFLOW__CORE__SQL_ALCHEMY_CONN

    # Check if the user has provided explicit Airflow configuration for the broker's connection to the database
    if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
      AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
      export AIRFLOW__CELERY__RESULT_BACKEND
    fi
  else
    if [[ "$AIRFLOW__CORE__EXECUTOR" == "CeleryExecutor" && -z "$AIRFLOW__CELERY__RESULT_BACKEND" ]]; then
      >&2 printf '%s\n' "FATAL: if you set AIRFLOW__CORE__SQL_ALCHEMY_CONN manually with CeleryExecutor you must also set AIRFLOW__CELERY__RESULT_BACKEND"
      exit 1
    fi

    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    POSTGRES_ENDPOINT=$(echo -n "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" | cut -d '/' -f3 | sed -e 's,.*@,,')
    POSTGRES_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    POSTGRES_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Postgres" "$POSTGRES_HOST" "$POSTGRES_PORT"
fi

# CeleryExecutor drives the need for a Celery broker, here Redis is used
if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the broker
  if [ -z "$AIRFLOW__CELERY__BROKER_URL" ]; then
    # Default values corresponding to the default compose files
    : "${REDIS_PROTO:="redis://"}"
    : "${REDIS_HOST:="redis"}"
    : "${REDIS_PORT:="6379"}"
    : "${REDIS_PASSWORD:=""}"
    : "${REDIS_DBNUM:="1"}"

    # When Redis is secured by basic auth, it does not handle the username part of basic auth, only a token
    if [ -n "$REDIS_PASSWORD" ]; then
      REDIS_PREFIX=":${REDIS_PASSWORD}@"
    else
      REDIS_PREFIX=
    fi

    AIRFLOW__CELERY__BROKER_URL="${REDIS_PROTO}${REDIS_PREFIX}${REDIS_HOST}:${REDIS_PORT}/${REDIS_DBNUM}"
    export AIRFLOW__CELERY__BROKER_URL
  else
    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    REDIS_ENDPOINT=$(echo -n "$AIRFLOW__CELERY__BROKER_URL" | cut -d '/' -f3 | sed -e 's,.*@,,')
    REDIS_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    REDIS_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Redis" "$REDIS_HOST" "$REDIS_PORT"
fi

case "$1" in
  webserver)
    airflow initdb
    if [ "$AIRFLOW__CORE__EXECUTOR" = "LocalExecutor" ] || [ "$AIRFLOW__CORE__EXECUTOR" = "SequentialExecutor" ]; then
      # With the "Local" and "Sequential" executors it should all run in one container.
      airflow scheduler &
    fi
    exec airflow webserver
    ;;
  worker|scheduler)
    # Give the webserver time to run initdb.
    sleep 10
    exec airflow "$@"
    ;;
  flower)
    sleep 10
    exec airflow "$@"
    ;;
  version)
    exec airflow "$@"
    ;;
  *)
    # The command is something like bash, not an airflow subcommand. Just run it in the right environment.
    exec "$@"
    ;;
esac
<path=>.\ADO\Extraction<content=>
<path=>.\ADO\Feasiblity<content=>
<path=>.\ADO\Mapping<content=>
<path=>ADO\Testing\adoapi_without_sdk.py<content=>
#%%
import requests
import pandas
import math
import concurrent.futures as cf

ado_user = ''
ado_personal_access_token = 'lxturceksjcxfqgqli5qjm57hit7xnuykkf7ig7kqswoz272okpq'

table = 'workitems'
base_api_url = f'https://analytics.dev.azure.com/kbxltrans/_odata/v3.0/Projects'
headers = {"Content-Type":"application/json","Accept":"application/json"}

response =  requests.get(url=base_api_url, auth=(ado_user, ado_personal_access_token), headers=headers)

# print (response.status_code)
# print (response.json())
#%%
class Ingest() :
    def __init__(self, base_url, organization, project, table, max_record_per_request, userid, password, headers) -> None:
        self.userid = userid
        self.password = password
        self.url_count = '/'.join((base_url, organization, project, '_odata', 'v3.0', table))
        self.url_data = '/'.join((base_url, organization, project, '_odata', 'v3.0', table))
        self.max_record_per_request = max_record_per_request
        self.headers = headers

    def count_api_response(self, params) :
        response =  requests.get(url=self.url_count, auth=(self.userid, self.password), params=params, headers=self.headers)
        if response.json()['value'] != [] :
            countapiresponse = response.json()['value'][0]['Count']
        else :
            countapiresponse = 0
        countapiresponse = 1
        self.countapiresponse = countapiresponse
        return countapiresponse
    
    def number_of_request(self, count_params) :
        numberofrequest = math.ceil(self.count_api_response(count_params)/self.max_record_per_request)
        if numberofrequest == 0 :
            numberofrequest = 1
        self.numberofrequest = numberofrequest
        return numberofrequest

    def get_data(self, data_params, count_params) :
        def send_request(params) :
            response = requests.get(url=self.url_data, params=params, auth=(self.userid, self.password), headers=self.headers)
            if response.status_code != 200 :
                #job.logger().critical(f, f"api response status code : {response.status_code}")
                raise Exception(f"api response status code : {response.status_code}")
            else :
                #job.logger().info(f, f"DATA COUNT {len(response.json()['value'])}")
                #job.logger().info(f, f"api headers {response.headers}")
                return response

        params_params = []
        co = self.number_of_request(count_params)
        for n in range(co) :
            parameters = data_params.copy()
            parameters['$skip'] = int(self.max_record_per_request)*n
            params_params.append(parameters)

        with cf.ThreadPoolExecutor() as executor :
            results = executor.map(send_request, params_params)
        
        #job.logger().info(f, f'##### All requests Response Received #####')

        #job.logger().info(f, f'##### Validate initial_total_count of each response and append data #####')
        response = next(results)
        #job.logger().info(f, f'requested_url : {response.url}')
        data = response.json()
        initial_total_count = self.countapiresponse
        #job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['value'])}")
        #job.logger().info(f, f"initial_total_count from parallel response: {initial_total_count}")
        #job.logger().info(f, f"Total Data Appended : {len(data['value'])}")        

        for response in results :
            #job.logger().info(f, f'requested_url : {response.url}')
            data['value'].extend(response.json()['value'])
            #job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['value'])}")
            #job.logger().info(f, f"initial_total_count from parallel response: {initial_total_count}")
            #job.logger().info(f, f"Total Data Appended : {len(data['value'])}")
        
        if len(data['value']) != int(initial_total_count) :
            #job.logger().info(f, f"SOME DATA MISSED IN SOME RESPONSE")
            raise Exception(f"ALL DATA NOT RECEIVED")
        else :
            #job.logger().info(f, f"ALL DATA RECEIVED")
            pass

        self.data = data
        return data

#%%
url_project = 'https://analytics.dev.azure.com/kbxltrans/_odata/v3.0/Projects?$Select=ProjectName'
response =  requests.get(url=url_project, auth=(ado_user, ado_personal_access_token), headers=headers)
projects = [project['ProjectName'] for project in response.json()['value']]

# %%
max_record_per_request = 10
OFFSET = 0
data_params = {
    "$expand" : "Project($select=ProjectName), Area($select=AreaName), AssignedTo($select=UserName), ChangedBy($select=UserName), ClosedBy($select=UserName), CreatedBy($select=UserName), ActivatedBy($select=UserName), Iteration ",
    "$top" : f'{max_record_per_request}',
    "$skip" : OFFSET,
    "$orderby" : "StateChangeDate asc",
    "$count" : "true"
}
count_params={
    '$apply' : 'aggregate($count as Count)'
}

#%%
obj = {}
data = {}

organization = 'kbxltrans'
for project in projects :
    print (project)
    o = Ingest(base_url=base_api_url, organization=organization, project=project, table=table, max_record_per_request=max_record_per_request, userid=ado_user, password=ado_personal_access_token, headers = headers)
    obj[project] = {'object' : o, 'data' : o.get_data(data_params=data_params, count_params=count_params)}
    data[project] = o.data
# %%
<path=>ADO\Testing\ado_ingest_incident.py<content=>
from azure.devops.connection import Connection
from msrest.authentication import BasicAuthentication
import pprint

# Fill in with your personal access token and org URL
personal_access_token = 'lxturceksjcxfqgqli5qjm57hit7xnuykkf7ig7kqswoz272okpq'
organization_url = 'https://dev.azure.com/kbxltrans'

# Create a connection to the org
credentials = BasicAuthentication('', personal_access_token)
connection = Connection(base_url=organization_url, creds=credentials)

# Get a client (the "core" client provides access to projects, teams, etc)
core_client = connection.clients.get_core_client()

# Get the first page of projects
get_projects_response = core_client.get_projects()
index = 0
while get_projects_response is not None:
    for project in get_projects_response.value:
        pprint.pprint("[" + str(index) + "] " + project.name)
        index += 1
    if get_projects_response.continuation_token is not None and get_projects_response.continuation_token != "":
        # Get the next page of projects
        get_projects_response = core_client.get_projects(continuation_token=get_projects_response.continuation_token)
    else:
        # All projects have been retrieved
        get_projects_response = None<path=>ADO\Testing\getAPI.txt<content=>
https://analytics.dev.azure.com/kbxltrans/KBXTrack/_odata/v3.0/workitems?$select=AssignedToUserSK&$expand=Project($select=ProjectName), Area($select=AreaName), AssignedTo($select=UserName)<path=>ADO\Testing\token.txt<content=>
yndoipjw6yuopo5gu3ger3j7l2jwjrso52hfruarfllc75xzldoa

ado all
lxturceksjcxfqgqli5qjm57hit7xnuykkf7ig7kqswoz272okpq<path=>.\ADO\Tmp<content=>
<path=>KbxtDlPy\Harness.py<content=>
#%% job
import os
from KbxtDlPy._logger import InternalLogger 
from KbxtDlPy._cursor import Cursor

# file
f = os.path.basename(__file__)

class Logging:
  def __init__(self, name, level = 'INFO'):
    """ Bootstrapper """    
    self.__logger = InternalLogger(name, level)
    self.__logger.debug(f, "Logger init entered.")

  def logger(self):
    return self.__logger

class Job:   
  """
  Job implementation.

  @param name: The job name, usually the calling application.
  @param level: The logging level, ['CRITICAL','ERROR','WARNING','INFO','DEBUG'].
  @param protocol: The protocol for file access, ['s3a','s3n'].
  """

  def __init__(self, name, level, protocol="s3a"):
    """ Bootstrapper """    
    self.__logger = InternalLogger(name, level)
    self.__job = Cursor(self.__logger, protocol)
    self.__logger.debug(f, "Job init entered.")
    
  def logger(self):
    self.__logger.debug(f, "Job logger entered.")
    return self.__logger
  
  def runtime(self):
    self.__logger.debug(f, "Job runtime entered.")
    return self.__job
        
# %%
<path=>KbxtDlPy\_cursor.py<content=>
#%% cursor
import io
import botocore
import boto3
import os
from pyspark.sql.types import StructType, StructField, StringType, LongType
import json
import awswrangler as wr
from datetime import datetime, timezone

# file
f = os.path.basename(__file__)

class Cursor: 
  """
  Used to provide a job a window of data and saves inter-partition state.

  @param logger: The logger instance.
  """
  
  def __init__(self, logger, protocol="s3a"):
    self.__spark = None
    self.__bucket = ""
    self.__prefix = "run-"
    self.__partition = ""
    self.__is_replay = False
    self.__file_format = ""
    self.__include_header = False
    self.__folder_url = "" 
    self.__lock_key = ""
    self.__index_key = ""
    self.__client = None
    self.__resource = None
    self.__logger = logger
    self.__logger.debug(f, "Cursor init entered.")
    self.__protocol = protocol
    self.__path = ""
  
  def __getCursorBatch(self, prefix, bucketName, startAfter="", maxKeys = 1000):
    keys = []

    self.__logger.debug(f, "Cursor __getCursorBatch entered. bucketName:{}, prefix:{}, startAfter:{}".format(bucketName, prefix, str(startAfter)))
    try:
      paginator = self.__client.get_paginator("list_objects_v2")
      respIter = paginator.paginate(Bucket=bucketName, Prefix=prefix, StartAfter="{}{}".format(prefix, startAfter), PaginationConfig =  {"PageSize": maxKeys})

      for page in respIter:
        if("Contents" in page):
          resp = page["Contents"]
          for obj in resp:
              keys.append(obj['Key'])

    except KeyError as e:
      self.__logger.debug(f, "Cursor __getCursorBatch errored. exception {}".format(e))
      keys=[]

    return keys

  def __get_s3_keys(self, bucket, prefix, start_after=""):    
    """ Gets s3 keys of all files in bucket and folder (prefix) """
    self.__logger.debug(f, "Cursor __get_s3_keys entered. bucket:{}, prefix:{}, start_after:{}".format(bucket, prefix, str(start_after)))

    keys = []
    # Don't page results, that way we can enforce the limitations of 10,000 records per file,
    # and 1,000 files per run to protect memory.
    resp = self.__client.list_objects_v2(
        Bucket=bucket, 
        Prefix=prefix, 
        StartAfter=start_after
      )
    try:
        for obj in resp['Contents']:
            keys.append(obj['Key'])
    except Exception as e:
        self.__logger.debug(f, "Cursor __get_s3_keys errored. exception {}".format(e))
        keys = []
    return keys

  def _getS3PathParts(self, path):
    """
    Splits an S3 URL into a bucket and key strings.

    @param path: The full (non-ARN) S3 path.
    """
    #Regex replacement would be better
    pathParts = path.replace("s3://", "").replace("s3a://", "").replace("s3n://", "").split("/")
    pRes = {
        "Bucket": pathParts.pop(0),
        "Key": ("/".join(pathParts))
    }
    return pRes

  def _isBucketExists(self, bucketName):
    try:
      self.__resource.meta.client.head_bucket(Bucket=bucketName)
    
    except botocore.exceptions.ClientError as e:
      return False
    
    return True

  def _uploadFile(self, inputStream, filePath, contentType, bucketName):
    self.__logger.debug(f, "Entering _uploadFile: filePath: {}, contentType: {}, bucketName: {}".format(filePath, contentType, bucketName))  
    if (not self._isBucketExists(bucketName)):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = self.__resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream, ContentType=contentType)

    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
      self.__logger.debug(f, "File uploaded at {}".format(filePath))
      return True

    return False

  def _getFile(self, path, bucketName):
    self.__logger.debug(f, "Entering _getFile: lockPath: {}, bucketName: {}".format(path, bucketName))
    try:
      resp = self.__client.list_objects_v2(
        Bucket=bucketName, 
        Prefix=path
      )
      if(("Contents" in resp) and len(resp["Contents"]) > 0):
        return resp["Contents"][0]["Key"]
    
    except Exception as e:
        self.__logger.debug(f, "Cursor _getFile errored. exception {}".format(e))
    
    return None

  def _deleteFile(self, lockPath, bucketName):
    self.__logger.debug(f, "Entering _deleteFile: lockPath: {}, bucketName: {}".format(lockPath, bucketName))
    try: 
      self.__client.delete_object(Bucket=bucketName, Key=lockPath)
    
    except botocore.exceptions.ClientError as e:
      return False

    return True

  def __lock(self, transactionName, lockPath, bucketName):
    self.__logger.debug(f, "Entering _lock: transactionName: {}, lockPath: {}, bucketName: {}".format(transactionName, lockPath, bucketName))
    inputStream = io.BytesIO(str.encode(transactionName))
    res = self._uploadFile(inputStream, lockPath, "text/plain", bucketName)
    inputStream.close()
    return res

  def __unlock(self, lockPath, bucketName):
    self.__logger.debug(f, "Entering _unlock: lockPath: {}, bucketName: {}".format(lockPath, bucketName))
    return self._deleteFile(lockPath, bucketName)

  def __locked(self, lockPath, bucketName):
    self.__logger.debug(f, "Entering _isLocked: lockPath: {}, bucketName: {}".format(lockPath, bucketName))
    lockRes = self._getFile(lockPath, bucketName)
    if(lockRes == None):
      return False

    return True
  
  def __cursor(self):
    """ Get cursor file if it exists, forward only for the Date """
    self.__logger.debug(f, "Cursor __cursor entered.")

    cursor_last_processed = None
    if (self.__is_replay is False):
        cursor_last_processed = self.__get_s3_keys(
            bucket=self.__bucket,
            prefix=self.__index_key
            )
        if (len(cursor_last_processed) > 0):
            key = self._getFile(self.__path, self.__bucket)
            obj = self.__resource.Object(self.__bucket, key)
            cursor_last_processed = obj.get()['Body'].read().decode('utf-8')
        else: 
            cursor_last_processed = None
    return cursor_last_processed

  def __advance_cursor(self, max_key):
    """ Move cursor to last file processed """
    self.__logger.debug(f, "Cursor __advance_cursor entered.")
    key = max_key[max_key.rfind("/")+1:]

    self.__client.delete_object(
            Bucket=self.__bucket,
            Key=self.__index_key
          )

    inputStream = io.BytesIO(str.encode(key))
    self._uploadFile(inputStream, self.__index_key, "text/plain", self.__bucket)
    inputStream.close()
  def start(self, spark, bucket, prefix, partition, is_replay, file_format, path="", schema_json=None, includes_header=True):
    """
    Get the data frame and move forward the cursor if it isn't backfilling or replaying a Date.

    @param spark: [string] SparkSession this job is using.
    @param bucket: [string] Bucket name.
    @param prefix: [string] File prefix.
    @param partition: [string] Partition to run for, such as ingest_date=2020-11-11.
    @param is_replay: [Boolean] Signifies if the Date isn't the current Date.
    @param file_format: [string] File format to be processed. Allowed values: csv, parquet, json (case sensitive).
    @param path: [string] The Path, including internal deliminators (this/is/a/path), to the partition.
    @param includes_header: [Boolean] Flag indicating whether a header row is present in the files being read.
    @return: [Spark] Data frame to process.
    """ 
    self.__logger.info(f, "Job start.")

    # Variables
    self.__spark = spark
    self.__bucket = bucket
    self.__prefix = prefix
    self.__path = path
    self.__partition = partition
    self.__is_replay = is_replay
    self.__file_format = file_format
    self.__includes_header = includes_header 
    # path shouldn't contain double //
    if (len(self.__path) == 0):
      self.__lock_key = "_cursor/_lock"
      self.__index_key = "_cursor/_index"
      self.__folder_url = "{}://{}/{}".format(self.__protocol, self.__bucket, self.__partition) 
    else:
      self.__lock_key = "{}/_cursor/_lock".format(self.__path)
      self.__index_key = "{}/_cursor/_index".format(self.__path)
      self.__folder_url = "{}://{}/{}/{}".format(self.__protocol, self.__bucket, self.__path, self.__partition)    
    self.__client = boto3.client('s3')
    self.__resource = boto3.resource("s3")
    
    if(self.__locked(self.__lock_key, self.__bucket)):
      raise Exception("Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.")
    if(not self.__lock(self.__folder_url, self.__lock_key, self.__bucket)):
      raise Exception("Cannot begin transaction; unable to lock the cursor")   

    # Retrieve cursor
    self.__logger.debug(f, "Cursor retrieve cursor.")
    cursor = self.__cursor()

    # Get window to process
    self.__logger.debug(f, "Cursor stage data to process.")
    keys_to_process = self.__getCursorBatch(("{}/{}/{}".format(self.__path, self.__partition, self.__prefix).lstrip("/")), self.__bucket, ("" if (cursor is None) else cursor))
    
    # Spark dataframe
    df = None    
    # Do processing files exist
    if (len(keys_to_process) > 0):
      self.__logger.debug(f, "Cursor loading data frame.")

      for i, k in enumerate(keys_to_process):
        keys_to_process[i] = "{}://{}/{}".format(self.__protocol, self.__bucket, k)

      try:        
        # Get dataframe
        if(schema_json is None):
          if(self.__file_format=="csv" and self.__includes_header):
            df = self.__spark.read \
              .option("header","true") \
              .format(self.__file_format) \
              .load(keys_to_process)
          else:
            df = self.__spark.read \
            .format(self.__file_format) \
            .load(keys_to_process)
        else:
          df = self.__spark.read \
            .format("json") \
            .schema(StructType.fromJson(json.loads(schema_json))) \
            .load(keys_to_process)

        # Move cursor to max file just processed 
        if (self.__is_replay is False):       
          self.__advance_cursor(max(keys_to_process))
      except Exception as e:
        self.__logger.critical(f, "{}".format(e))
        raise e  

    return df

  def end(self):
    """
    Removes the temporary files processed.
    """ 
    self.__logger.info(f, "Job end.")
    self.__unlock(self.__lock_key, self.__bucket)

  def commit(self, df, name, location, filename_prefix = None):
    """
      Validates then writes the dataframe.

    @param df: [dataframe] Spark dataframe to write.
    @param name: [string] The name of the dataframe, such as shipment.
    @param location: [string] Path to write to.
    @return: [bool] True if dataframe was written.
    """ 
    self.__logger.debug(f, "Job commit.")

    dt = datetime.now(timezone.utc)
    newFileName = str(dt).replace(".", "").replace("-","").replace(" ","").replace("+00:00","").replace(":","")
    if(filename_prefix is not None):
      newFileName = "{}.{}".format(newFileName, filename_prefix)
    newFileName = "{}.parquet".format(newFileName)

    try:
      pp = self._getS3PathParts(location)      
      pdf = df.toPandas()
      wr.s3.to_parquet(
        df = pdf,
        #path = "{}://{}/{}/{}".format(self.__protocol, pp["Bucket"], pp["Key"], newFileName)
        path = "{}://{}/{}/{}".format("s3", pp["Bucket"], pp["Key"], newFileName)
      )
    except Exception as e:
      self.__logger.critical(f, e)
      raise Exception("Unable to save dataframe {}, {}:{}".format(name, "b36f99b1-cb9d-4ec6-8e72-e004fb55f534", e))
    
    return True

# %%<path=>KbxtDlPy\_logger.py<content=>
#%% logger
import os

# file
f = os.path.basename(__file__)

def get_logger(name, level):
    import logging
    import sys

    logger = logging.getLogger(name)
    logger.setLevel(level)
    if logger.handlers:
       pass
    else:
        ch = logging.StreamHandler(sys.stdout)
        ch.setLevel(level)
        formatter = logging.Formatter('%(asctime)s:%(name)s-%(levelname)s,%(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
    return logger

class InternalLogger:    
    """
    Logger implementation.

    @param name: The logger name, usually the calling application.
    @param name: The logging level, ['CRITICAL','ERROR','WARNING','INFO','DEBUG'].
    """

    def __init__(self, name, level):
        self.__logger = get_logger(name=name, level=level)

    @staticmethod
    def __format(file, thumbprint, message):
        return ("" if (file is None) else "{}:{}:{}".format(file, thumbprint, message))

    def debug(self, file, message):
        self.__logger.debug(self.__format(file, "a058e8d7-f53a-4664-8aec-485b0fc96602", message))

    def info(self, file, message):
        self.__logger.info(self.__format(file, "0aa3feec-f870-475a-8392-b6ed9d0c2804", message))

    def warning(self, file, message):
        self.__logger.warning(self.__format(file, "3166e5d1-3d0d-43b9-9a95-ea2462e3ae95", message))

    def error(self, file, message):
        self.__logger.error(self.__format(file, "e74149ae-fce3-4f45-a88e-7c9781c73b17", message))

    def critical(self, file, message):
        self.__logger.critical(self.__format(file, "80b6b08a-9d96-4d18-9f5f-c4c710dd3d98", message))

# %%
<path=>KbxtDlPy\__init__.py<content=>

<path=>SERVICENOW\Development\athena\New Text Document.txt<content=>
SHOW PARTITIONS kbxt_dl_analytics_db_structured_dev.servicenow_task

ALTER TABLE kbxt_dl_analytics_db_structured_dev.servicenow_task
DROP PARTITION (ingest_date = '2022-07-25', country = 'IN');

ALTER TABLE kbxt_dl_analytics_db_structured_dev.servicenow_task ADD
  PARTITION (ingest_date = "2022-07-25");<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.ServiceNow.Incident.Curated"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.ServiceNow.Incident.Curated"
$replacementEntityName = "INCIDENT"
$replacementEntityNameLower = "incident"
$replacementEntityNamePascal = "Incident"
$replacementEntityNameUpper = "INCIDENT"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlservicenowincident"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\README.md<content=>
KBX.Analytics.DL.ServiceNow.Incident.Curated
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'incident' # Determined by CodeTemplate ProjectName parameter.
  domain: 'servicenow'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Incident.Curated/KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Incident.Curated/KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Incident.Curated/KBX.Analytics.DL.ServiceNow.Incident.Curated.Jobs
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Incident.Curated/KBX.Analytics.DL.ServiceNow.Incident.Curated.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.0.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain  
      }

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Jobs\transform.py<content=>
#%% transform
import os
import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import argparse
import boto3
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.now()
client = boto3.client('glue')
# file
f = os.path.basename(__file__)

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.TaskCurated.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%% Init
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--schema_json', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
#date_partition_override = "ingest_date=2022-09-06"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-servicenow-incident-structured-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-servicenow-incident-curated-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'incident' # delete
schema_json = args.schema_json
# schema_json = None

# Prefix of files to process, in case files need to be excluded
file_prefix = "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'incident'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'
JOB_NAME = args.JOB_NAME

# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATING_INGEST_DATE_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition {date_partition}')


#%% JOB START
# All files for a date partition that haven't been processed are 
# returned, so be cognizent of the size of this dataframe.
# json_schema parameter is optional; the dataframe schema is inferred when this parameter is not supplied.
job.logger().info(f, f'###################_TASK-2_JOB_START_READ_DATAFRAME_###################')

filter_clause = f"ingest_date='{date_partition.split('=')[1]}'"
job.logger().info(f, f'filter_clause {filter_clause}')
df = spark.sql(f'''
    select 
        *
    from 
        {structured_db}.{tbl}
    where 
        {filter_clause}
''')
job.logger().info(f, f'df.show() {df.show(5)}')
job.logger().info(f, f'df.printSchema() {df.printSchema()}')

#%% LAST CURATED INGEST DATE
job.logger().info(f, f'###################_TASK-3_LAST_CURATED_INGEST_DATE_###################')
try :
    last_curated = spark.sql(f'''
    select 
        distinct ingest_date 
    from 
        {curated_db}.{tbl}
    where
        ingest_date < '{date_partition.split('=')[1]}'
    order by ingest_date desc
    ''')
    last_curated.show()
    last_curated = last_curated.collect()[0][0]
    last_curated_ingest_date = f"ingest_date='{last_curated}'"
    job.logger().info(f, f'last_curated_partition_to_process {last_curated_ingest_date}')
    curated = True
    job.logger().info(f, f'curated {curated}')

except Exception as e:
    job.logger().info(f, e)
    curated = False
    job.logger().info(f, f'curated {curated}')


#%% Job Process
try:
    # df_init = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=schema_json)
    df_init = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=None, file_format='parquet')
    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-4_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])  

        # dataframe to commit
        df_tmp = df.drop(col('ingest_date')).drop(col('job_id')).distinct()
        df_tmp.printSchema()
        df_tmp.createOrReplaceTempView('structured_data')
        
        
        if not curated :
            df_curated = spark.sql(f'''
            select
                {JOB_ID}            job_id,
                source,
                number,
                state,
                u_escalate,
                priority,
                assigned_to,
                sys_updated_by,
                description,
                short_description,
                cmdb_ci,
                assignment_group,
                sys_created_on,
                sys_updated_on,
                resolved_at,
                closed_at,
                'incident'          table_name,
                close_code,
                u_type_of_fix 
            from 
                structured_data
            where 
                assignment_group = 'KBXL Systems Coordinators'
                or
                assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
            ''')
        
        
        else :
            df_curated = spark.sql(f'''
            select
                {JOB_ID}          job_id,
                source,
                number,
                state,
                u_escalate,
                priority,
                assigned_to,
                sys_updated_by,
                description,
                short_description,
                cmdb_ci,
                assignment_group,
                sys_created_on,
                sys_updated_on,
                resolved_at,
                closed_at,
                'incident'         table_name,
                close_code,
                u_type_of_fix 
            from 
                structured_data
            where 
                assignment_group = 'KBXL Systems Coordinators'
                or
                assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
                
                union
                
                select
                    {JOB_ID}          job_id,
                    source,
                    number,
                    state,
                    u_escalate,
                    priority,
                    assigned_to,
                    sys_updated_by,
                    description,
                    short_description,
                    cmdb_ci,
                    assignment_group,
                    sys_created_on,
                    sys_updated_on,
                    resolved_at,
                    closed_at,
                    table_name,
                    close_code,
                    u_type_of_fix
                from
                    {curated_db}.{tbl}
                where
                    {last_curated_ingest_date}
                    and
                    number not in ( select 
                                        number
                                    from 
                                        structured_data
                                        )
            ''')
        
        df_curated.show(20)
        df_transformed = df_curated
        
        
        # Commit files
        job.logger().info(f, f'###################_TASK-5_COMMIT_FILE_###################')
        job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))

        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-6_REFRESH_PARTITION/RUN_CRAWLER_###################')
        
        df_table = spark.sql(f'''show tables in {curated_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {curated_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
            except Exception as e:
                job.logger().info(f, e)
        else :
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
                    state_previous = state
        job.logger().info(f, f'###################_TASK-7_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-7_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%% Job End

job.runtime().end()

# %%<path=>.\SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated\KBX.Analytics.DL.ServiceNow.Incident.Curated.Jobs\lib<content=>
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\README.md<content=>
KBX.Analytics.DL.ServiceNow.Task.Curated
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'task' # Determined by CodeTemplate ProjectName parameter.
  domain: 'servicenow'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Task.Curated/KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Task.Curated/KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Task.Curated/KBX.Analytics.DL.ServiceNow.Task.Curated.Jobs
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Task.Curated/KBX.Analytics.DL.ServiceNow.Task.Curated.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain        
      }

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Jobs\transform.py<content=>
#%% transform
import os
import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import argparse
import boto3
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.now()
client = boto3.client('glue')
# file
f = os.path.basename(__file__)

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.TaskCurated.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%% Init
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--schema_json', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
#date_partition_override = "ingest_date=2022-09-06"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-servicenow-task-structured-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-servicenow-task-curated-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'task' # delete
schema_json = args.schema_json
# schema_json = None

# Prefix of files to process, in case files need to be excluded
file_prefix = "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'task'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'
JOB_NAME = args.JOB_NAME

# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATING_INGEST_DATE_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition {date_partition}')


#%% JOB START
# All files for a date partition that haven't been processed are 
# returned, so be cognizent of the size of this dataframe.
# json_schema parameter is optional; the dataframe schema is inferred when this parameter is not supplied.
job.logger().info(f, f'###################_TASK-2_JOB_START_READ_DATAFRAME_###################')

filter_clause = f"ingest_date='{date_partition.split('=')[1]}'"
job.logger().info(f, f'filter_clause {filter_clause}')
df = spark.sql(f'''
    select 
        *
    from 
        {structured_db}.{tbl}
    where 
        {filter_clause}
''')
job.logger().info(f, f'df.show() {df.show(5)}')
job.logger().info(f, f'df.printSchema() {df.printSchema()}')

#%% LAST CURATED INGEST DATE
job.logger().info(f, f'###################_TASK-3_LAST_CURATED_INGEST_DATE_###################')
try :
    last_curated = spark.sql(f'''
    select 
        distinct ingest_date 
    from 
        {curated_db}.{tbl}
    where
        ingest_date < '{date_partition.split('=')[1]}'
    order by ingest_date desc
    ''')
    last_curated.show()
    last_curated = last_curated.collect()[0][0]
    last_curated_ingest_date = f"ingest_date='{last_curated}'"
    job.logger().info(f, f'last_curated_partition_to_process {last_curated_ingest_date}')
    curated = True
    job.logger().info(f, f'curated {curated}')

except Exception as e:
    job.logger().info(f, e)
    curated = False
    job.logger().info(f, f'curated {curated}')


#%% Job Process

try:
    # df_init = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=schema_json)
    df_init = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=None, file_format='parquet')
    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-4_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])  

        # dataframe to commit
        df_tmp = df.drop(col('ingest_date')).drop(col('job_id')).distinct()
        df_tmp.printSchema()
        df_tmp.createOrReplaceTempView('structured_data')
        
        
        if not curated :
            df_curated = spark.sql(f'''
                select
                    {JOB_ID}          job_id,
                    source,
                    number,
                    state,
                    u_escalate,
                    priority,
                    assigned_to,
                    sys_updated_by,
                    description,
                    short_description,
                    cmdb_ci,
                    assignment_group,
                    sys_created_on,
                    sys_updated_on,
                    closed_at,
                    'task'         table_name,
                    case 
                        when locate('<!u_type_of_fix>', description) <> 0 and locate('<u_type_of_fix>', description) <> 0
                            then substring(description, locate('<u_type_of_fix>', description)+length('<u_type_of_fix>'), locate('<!u_type_of_fix>', description)-locate('<u_type_of_fix>', description)-length('<!u_type_of_fix>'))
                        else '' 
                    end u_type_of_fix,
                    case 
                        when locate('<!resolution_description>', description) <> 0 and locate('<resolution_description>', description) <> 0
                            then substring(description, locate('<resolution_description>', description)+length('<resolution_description>'), locate('<!resolution_description>', description)-locate('<resolution_description>', description)-length('<!resolution_description>'))
                        else '' 
                    end resolution_description 
                from 
                    structured_data
                where 
                    assignment_group = 'KBXL Systems Coordinators'
                    or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
            ''')
        
        
        else :
            df_curated = spark.sql(f'''
                select
                    {JOB_ID}          job_id,
                    source,
                    number,
                    state,
                    u_escalate,
                    priority,
                    assigned_to,
                    sys_updated_by,
                    description,
                    short_description,
                    cmdb_ci,
                    assignment_group,
                    sys_created_on,
                    sys_updated_on,
                    closed_at,
                    'task'         table_name,
                    case 
                        when locate('<!u_type_of_fix>', description) <> 0 and locate('<u_type_of_fix>', description) <> 0
                            then substring(description, locate('<u_type_of_fix>', description)+length('<u_type_of_fix>'), locate('<!u_type_of_fix>', description)-locate('<u_type_of_fix>', description)-length('<!u_type_of_fix>'))
                        else '' 
                    end u_type_of_fix,
                    case 
                        when locate('<!resolution_description>', description) <> 0 and locate('<resolution_description>', description) <> 0
                            then substring(description, locate('<resolution_description>', description)+length('<resolution_description>'), locate('<!resolution_description>', description)-locate('<resolution_description>', description)-length('<!resolution_description>'))
                        else '' 
                    end resolution_description 
                from 
                    structured_data
                where 
                    assignment_group = 'KBXL Systems Coordinators'
                    or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
                
                union
                
                select
                    {JOB_ID}          job_id,
                    source,
                    number,
                    state,
                    u_escalate,
                    priority,
                    assigned_to,
                    sys_updated_by,
                    description,
                    short_description,
                    cmdb_ci,
                    assignment_group,
                    sys_created_on,
                    sys_updated_on,
                    closed_at,
                    table_name,
                    u_type_of_fix,
                    resolution_description 
                    from
                        {curated_db}.{tbl}
                    where
                        {last_curated_ingest_date}
                        and
                        number not in ( select 
                                            number
                                        from 
                                            structured_data
                                            )
                ''')
        
        df_curated.show(20)
        df_transformed = df_curated
        
        
        # Commit files
        job.logger().info(f, f'###################_TASK-5_COMMIT_FILE_###################')
        job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))

        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-6_REFRESH_PARTITION/RUN_CRAWLER_###################')
        
        df_table = spark.sql(f'''show tables in {curated_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {curated_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
            except Exception as e:
                job.logger().info(f, e)
        else :
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
                    state_previous = state
        job.logger().info(f, f'###################_TASK-7_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-7_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%% Job End

job.runtime().end()

# %%
<path=>.\SERVICENOW\Development\Curated\KBX.Analytics.DL.ServiceNow.Task.Curated\KBX.Analytics.DL.ServiceNow.Task.Curated.Jobs\lib<content=>
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\README.md<content=>
KBX.Analytics.DL.ServiceNow.Incident.Ingest
============

## Introduction 

This solutions is reponsible for ingesting the data.  It has python scripts that are scheduled and ran with Glue to. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in ingest.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## ingest.py

- Writes data to the **bucket_target**/**prefix_target**/ingest_date=yyyy-MM-dd partition.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe ingest.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagetarget
  displayName: Target Stage
  default: Target ingest name, such as raw

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'incident' # Determined by CodeTemplate ProjectName parameter.
  domain: 'servicenow'
  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Ingest.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure\Ingest.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Ingest deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Incident.Ingest/KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Incident.Ingest/KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Incident.Ingest/KBX.Analytics.DL.ServiceNow.Incident.Ingest.Jobs
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Incident.Ingest/KBX.Analytics.DL.ServiceNow.Incident.Ingest.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  IngestStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  IngestJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/ingest.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  IngestCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "Standard"
      NumberOfWorkers: 15
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--sysparm_limit": "500",
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--prefix_target" : !Ref Entity,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain,
        "--History": "False"
      }

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Jobs\ingest.py<content=>
#%%------------------------------------------ingest------------------------------------------

import os
import sys
import argparse
import requests
import json
import boto3
import botocore
from datetime import datetime
from datetime import timedelta
import math
import concurrent.futures as cf
import logging

# Timer
start_time = datetime.utcnow()
ingest_start_time = f'''{(start_time - timedelta(days=1)).strftime("'%Y-%m-%d','00:00:00'")}'''
ingest_end_time = f'''{start_time.strftime("'%Y-%m-%d','00:00:00'")}'''

# file
f = os.path.basename(__file__)

# secret
servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%------------------------------------------Init------------------------------------------
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
parser.add_argument('--sysparm_limit')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--History')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target
#bucket_target = "kbxt-dl-analytics-servicenow-incident-raw-dev" #remove

prefix_target = args.prefix_target
#prefix_target = "incident" #remove

sysparm_limit = args.sysparm_limit
#sysparm_limit = 100 #remove

history = args.History

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'incident'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'
OFFSET = 0
JOB_NAME = args.JOB_NAME

# KbxtDlPy logger
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)


job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

table = prefix_target
base_api_url = f'https://kochprod.service-now.com/api/now/table/{table}'
headers = {"Content-Type":"application/json","Accept":"application/json"}
crawler = bucket_target.replace('-', '_')+'-crawler'

params = {
    'sysparm_query': f"sys_updated_on>=javascript:gs.dateGenerate({ingest_start_time})^sys_updated_on<javascript:gs.dateGenerate({ingest_end_time})",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': sysparm_limit,
    'sysparm_offset': OFFSET		   
}


if history == 'True' :
    params['sysparm_query'] = "assignment_group=3a9eb4071b859090e6cea687bd4bcb68^ORassignment_group=5d7921630fd68a006ee822d8b1050ed5"
    job.logger().info(f, f"ingesting historic data")
    ingest_date = "1800-01-01"

else :
    job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")
    ingest_date = start_time.strftime('%Y-%m-%d')
    job.logger().info(f, f'ingest_start_time : {ingest_start_time}')
    job.logger().info(f, f'ingest_end_time : {ingest_end_time}')


job.logger().info(f, f'servicenow table to injest : {table}')
job.logger().info(f, f'base_api_url : {base_api_url}')
job.logger().info(f, f'sysparm_limit : {sysparm_limit}')
job.logger().info(f, f'params : {params}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'prefix_target : {prefix_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'crawler : {crawler}')

#variables

#determine s3 key
File = f.split('.')[0]
devtemplateprojectname = f"kbx.{PRODUCT}.dl.{DOMAIN}.{ENTITY}.{File}"

extract_date = f'ingest_date={ingest_date}'
extract_datetime = start_time.strftime("%Y%m%d%H%M%S%f")
filepath = f"{prefix_target}/{extract_date}/{devtemplateprojectname}+py+{extract_datetime}.json"
job.logger().info(f, f"object key={filepath}")

job.logger().info(f, f'###################_TASK-1_DEF_UDF_###################')
# User Defined Functions

def uploadFile(inputStream, filePath, bucketName):
    s3_resource = boto3.resource('s3')
    s3_client = boto3.client('s3')

    def isBucketExists():
        try:
            s3_resource.meta.client.head_bucket(Bucket=bucketName)
        except botocore.exceptions.ClientError as e:
            return False
        else :
            return True
    #logger  
    if (not isBucketExists()):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = s3_resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream)
    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
        job.logger().info(f, f"File uploaded at {filePath}")
        return True
    else :
        job.logger().info(f, f"Upload failed with HTTPStatusCode {res.get('HTTPStatusCode')}")
        return False



#%%------------------------------------------Job Process ------------------------------------------
job.logger().info(f, f'###################_TASK-2_START_INGESTION_###################')

try:
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
    initial_x_total_count = 0
    def calculate_offsets(params) :
        global initial_x_total_count
        params = params.copy()
        params['sysparm_limit'] = 1
        response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
        initial_x_total_count = response.headers['X-Total-Count']
        job.logger().info(f, f"initial_x_total_count {initial_x_total_count}")
        number_of_offsets = math.ceil(int(initial_x_total_count)/int(sysparm_limit))
        return number_of_offsets

    def send_request(params) :
        response = requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
        if response.status_code != 200 :
            job.logger().critical(f, f"api response status code : {response.status_code}")
            raise Exception(f"api response status code : {response.status_code}")
        else :
            job.logger().info(f, f"DATA COUNT {len(response.json()['result'])}")
            job.logger().info(f, f"api headers {response.headers}")
            return response

    params_params = []
    co = calculate_offsets(params)
    for n in range(co) :
        parameters = params.copy()
        parameters['sysparm_offset'] = int(sysparm_limit)*n
        params_params.append(parameters)
    job.logger().info(f, f'##### Sending Parallel Request #####')
    with cf.ThreadPoolExecutor() as executor :
        results = executor.map(send_request, params_params)

    job.logger().info(f, f'##### All requests Response Received #####')
    
    job.logger().info(f, f'##### Validate X-Total-Count of each response and append data #####')
    response = next(results)
    job.logger().info(f, f'requested_url : {response.url}')
    data = response.json()
    x_total_count = response.headers['X-Total-Count']
    job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['result'])}")
    job.logger().info(f, f"x_total_count from parallel response: {x_total_count}")
    job.logger().info(f, f"Total Data Appended : {len(data['result'])}")
    if x_total_count != initial_x_total_count :
        job.logger().info(f, f"SOME DATA MISSED IN CURRENT RESPONSE")
    

    for response in results :
        job.logger().info(f, f'requested_url : {response.url}')
        data['result'].extend(response.json()['result'])
        x_total_count = response.headers['X-Total-Count']
        job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['result'])}")
        job.logger().info(f, f"x_total_count from parallel response: {x_total_count}")
        job.logger().info(f, f"Total Data Appended : {len(data['result'])}")
        if x_total_count != initial_x_total_count :
            job.logger().info(f, f"SOME DATA MISSED IN CURRENT RESPONSE")
    
    if len(data['result']) != int(initial_x_total_count) :
        job.logger().info(f, f"SOME DATA MISSED IN SOME RESPONSE")
        raise Exception(f"ALL DATA NOT RECEIVED")
    else :
        job.logger().info(f, f"ALL DATA RECEIVED")
    
    
    input_stream = bytes(json.dumps(data).encode('UTF-8'))
    if uploadFile(input_stream, filepath, bucket_target) :
        job.logger().info(f, f"File s3a://{bucket_target}/{filepath} uploaded successfully")
        job.logger().info(f, f'###################_TASK-3_JOB_RUN_SUCCESSFULL_###################')
    else :
        raise Exception(f"Upload Failed")

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().info(f, f'###################_TASK-3_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

# %%<path=>.\SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Incident.Ingest\KBX.Analytics.DL.ServiceNow.Incident.Ingest.Jobs\lib<content=>
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1**

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[Domain].[EntityName]
    ```

    For example: KBX.Analytics.DL.ServiceNow.Task

- Navigate to the new solutions directory

**NOTE**: You may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project ReadMe Files
- Review your new solutions ReadMe.md file

## Congrats
- You have completed setup of your solution.  Please remove this file.
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. MUST take on the naming convention of KBX.[Product].DL.[Entity]  ex) KBX.eDock.DL.Shipment

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Shipment

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format MUST be: KBX.[Product].DL.[Entity]:")]
	[String]
	$ProjectName
)

$TemplateProject = "KBX.Analytics.DL.ServiceNow.Task.Ingest"
$ProductName = $ProjectName.Split('.')[1]
$Domain = $ProjectName.Split('.')[3]
$EntityName = $ProjectName.Split('.')[4]

#Change these to accomidate new templates
$oldProjectName = "KBX.Analytics.DL.ServiceNow.Task.Ingest"
$replacementEntityName = "TASK"
$replacementEntityNameLower = "task"
$replacementEntityNamePascal = "Task"
$replacementEntityNameUpper = "TASK"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementProjectAliasLower = "kbxanalyticsdlservicenowtask"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")
$replacementProductAliasLower = "analytics"
$newProductAliasLower = $ProductName.ToLower()

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productNameLower = $ProductName.ToLower()

$replacementDomainLower = "\[domain\]"
$domainLower = $Domain.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $FullProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $FullProjectName -creplace $replacementProductAliasLower, $newProductAliasLower -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName -creplace $replacementDomainLower, $domainLower  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed


If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\README.md<content=>
KBX.Analytics.DL.ServiceNow.Task.Ingest
============

## Introduction 

This solutions is reponsible for ingesting the data.  It has python scripts that are scheduled and ran with Glue to. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in ingest.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## ingest.py

- Writes data to the **bucket_target**/**prefix_target**/ingest_date=yyyy-MM-dd partition.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe ingest.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagetarget
  displayName: Target Stage
  default: Target ingest name, such as raw

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'task' # Determined by CodeTemplate ProjectName parameter.
  domain: 'servicenow'
  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Ingest.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure\Ingest.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Ingest deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Task.Ingest/KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Task.Ingest/KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Task.Ingest/KBX.Analytics.DL.ServiceNow.Task.Ingest.Jobs
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Task.Ingest/KBX.Analytics.DL.ServiceNow.Task.Ingest.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  IngestStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  IngestJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/ingest.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  IngestCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String  

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "Standard"
      NumberOfWorkers: 15
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--sysparm_limit": "500",
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--prefix_target" : !Ref Entity,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain,
        "--History": "False"
      }

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Jobs\ingest.py<content=>
#%%------------------------------------------ingest------------------------------------------

import os
import sys
import argparse
import requests
import json
import boto3
import botocore
from datetime import datetime
from datetime import timedelta
import math
import concurrent.futures as cf
import logging

# Timer
start_time = datetime.utcnow()
ingest_start_time = f'''{(start_time - timedelta(days=1)).strftime("'%Y-%m-%d','00:00:00'")}'''
ingest_end_time = f'''{start_time.strftime("'%Y-%m-%d','00:00:00'")}'''

# file
f = os.path.basename(__file__)

# secret
servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%------------------------------------------Init------------------------------------------

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
parser.add_argument('--sysparm_limit')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--History')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target
#bucket_target = "kbxt-dl-analytics-servicenow-task-raw-dev" #remove

prefix_target = args.prefix_target
#prefix_target = "task" #remove

sysparm_limit = args.sysparm_limit
#sysparm_limit = 100 #remove

history = args.History

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'task'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'
OFFSET = 0
JOB_NAME = args.JOB_NAME

# KbxtDlPy logger
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

table = prefix_target
base_api_url = f'https://kochprod.service-now.com/api/now/table/{table}'
headers = {"Content-Type":"application/json","Accept":"application/json"}
crawler = bucket_target.replace('-', '_')+'-crawler'

params = {
    'sysparm_query': f"sys_updated_on>=javascript:gs.dateGenerate({ingest_start_time})^sys_updated_on<javascript:gs.dateGenerate({ingest_end_time})",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': sysparm_limit,
    'sysparm_offset': OFFSET,
    'sys_class_name': 'sc_task'
							   
}


if history == 'True' :
    params['sysparm_query'] = "assignment_group=3a9eb4071b859090e6cea687bd4bcb68^ORassignment_group=5d7921630fd68a006ee822d8b1050ed5"
    job.logger().info(f, f"ingesting historic data")
    ingest_date = "1800-01-01"

else :
    job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")
    ingest_date = start_time.strftime('%Y-%m-%d')
    job.logger().info(f, f'ingest_start_time : {ingest_start_time}')
    job.logger().info(f, f'ingest_end_time : {ingest_end_time}')


job.logger().info(f, f'servicenow table to injest : {table}')
job.logger().info(f, f'base_api_url : {base_api_url}')
job.logger().info(f, f'sysparm_limit : {sysparm_limit}')
job.logger().info(f, f'params : {params}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'prefix_target : {prefix_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'crawler : {crawler}')

#variables

#determine s3 key
File = f.split('.')[0]
devtemplateprojectname = f"kbx.{PRODUCT}.dl.{DOMAIN}.{ENTITY}.{File}"

extract_date = f'ingest_date={ingest_date}'
extract_datetime = start_time.strftime("%Y%m%d%H%M%S%f")
filepath = f"{prefix_target}/{extract_date}/{devtemplateprojectname}+py+{extract_datetime}.json"
job.logger().info(f, f"object key={filepath}")

job.logger().info(f, f'###################_TASK-1_DEF_UDF_###################')
# User Defined Functions

def uploadFile(inputStream, filePath, bucketName):
    s3_resource = boto3.resource('s3')
    s3_client = boto3.client('s3')

    def isBucketExists():
        try:
            s3_resource.meta.client.head_bucket(Bucket=bucketName)
        except botocore.exceptions.ClientError as e:
            return False
        else :
            return True
    #logger  
    if (not isBucketExists()):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = s3_resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream)
    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
        job.logger().info(f, f"File uploaded at {filePath}")
        return True
    else :
        job.logger().info(f, f"Upload failed with HTTPStatusCode {res.get('HTTPStatusCode')}")
        return False



#%%------------------------------------------Job Process ------------------------------------------
job.logger().info(f, f'###################_TASK-2_START_INGESTION_###################')

try:
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
    initial_x_total_count = 0
    def calculate_offsets(params) :
        global initial_x_total_count
        params = params.copy()
        params['sysparm_limit'] = 1
        response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
        initial_x_total_count = response.headers['X-Total-Count']
        job.logger().info(f, f"initial_x_total_count {initial_x_total_count}")
        number_of_offsets = math.ceil(int(initial_x_total_count)/int(sysparm_limit))
        return number_of_offsets

    def send_request(params) :
        response = requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
        if response.status_code != 200 :
            job.logger().critical(f, f"api response status code : {response.status_code}")
            raise Exception(f"api response status code : {response.status_code}")
        else :
            job.logger().info(f, f"DATA COUNT {len(response.json()['result'])}")
            job.logger().info(f, f"api headers {response.headers}")
            return response

    params_params = []
    co = calculate_offsets(params)
    for n in range(co) :
        parameters = params.copy()
        parameters['sysparm_offset'] = int(sysparm_limit)*n
        params_params.append(parameters)
    job.logger().info(f, f'##### Sending Parallel Request #####')
    with cf.ThreadPoolExecutor() as executor :
        results = executor.map(send_request, params_params)

    job.logger().info(f, f'##### All requests Response Received #####')
    
    job.logger().info(f, f'##### Validate X-Total-Count of each response and append data #####')
    response = next(results)
    job.logger().info(f, f'requested_url : {response.url}')
    data = response.json()
    x_total_count = response.headers['X-Total-Count']
    job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['result'])}")
    job.logger().info(f, f"x_total_count from parallel response: {x_total_count}")
    job.logger().info(f, f"Total Data Appended : {len(data['result'])}")
    if x_total_count != initial_x_total_count :
        job.logger().info(f, f"SOME DATA MISSED IN CURRENT RESPONSE")
    

    for response in results :
        job.logger().info(f, f'requested_url : {response.url}')
        data['result'].extend(response.json()['result'])
        x_total_count = response.headers['X-Total-Count']
        job.logger().info(f, f"count of chunk data received from parallel response: {len(response.json()['result'])}")
        job.logger().info(f, f"x_total_count from parallel response: {x_total_count}")
        job.logger().info(f, f"Total Data Appended : {len(data['result'])}")
        if x_total_count != initial_x_total_count :
            job.logger().info(f, f"SOME DATA MISSED IN CURRENT RESPONSE")
    
    if len(data['result']) != int(initial_x_total_count) :
        job.logger().info(f, f"SOME DATA MISSED IN SOME RESPONSE")
        raise Exception(f"ALL DATA NOT RECEIVED")
    else :
        job.logger().info(f, f"ALL DATA RECEIVED")
    
    
    input_stream = bytes(json.dumps(data).encode('UTF-8'))
    if uploadFile(input_stream, filepath, bucket_target) :
        job.logger().info(f, f"File s3a://{bucket_target}/{filepath} uploaded successfully")
        job.logger().info(f, f'###################_TASK-3_JOB_RUN_SUCCESSFULL_###################')
    else :
        raise Exception(f"Upload Failed")

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().info(f, f'###################_TASK-3_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

# %%<path=>.\SERVICENOW\Development\Ingestion\KBX.Analytics.DL.ServiceNow.Task.Ingest\KBX.Analytics.DL.ServiceNow.Task.Ingest.Jobs\lib<content=>
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\.dockerignore<content=>
.git
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\.gitignore<content=>
### Vim ###
[._]*.s[a-w][a-z]
[._]s[a-w][a-z]
*.un~
Session.vim
.netrwhist
*~

### SublimeText ###
# cache files for sublime text
*.tmlanguage.cache
*.tmPreferences.cache
*.stTheme.cache

# workspace files are user-specific
*.sublime-workspace

# project files should be checked into the repository, unless a significant
# proportion of contributors will probably not be using SublimeText
# *.sublime-project

# sftp configuration file
sftp-config.json

# Python
__pycache__
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-airflow-$(environment)' 
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: 'deployments/$(Build.Repository.Name)/'
 
stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**/!(*_example_*)'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Copy
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)dags'
        globExpressions: '**/!(*_example_*)'
        targetFolder: 'dags'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'


<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\docker-compose.yml<content=>
version: '3.7'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"

    webserver:
        image: puckel/docker-airflow:latest
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            # AWS
            # Only used for local development
            - AWS_ACCESS_KEY_ID=ASIA4GECNIBZKRULQWUZ
            - AWS_SECRET_ACCESS_KEY=4X6asop5i6TR5CH7uwdBqdBGob3mzfhFiOKekR/m
            - AWS_SESSION_TOKEN=FwoGZXIvYXdzEK7//////////wEaDKzGynHuCkec0YlfnyKWAoT7eGwiltryJHPSIc+4Mi48Skcl+4740gFTTcN1TRRi39Y4LmL1A/v1LsHCdGPov11szaR4F6JzEZ6Ab9VG9Mq3F+90dkIZ3+EK05Jqmgj6rZgUlUR5xZEr/Y6jTN/Y5H3cy5S7Tf7WoYIxmZiooMTC0Pv3FyDB7c+X8RDU+b6xFTr/cFsb1sD9stTLiF6wNbVG4ruUoMNUj55zpepBDGvNlog9z6w2H1AHDtQunC+KxFvLrf7Hvwy1AYMzOudyjEzdA+PsyP7WU9doiccBzCocgHdzC3IM3sokUtqFdErq4V8hB6IEWvKQGcisvirAiEZOw9rKo4E7EFXmzewgf7T1E4sCDdApDrk+YfWpWaCAKqS2dU4aKLi9oZkGMisX+R12uepWbNjgchIEEyYTWAZsO6RAC9xjIXaJ97Iv5NhKPAUfheqU3aIn
            - AWS_DEFAULT_REGION=us-east-1
            # Encryption
            # Can be static, it's only used for local development
            - FERNET_KEY=lUDB1r1SNvYy4kZAhA-4z8PqC0x8UQfYmo4uFP8UFcg=
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ./dags:/usr/local/airflow/dags
            - ./plugins:/usr/local/airflow/plugins
            - ./requirements.txt:/requirements.txt
        ports:
            - "8080:8080"
        build: .
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\Dockerfile<content=>
# VERSION 1.10.9
# AUTHOR: Matthieu "Puckel_" Roisil
# DESCRIPTION: Basic Airflow container
# BUILD: docker build --rm -t puckel/docker-airflow .
# SOURCE: https://github.com/puckel/docker-airflow

FROM python:3.7-slim-buster
LABEL maintainer="KBX"

# Never prompt the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux

# Airflow
ARG AIRFLOW_VERSION=1.10.9
ARG AIRFLOW_USER_HOME=/usr/local/airflow
ARG AIRFLOW_DEPS=""
ARG PYTHON_DEPS="MarkupSafe==2.0.1"
ENV AIRFLOW_HOME=${AIRFLOW_USER_HOME}

# Define en_US.
ENV LANGUAGE en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LC_ALL en_US.UTF-8
ENV LC_CTYPE en_US.UTF-8
ENV LC_MESSAGES en_US.UTF-8

# Disable noisy "Handling signal" log messages:
# ENV GUNICORN_CMD_ARGS --log-level WARNING

RUN set -ex \
    && buildDeps=' \
        freetds-dev \
        libkrb5-dev \
        libsasl2-dev \
        libssl-dev \
        libffi-dev \
        libpq-dev \
        git \
    ' \
    && apt-get update -yqq \
    && apt-get upgrade -yqq \
    && apt-get install -yqq --no-install-recommends \
        $buildDeps \
        freetds-bin \
        build-essential \
        default-libmysqlclient-dev \
        apt-utils \
        curl \
        rsync \
        netcat \
        locales \
    && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \
    && locale-gen \
    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \
    && useradd -ms /bin/bash -d ${AIRFLOW_USER_HOME} airflow \
    && pip install -U pip setuptools wheel \
    && pip install pytz \
    && pip install pyOpenSSL \
    && pip install ndg-httpsclient \
    && pip install pyasn1 \
    && pip install apache-airflow[crypto,celery,postgres,hive,jdbc,mysql,ssh${AIRFLOW_DEPS:+,}${AIRFLOW_DEPS}]==${AIRFLOW_VERSION} \
    && pip install 'redis==3.2' \
    && if [ -n "${PYTHON_DEPS}" ]; then pip install ${PYTHON_DEPS}; fi \
    && apt-get purge --auto-remove -yqq $buildDeps \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf \
        /var/lib/apt/lists/* \
        /tmp/* \
        /var/tmp/* \
        /usr/share/man \
        /usr/share/doc \
        /usr/share/doc-base

COPY script/entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_USER_HOME}/airflow.cfg

RUN chown -R airflow: ${AIRFLOW_USER_HOME}

EXPOSE 8080 5555 8793

RUN pip install apache-airflow-backport-providers-amazon \
 && pip install apache-airflow-backport-providers-apache-spark \
 && pip install apache-airflow-backport-providers-jdbc \
 && pip install apache-airflow-backport-providers-datadog \
 && pip install apache-airflow-backport-providers-postgres \
 && pip install sqlalchemy==1.3.13

USER airflow
WORKDIR ${AIRFLOW_USER_HOME}
ENTRYPOINT ["/entrypoint.sh"]
CMD ["webserver"]
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\LICENSE<content=>
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2017 Matthieu "Puckel_" Roisil

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\package-lock.json<content=>
{
  "name": "KBX.DL.CodeTemplates.Airflow",
  "lockfileVersion": 2,
  "requires": true,
  "packages": {}
}
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\package.json<content=>
{}
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\README.md<content=>
# docker-airflow

This repository contains **Dockerfile** of [apache-airflow](https://github.com/apache/incubator-airflow) for [Docker](https://www.docker.com/)'s [automated build](https://registry.hub.docker.com/u/puckel/docker-airflow/) published to the public [Docker Hub Registry](https://registry.hub.docker.com/).

## Information

* Based on Python (3.7-slim-buster) official Image [python:3.7-slim-buster](https://hub.docker.com/_/python/) and uses the official [Postgres](https://hub.docker.com/_/postgres/) as backend and [Redis](https://hub.docker.com/_/redis/) as queue
* Install [Docker](https://www.docker.com/)
* Install [Docker Compose](https://docs.docker.com/compose/install/)
* This follows the Airflow release from this page [Python Package Index](https://pypi.python.org/pypi/apache-airflow)

## Installation

This solution has been created by a code template.

## Build

Set any necessary environment variables in the docker-compose.yml file

* LOAD_EX can be set to "n" if you do not want to deploy the sample DAGs
* AWS_ACCESS_KEY_ID should be set to an appropriate AWS access key Id
* AWS_SECRET_ACCESS_KEY should be set to the key value of the aforementioned AWS access key Id

Compose and start a container from the docker-compose.yml file:

* docker-compose -f docker-compose.yml up

A docker container will now be visible and manageable in Docker Desktop.
The Airflow Web UI can be accessed at: [localhost:8080](http://localhost:8080/)

## Configuration

* Use the variable template DAG to set Airflow variables which your other DAGs require.  Please refer to the [yourprojectname]_variables.py template DAG
* Use the connection template DAG to set Airflow connections; connections should be named [yourprojectname]_[resource]; for example, kbxdlcodetemplatesworkflow_aws for a project named KBX.DL.CodeTemplates.Workflow.  Please refer to the [yourprojectname]_connections.py template DAG

## Management

Stop and remove the container along with any associated resources created via the aforementioned "up" command:

* docker-compose -f docker-compose.yml down

List running containers:

* docker ps

Access the shell of a running container as the default user:

* docker exec -ti "container name" bash

Access the shell of a running container as root:

* docker exec -u 0 -ti "container name" bash

## Pitfalls

* **Connections and Variables** are not persisted between containers unless you include them as environment variables in the docker-compose YAML file.
* Manually refreshing Airflow Web UI pages excessively will cause the instance to become temporarily unresponsive
* Ensure that the "dag" parameter is set for all operators to avoid vague error messages
* A DAG must be in the "On" state to be triggered by schedule or manually
* DAGs which leverage dynamic operator generation based on the results of a query will execute that query every time the "DAG bag" is filled or the DAG is accessed if the query exists within the same DAG.  The query which the dynamic operator generation relies on should exist in a separate DAG (scheduled as per requirements) where the results are written to an Airflow variable; this variable will then be referenced in the DAG containing dynamic operator generation
* Implementation of sub-DAGs requires explicitly setting the "schedule_interval" and "start_date" parameters of the parent DAG instead of passing them as part of the "default_args" parameter to avoid vague error messages
* Provider packages expose operators, hooks, and sensors but only the operators provide "out-of-the-box" functionality; resort to wiring up the hook and sensor only if an operator is unavailable or cannot meet a specific requirement
* For an unscheduled DAG either do not reference the "schedule_interval" property or set it to None (the type, not a string); for example, schedule_interval=None
* Setting the "start_date" parameter of any DAG to the current or a future date/time will cause the scheduler to fail; jobs will look as though they have started but not execute any tasks and be stuck in a "running" state
* **start_date** is counterintuitive but by design; for example, a job scheduled hourly and starting at 1400h will actually execute at 1500h.  All times in airflow are UTC and so don't execute at your local time.  Avoid using datetime.now() to offset this. Please see https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag and https://marclamberti.com/blog/apache-airflow-best-practices-1/ for further information

## References

* puckel Airflow Docker image documentation - https://hub.docker.com/r/puckel/docker-airflow
* Airflow core concepts - https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html
* Airflow best practices - https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html
* Airflow lesser-known tips and tricks - https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\requirements.txt<content=>
# KBX does not support the automation of this file, create a help request
wtforms==2.3.3
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\aws\credentials<content=>
[837792120946-kbxt-dl-analytics-developer-role-dev]
region = us-east-1
aws_access_key_id = ASIA4GECNIBZPGTF36UZ
aws_secret_access_key = tY1wueC2U2CDET7fZCcDdgowE1wandbLwfT2UJGL
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDLnw0EgKO32qKakY/iKWAmWOVNFfvKTw0fqXDmCRp2t3zHyQSq2v8Zc+SLLETyBGpYh906NnYfP0BTW0yx5x5418a/6g4P0pIOIyGg1qDWmE/GXKHlKixsY9Zuzf85VigXWLlsBkF6kiVoTvJ3kuHV2ju48qptfBmgK7Dz373cBhxq12rqRNmE/kAfQlsSJe8BZGLbtK2vcHeO6s/LEIM7doY3G3mYlNQAZPqgJ+B1724eecB/auiE5IIdCSFseZczoVUnf/CRldkfmL13t+9HaXg+cwNHKvTyIM7S/9MfTEhXjm7aeCs1hb7EXO1wJq9tdkaA+geSu7cVzxL6lclRaJsGiHXF4uxGIhRqeYiXF9Ohl/1ZlSf7qcbSJfowcqHyBZYdEdKJqjt5gGMisQcr+JBIlm6pRJfw6SMqDZU8vXI+IkkCSRjb26CA73TaEUqfzo5BAFnzp5
[dev]
region = us-east-1
aws_access_key_id = ASIA4GECNIBZPGTF36UZ
aws_secret_access_key = tY1wueC2U2CDET7fZCcDdgowE1wandbLwfT2UJGL
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDLnw0EgKO32qKakY/iKWAmWOVNFfvKTw0fqXDmCRp2t3zHyQSq2v8Zc+SLLETyBGpYh906NnYfP0BTW0yx5x5418a/6g4P0pIOIyGg1qDWmE/GXKHlKixsY9Zuzf85VigXWLlsBkF6kiVoTvJ3kuHV2ju48qptfBmgK7Dz373cBhxq12rqRNmE/kAfQlsSJe8BZGLbtK2vcHeO6s/LEIM7doY3G3mYlNQAZPqgJ+B1724eecB/auiE5IIdCSFseZczoVUnf/CRldkfmL13t+9HaXg+cwNHKvTyIM7S/9MfTEhXjm7aeCs1hb7EXO1wJq9tdkaA+geSu7cVzxL6lclRaJsGiHXF4uxGIhRqeYiXF9Ohl/1ZlSf7qcbSJfowcqHyBZYdEdKJqjt5gGMisQcr+JBIlm6pRJfw6SMqDZU8vXI+IkkCSRjb26CA73TaEUqfzo5BAFnzp5
[default]
region = us-east-1
aws_access_key_id = ASIA4GECNIBZPGTF36UZ
aws_secret_access_key = tY1wueC2U2CDET7fZCcDdgowE1wandbLwfT2UJGL
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDLnw0EgKO32qKakY/iKWAmWOVNFfvKTw0fqXDmCRp2t3zHyQSq2v8Zc+SLLETyBGpYh906NnYfP0BTW0yx5x5418a/6g4P0pIOIyGg1qDWmE/GXKHlKixsY9Zuzf85VigXWLlsBkF6kiVoTvJ3kuHV2ju48qptfBmgK7Dz373cBhxq12rqRNmE/kAfQlsSJe8BZGLbtK2vcHeO6s/LEIM7doY3G3mYlNQAZPqgJ+B1724eecB/auiE5IIdCSFseZczoVUnf/CRldkfmL13t+9HaXg+cwNHKvTyIM7S/9MfTEhXjm7aeCs1hb7EXO1wJq9tdkaA+geSu7cVzxL6lclRaJsGiHXF4uxGIhRqeYiXF9Ohl/1ZlSf7qcbSJfowcqHyBZYdEdKJqjt5gGMisQcr+JBIlm6pRJfw6SMqDZU8vXI+IkkCSRjb26CA73TaEUqfzo5BAFnzp5
[456614517894-kbxt-dl-analytics-developer-role-prod]
region = us-east-1
aws_access_key_id = ASIAWUUCYTSDAUIQAYFQ
aws_secret_access_key = 5oabaqKNIZ9eSDD7GUjWLs2ShvqHLXXRzWh/W0hV
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDEQuX92QK8Msrg2nmCKWAvOYPs+R5D2RX84sqL/O1fXrXuAqh6a85r/yZxl5Nggs99lEivsdRsZepkM+3SmNis3Tsf7ThhjmUfeZ1AU2+qzIpym7LZJxxz6BTybCXLRkMYbn0GKrAqP9yDW0wRcS5S9WFVL0u1wzpn72jFEeUQsbeUKIDhgFQA5GaLpbB8gOjw2pgmE5MWOMAQHMW3ud4/Ka/UieiT1ICHGxDfg/+DFPNw/q8eO6y92PjxFu3L0xZ4TYqGueeECTwVKSZUjEnvbwUqs3dxYBDq8n4+Y+gQuL/RIFSwaWXyX9stxwMnndjDiJ/gpmobdGUrTaKJR4QFPwpfq1dZTYQF51aaiZP0405aJ6QW0kTlsd77+tNb6kHKr0nx1kKJqjt5gGMisFf5NyYhhKhn2uejJTUuXRQWjykrn8EKGlkZotfKhgPB3ZEWElxhnNcQYV
[prod]
region = us-east-1
aws_access_key_id = ASIAWUUCYTSDAUIQAYFQ
aws_secret_access_key = 5oabaqKNIZ9eSDD7GUjWLs2ShvqHLXXRzWh/W0hV
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDEQuX92QK8Msrg2nmCKWAvOYPs+R5D2RX84sqL/O1fXrXuAqh6a85r/yZxl5Nggs99lEivsdRsZepkM+3SmNis3Tsf7ThhjmUfeZ1AU2+qzIpym7LZJxxz6BTybCXLRkMYbn0GKrAqP9yDW0wRcS5S9WFVL0u1wzpn72jFEeUQsbeUKIDhgFQA5GaLpbB8gOjw2pgmE5MWOMAQHMW3ud4/Ka/UieiT1ICHGxDfg/+DFPNw/q8eO6y92PjxFu3L0xZ4TYqGueeECTwVKSZUjEnvbwUqs3dxYBDq8n4+Y+gQuL/RIFSwaWXyX9stxwMnndjDiJ/gpmobdGUrTaKJR4QFPwpfq1dZTYQF51aaiZP0405aJ6QW0kTlsd77+tNb6kHKr0nx1kKJqjt5gGMisFf5NyYhhKhn2uejJTUuXRQWjykrn8EKGlkZotfKhgPB3ZEWElxhnNcQYV
[858960246736-kbx-it-developer]
region = us-east-1
aws_access_key_id = ASIA4P7QFG7IJSIAZO7B
aws_secret_access_key = CPKessLRJlL7OOLYKSdMDQT9nUY4x9/R1OBTTXW9
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDMQ5McBPBF4kfZ/RhCKWApIxFPeuHILlVjurhqSmh1dWftpgOCZJDvGNmBHSUneqiFyY7ED6kthXYSc92DFM+SmAg6jEtuPOJylG+KAPNSp9gBiDOO97YLRzDAyS3VbBL26qifjSqPqtL/Vim0azbpnyzQ5H8OeBLX57oq7uoq85IOeJpb/dkst2EIVrce6rlTBVUAYyXCsx3It9P6te4ohX4aqGZNBhYJPrb3OdoFaOwaxSWcGSszecgTMcrK55dVIgxNuDIlkgaVIsgFAMXB2tpZ8DAOd2cdmGSJ/EiT3Pt9AI6yxzSsY5IW8TiF8m6WHk42l8Hh85VTI1Jyzn1AYO1jxTpBK+WsIg5XT/y/WfnnsEZNL3cefS9N8R/h7p6sCPeERhKJujt5gGMisD2ohgtar5i71yNeP5W+gL0nWNxkSix945ng/UCOYS2wvpH/gOZwrj77LG
[837792120946-kbx-it-developer]
region = us-east-1
aws_access_key_id = ASIA4GECNIBZCSE75H6J
aws_secret_access_key = vq2pWyv7MRfY0wtKv1EaSn5CIw4bUBiDYn/xz8Tg
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDOzRZgX3ohaMigu/QSKWAi+bo8Ij1rxsn9vKhN191GqXIgwKsWfzIqIFrndwWyxZmCHrbhFYjjWoK8pzR1EPsv04da4S9FyarvnfqfcgxeQbbEgiYiQaEgWBbzGmAcD2+G+jdyHCWrp3vpsPoP64wlpB34sLo7LITw/uHuUJyn4F3fB68LJsib363DAJRp/vi63jXT4HOZe+SWINhRTDAABF/90T7Uvc5NGO62EJvOZL/pPuNe9p4u1KGQithM6Ppi7qAXQB/rCSdqghJYDdbRgbr6Ubq/1OpK6cdV8QoUgCxDfIqf4KROUmEjmwOSELjotjiEZQu5J7osr23aUx8nIxx3TuCXsP9yiCGS85hJxNc0xcl7te+xz+M6hCF1cNpSs/8H3/KJujt5gGMiuyVhRNydycwJLy9FP217KxGGfztyY3pdEalLMtVDuFSCt2mF3fowZXACnv
[381401522913-kbx-it-developer]
region = us-east-1
aws_access_key_id = ASIAVRTKNMLQXBMSXJ7O
aws_secret_access_key = bfYCzB/JyIy+jL6LIPcJ9nGY4+L76inzku8Y7/WP
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDA2OgY+VPMG2FEkGaCKWAi+hU7Tt+OY+XTwF0gqy/KcVT75QKuIntVefV7VerDus5A3sLFTYtL3Rnp89ek4zWdCXjcqLvT1Dz7SasnbmcVXy5ZCXkRxjCtdLOlRE3brWas+lHN8JksobtjNf5sMHFTSeDnFpAouk0FA7jAHbBpWfFYBExCq1Wv6e5vtwxrknrPKz1AKGfTJ1RM7AXNZ4d5l3hR5E5sdOogH01v59s3MZc4obNTt1uuxoT9IBsYETHA9A7KJVwZqWyFeWMCK8rJbTmKwndRaYkmRRQc66tw0rCr6j57ZCniuKEFrpcWw7Dcv1Y1Bk6Y7LxiZ6wxVpxJSvt/lGfvc2njxCv43iZUgkLcyvR3CsTQJdfF5fnuntng5LBTzYKJujt5gGMiuzZoINl6le13nIblICjuZBjUHItlf9SiCWytaKd7AxWttSRE+jY/em64kb
[456614517894-kbx-it-developer]
region = us-east-1
aws_access_key_id = ASIAWUUCYTSDIVQGS5VP
aws_secret_access_key = xz4THDAvOffW0+Cb8GOMrio+8SyIkrZd0Wb4VvtU
aws_session_token = FwoGZXIvYXdzEMr//////////wEaDAWW3/GaeIYuIJHOyCKWAg/sfrmFdTCxJFOmfCT3zaU1bDr4Y2H60op9oK2XoD2esiFX9Y0Bq6wxVNnpmInVMrnoGWHM/7fGIUEoM5YyZvujnMEYfddM26btyPB8ui+dFMYKsonJHBFeRRoyvWcT4Fj//ewFSnMnOBGS4toI3jgHeK77LG7RopiNN/G1XxyRi6/eAiXifBtAIWNbg08whN6PcCjA9RRm/+HImO7Z/MI5AFt1Y60HTe5IKjvZ0WOcjsBLzD6sHo/3X21iowBXpF7vHtCsTmQBdK3OQo6ue7D8zinVWBNscAZZbzyFXyVmNt/Urfum8qvB9h/thG7Xd9k9d3NqNevG5pr0Mfn1M7JduQhZt/sqeVU7wl0B/iLhJVGQDQ2OKJyjt5gGMiueEoXQIPKD1hMcBfnkB1vL64RAnZfqTyUHgGgLGyTufMR7VazAiOxfNY1A
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\config\airflow.cfg<content=>
[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Users must supply an Airflow connection id that provides access to the storage
# location.
remote_log_conn_id =
remote_base_log_folder =
encrypt_s3_logs = False

# Logging level
logging_level = INFO

# Logging level for Flask-appbuilder UI
fab_logging_level = WARN

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
# Example: logging_config_class = my.path.default_local_settings.LOGGING_CONFIG
logging_config_class =

# Flag to enable/disable Colored logs in Console
# Colour the logs when the controlling terminal is a TTY.
colored_console_log = True

# Log format for when Colored logs is enabled
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d}} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Format of Log line
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Name of handler to read task instance logs.
# Default to use task handler.
task_log_reader = task

# Hostname by providing a path to a callable, which will resolve the hostname.
# The format is "package:function".
#
# For example, default value "socket:getfqdn" means that result from getfqdn() of "socket"
# package will be used as hostname.
#
# No argument should be required in the function specified.
# If using IP address as hostname is preferred, use value ``airflow.utils.net:get_host_ip_address``
hostname_callable = socket:getfqdn

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = utc

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
# sql_alchemy_conn = sqlite:////tmp/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool. 0 indicates no limit.
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
# When the number of checked-out connections reaches the size set in pool_size,
# additional connections will be returned up to this limit.
# When those additional connections are returned to the pool, they are disconnected and discarded.
# It follows then that the total number of simultaneous connections the pool will allow
# is pool_size + max_overflow,
# and the total number of "sleeping" connections the pool will allow is pool_size.
# max_overflow can be set to -1 to indicate no overflow limit;
# no limit will be placed on the total number of concurrent connections. Defaults to 10.
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite. If the number of DB connections is ever exceeded,
# a lower config value will allow the system to recover faster.
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout.
# Typically, this is a simple statement like "SELECT 1".
# More information here:
# https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic
sql_alchemy_pool_pre_ping = True

# The schema to use for the metadata database.
# SqlAlchemy supports databases with the concept of multiple schemas.
sql_alchemy_schema =

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = True

# Where your Airflow plugins are stored
plugins_folder = /usr/local/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = $FERNET_KEY

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a ``run_as_user`` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
default_impersonation =

# What security module to use (for example kerberos)
security =

# If set to False enables some unsecure features like Charts and Ad Hoc Queries.
# In 2.0 will default to True.
secure_mode = False

# Turn unit test mode on (overwrites many configuration options with test
# values at runtime)
unit_test_mode = False

# Whether to enable pickling for xcom (note that this is insecure and allows for
# RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
enable_xcom_pickling = True

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

# Whether to override params with dag_run.conf. If you pass some key-value pairs
# through ``airflow dags backfill -c`` or
# ``airflow dags trigger -c``, the key-value pairs will override the existing ones in params.
dag_run_conf_overrides_params = False

# Worker initialisation check to validate Metadata Database connection
worker_precheck = False

# When discovering DAGs, ignore any files that don't contain the strings ``DAG`` and ``airflow``.
dag_discovery_safe_mode = True

# The number of retries each task is going to have by default. Can be overridden at dag or task level.
default_task_retries = 0

# Whether to serialises DAGs and persist them in DB.
# If set to True, Webserver reads from DB instead of parsing DAG files
# More details: https://airflow.apache.org/docs/stable/dag-serialization.html
store_serialized_dags = False

# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate.
min_serialized_dag_update_interval = 30

# On each dagrun check against defined SLAs
check_slas = True

[cli]
# In what way should the cli access the API. The LocalClient will use the
# database directly, while the json_client will use the api running on the
# webserver
api_client = airflow.api.client.local_client

# If you set web_server_url_prefix, do NOT forget to append it here, ex:
# ``endpoint_url = http://localhost:8080/myroot``
# So api will look like: ``http://localhost:8080/myroot/api/experimental/...``
endpoint_url = http://localhost:8080

[debug]
# Used only with DebugExecutor. If set to True DAG will fail with first
# failed task. Helpful for debugging purposes.
fail_fast = False

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[lineage]
# what lineage backend to use
backend =

[atlas]
sasl_enabled = False
host =
port = 21000
username =
password =

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via ``default_args``
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[hive]
# Default mapreduce queue for HiveOperator tasks
default_hive_mapred_queue =

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_key =

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# Secret key used to run your flask app
# It should be as random as possible
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use. Choices include
# sync (default), eventlet, gevent
worker_class = sync

# Log files for the gunicorn webserver. '-' means log to stderr.
access_logfile = -

# Log files for the gunicorn webserver. '-' means log to stderr.
error_logfile = -

# Expose the configuration file in the web server
expose_config = True

# Expose hostname in the web server
expose_hostname = True

# Expose stacktrace in the web server
expose_stacktrace = True

# Set to true to turn on authentication:
# https://airflow.apache.org/security.html#web-authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode. Choices include user (default) and ldapgroup.
# Ldap group filtering requires using the ldap backend
#
# Note that the ldap server needs the "memberOf" overlay to be set up
# in order to user the ldapgroup mode.
owner_mode = user

# Default DAG view. Valid values are:
# tree, graph, duration, gantt, landing_times
dag_default_view = tree

# "Default DAG orientation. Valid values are:"
# LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Puts the webserver in demonstration mode; blurs the names of Operators for
# privacy.
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
# while fetching logs from other worker machine
log_fetch_timeout_sec = 5

# Time interval (in secs) to wait before next log fetching.
log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing.
log_auto_tailing_offset = 30

# Animation speed for auto tailing log display.
log_animation_speed = 1000

# By default, the webserver shows paused DAGs. Flip this to hide paused
# DAGs by default
hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
page_size = 100

# Use FAB-based webserver with RBAC feature
rbac = False

# Define the color of navigation bar
navbar_color = #007A87

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
enable_proxy_fix = False

# Number of values to trust for ``X-Forwarded-For``.
# More info: https://werkzeug.palletsprojects.com/en/0.16.x/middleware/proxy_fix/
proxy_fix_x_for = 1

# Number of values to trust for ``X-Forwarded-Proto``
proxy_fix_x_proto = 1

# Number of values to trust for ``X-Forwarded-Host``
proxy_fix_x_host = 1

# Number of values to trust for ``X-Forwarded-Port``
proxy_fix_x_port = 1

# Number of values to trust for ``X-Forwarded-Prefix``
proxy_fix_x_prefix = 1

# Set secure flag on session cookie
cookie_secure = False

# Set samesite policy on session cookie
cookie_samesite =

# Default setting for wrap toggle on DAG code and TI log views.
default_wrap = False

# Allow the UI to be rendered in a frame
x_frame_enabled = True

# Send anonymous user activity to your analytics tool
# choose from google_analytics, segment, or metarouter
# analytics_tool =

# Unique ID of your account in the analytics tool
# analytics_id =

# Update FAB permissions and sync security manager roles
# on webserver startup
update_fab_perms = True

# Minutes of non-activity before logged out from UI
# 0 means never get forcibly logged out
force_log_out_after = 0

# The UI cookie lifetime in days
session_lifetime_days = 30

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]

# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
# smtp_user =
# Example: smtp_password = airflow
# smtp_password =
smtp_port = 25
smtp_mail_from = airflow@example.com

[sentry]

# Sentry (https://docs.sentry.io) integration
sentry_dsn =

[celery]

# This section only applies if you are using the CeleryExecutor in
# ``[core]`` section above
# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# The maximum and minimum concurrency that will be used when starting workers with the
# ``airflow celery worker`` command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Example: worker_autoscale = 16,12
worker_autoscale = 16,12

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://redis:6379/1

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it ``airflow flower``. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# The root URL for Flower
# Example: flower_url_prefix = /flower
flower_url_prefix =

# This defines the port that Celery Flower runs on
flower_port = 5555

# Securing Flower with Basic Authentication
# Accepts user:password pairs separated by a comma
# Example: flower_basic_auth = user1:password1,user2:password2
flower_basic_auth =

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default

# How many processes CeleryExecutor uses to sync task state.
# 0 means to use max(1, number of cores - 1) processes.
sync_parallelism = 0

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# In case of using SSL
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

# Celery Pool implementation.
# Choices include: prefork (default), eventlet, gevent or solo.
# See:
# https://docs.celeryproject.org/en/latest/userguide/workers.html#concurrency
# https://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html
pool = prefork

# The number of seconds to wait before timing out ``send_task_to_executor`` or
# ``fetch_celery_task_state`` operations.
operation_timeout = 2

[celery_broker_transport_options]

# This section is for specifying options which can be passed to the
# underlying celery broker transport. See:
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_transport_options
# The visibility timeout defines the number of seconds to wait for the worker
# to acknowledge the task before the message is redelivered to another worker.
# Make sure to increase the visibility timeout to match the time of the longest
# ETA you're planning to use.
# visibility_timeout is only supported for Redis and SQS celery brokers.
# See:
# http://docs.celeryproject.org/en/master/userguide/configuration.html#std:setting-broker_transport_options
# Example: visibility_timeout = 21600
# visibility_timeout =

[dask]

# This section only applies if you are using the DaskExecutor in
# [core] section above
# The IP address and port of the Dask cluster's scheduler.
cluster_address = 127.0.0.1:8786

# TLS/ SSL settings to access a secured Dask scheduler.
tls_ca =
tls_cert =
tls_key =

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# After how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# The number of times to try to schedule each DAG file
# -1 indicates unlimited number
num_runs = -1

# The number of seconds to wait between consecutive DAG file processing
processor_poll_interval = 1

# after how much time (seconds) a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
dag_dir_list_interval = 300

# How often should stats be printed to the logs. Setting to 0 will disable printing stats
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
# This is used by the health check in the "/health" endpoint
scheduler_health_check_threshold = 30
child_process_log_directory = /usr/local/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is False,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
catchup_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# If this is too high, SQL query performance may be impacted by one
# or more of the following:
# - reversion to full table scan
# - complexity of query predicate
# - excessive locking
# Additionally, you may hit the maximum allowable query length for your db.
# Set this to 0 for no limit (not advised)
max_tis_per_query = 512

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# If you want to avoid send all the available metrics to StatsD,
# you can configure an allow list of prefixes to send only the metrics that
# start with the elements of the list (e.g: scheduler,executor,dagrun)
statsd_allow_list =

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
max_threads = 2
authenticate = False

# Turn off scheduler use of cron intervals by setting this to False.
# DAGs submitted manually in the web UI or with trigger_dag will still run.
use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
# Only has effect if schedule_interval is set to None in DAG
allow_trigger_in_future = False

[ldap]
# set this to ldaps://<your.ldap.server>:<port>
uri =
user_filter = objectClass=*
user_name_attr = uid
group_member_attr = memberOf
superuser_filter =
data_profiler_filter =
bind_user = cn=Manager,dc=example,dc=com
bind_password = insecure
basedn = dc=example,dc=com
cacert = /etc/ca/ldap_ca.crt
search_scope = LEVEL

# This setting allows the use of LDAP servers that either return a
# broken schema, or do not return a schema.
ignore_malformed_schema = False

[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

# Number of cpu cores required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_cpu = 1

# Memory in MB required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_memory = 256

# Enable framework checkpointing for mesos
# See http://mesos.apache.org/documentation/latest/slave-recovery/
checkpoint = False

# Failover timeout in milliseconds.
# When checkpointing is enabled and this option is set, Mesos waits
# until the configured timeout for
# the MesosExecutor framework to re-register after a failover. Mesos
# shuts down running tasks if the
# MesosExecutor framework fails to re-register within this timeframe.
# Example: failover_timeout = 604800
# failover_timeout =

# Enable framework authentication for mesos
# See http://mesos.apache.org/documentation/latest/configuration/
authenticate = False

# Mesos credentials, if authentication is enabled
# Example: default_principal = admin
# default_principal =
# Example: default_secret = admin
# default_secret =

# Optional Docker Image to run on slave before running the command
# This image should be accessible from mesos slave i.e mesos slave
# should be able to pull this docker image before executing the command.
# Example: docker_image_slave = puckel/docker-airflow
# docker_image_slave =

[kerberos]
ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
api_rev = v3

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

[elasticsearch]
# Elasticsearch host
host =

# Format of the log_id, which is used to query for a given tasks logs
log_id_template = {{dag_id}}-{{task_id}}-{{execution_date}}-{{try_number}}

# Used to mark the end of a log stream for a task
end_of_log_mark = end_of_log

# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
# Code will construct log_id using the log_id template from the argument above.
# NOTE: The code will prefix the https:// automatically, don't include that here.
frontend =

# Write the task logs to the stdout of the worker, rather than the default files
write_stdout = False

# Instead of the default log formatter, write the log lines as JSON
json_format = False

# Log fields to also attach to the json output, if enabled
json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
use_ssl = False
verify_certs = True

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository =
worker_container_tag =
worker_container_image_pull_policy = IfNotPresent

# If True (default), worker pods will be deleted upon termination
delete_worker_pods = True

# Number of Kubernetes Worker Pod creation calls per scheduler loop
worker_pods_creation_batch_size = 1

# The Kubernetes namespace where airflow workers should be created. Defaults to ``default``
namespace = default

# The name of the Kubernetes ConfigMap containing the Airflow Configuration (this file)
# Example: airflow_configmap = airflow-configmap
airflow_configmap =

# The name of the Kubernetes ConfigMap containing ``airflow_local_settings.py`` file.
#
# For example:
#
# ``airflow_local_settings_configmap = "airflow-configmap"`` if you have the following ConfigMap.
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     airflow_local_settings.py: |
#         def pod_mutation_hook(pod):
#             ...
#     airflow.cfg: |
#         ...
# Example: airflow_local_settings_configmap = airflow-configmap
airflow_local_settings_configmap =

# For docker image already contains DAGs, this is set to ``True``, and the worker will
# search for dags in dags_folder,
# otherwise use git sync or dags volume claim to mount DAGs
dags_in_image = False

# For either git sync or volume mounted DAGs, the worker will look in this subpath for DAGs
dags_volume_subpath =

# For DAGs mounted via a volume claim (mutually exclusive with git-sync and host path)
dags_volume_claim =

# For volume mounted logs, the worker will look in this subpath for logs
logs_volume_subpath =

# A shared volume claim for the logs
logs_volume_claim =

# For DAGs mounted via a hostPath volume (mutually exclusive with volume claim and git-sync)
# Useful in local environment, discouraged in production
dags_volume_host =

# A hostPath volume for the logs
# Useful in local environment, discouraged in production
logs_volume_host =

# A list of configMapsRefs to envFrom. If more than one configMap is
# specified, provide a comma separated list: configmap_a,configmap_b
env_from_configmap_ref =

# A list of secretRefs to envFrom. If more than one secret is
# specified, provide a comma separated list: secret_a,secret_b
env_from_secret_ref =

# Git credentials and repository for DAGs mounted via Git (mutually exclusive with volume claim)
git_repo =
git_branch =
git_subpath =

# The specific rev or hash the git_sync init container will checkout
# This becomes GIT_SYNC_REV environment variable in the git_sync init container for worker pods
git_sync_rev =

# Use git_user and git_password for user authentication or git_ssh_key_secret_name
# and git_ssh_key_secret_key for SSH authentication
git_user =
git_password =
git_sync_root = /git
git_sync_dest = repo

# Mount point of the volume if git-sync is being used.
# i.e. /usr/local/airflow/dags
git_dags_folder_mount_point =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-secrets.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: airflow-secrets
#   data:
#     # key needs to be gitSshKey
#     gitSshKey: <base64_encoded_data>
# Example: git_ssh_key_secret_name = airflow-secrets
git_ssh_key_secret_name =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     known_hosts: |
#         github.com ssh-rsa <...>
#     airflow.cfg: |
#         ...
# Example: git_ssh_known_hosts_configmap_name = airflow-configmap
git_ssh_known_hosts_configmap_name =

# To give the git_sync init container credentials via a secret, create a secret
# with two fields: GIT_SYNC_USERNAME and GIT_SYNC_PASSWORD (example below) and
# add ``git_sync_credentials_secret = <secret_name>`` to your airflow config under the
# ``kubernetes`` section
#
# Secret Example:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: git-credentials
#   data:
#     GIT_SYNC_USERNAME: <base64_encoded_git_username>
#     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
git_sync_credentials_secret =

# For cloning DAGs from git repositories into volumes: https://github.com/kubernetes/git-sync
git_sync_container_repository = k8s.gcr.io/git-sync
git_sync_container_tag = v3.1.1
git_sync_init_container_name = git-sync-clone
git_sync_run_as_user = 65533

# The name of the Kubernetes service account to be associated with airflow workers, if any.
# Service accounts are required for workers that require access to secrets or cluster resources.
# See the Kubernetes RBAC documentation for more:
# https://kubernetes.io/docs/admin/authorization/rbac/
worker_service_account_name =

# Any image pull secrets to be given to worker pods, If more than one secret is
# required, provide a comma separated list: secret_a,secret_b
image_pull_secrets =

# GCP Service Account Keys to be provided to tasks run on Kubernetes Executors
# Should be supplied in the format: key-name-1:key-path-1,key-name-2:key-path-2
gcp_service_account_keys =

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has.
# cluster_context =
# config_file =

# Affinity configuration as a single line formatted JSON object.
# See the affinity model for top-level key names (e.g. ``nodeAffinity``, etc.):
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#affinity-v1-core
affinity =

# A list of toleration objects as a single line formatted JSON array
# See:
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#toleration-v1-core
tolerations =

# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis.
# See:
# https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
# Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely
# for kubernetes api responses, which will cause the scheduler to hang.
# The timeout is specified as [connect timeout, read timeout]
kube_client_request_args = {{"_request_timeout" : [60,60] }}

# Specifies the uid to run the first process of the worker pods containers as
run_as_user =

# Specifies a gid to associate with all containers in the worker pods
# if using a git_ssh_key_secret_name use an fs_group
# that allows for the key to be read, e.g. 65533
fs_group =

[kubernetes_node_selectors]

# The Key-value pairs to be given to worker pods.
# The worker pods will be scheduled to the nodes of the specified key-value pairs.
# Should be supplied in the format: key = value

[kubernetes_annotations]

# The Key-value annotations pairs to be given to worker pods.
# Should be supplied in the format: key = value

[kubernetes_environment_variables]

# The scheduler sets the following environment variables into your workers. You may define as
# many environment variables as needed and the kubernetes launcher will set them in the launched workers.
# Environment variables in this section are defined as follows
# ``<environment_variable_key> = <environment_variable_value>``
#
# For example if you wanted to set an environment variable with value `prod` and key
# ``ENVIRONMENT`` you would follow the following format:
# ENVIRONMENT = prod
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_secrets]

# The scheduler mounts the following secrets into your workers as they are launched by the
# scheduler. You may define as many secrets as needed and the kubernetes launcher will parse the
# defined secrets and mount them as secret environment variables in the launched workers.
# Secrets in this section are defined as follows
# ``<environment_variable_mount> = <kubernetes_secret_object>=<kubernetes_secret_key>``
#
# For example if you wanted to mount a kubernetes secret key named ``postgres_password`` from the
# kubernetes secret object ``airflow-secret`` as the environment variable ``POSTGRES_PASSWORD`` into
# your workers you would follow the following format:
# ``POSTGRES_PASSWORD = airflow-secret=postgres_credentials``
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_labels]

# The Key-value pairs to be given to worker pods.
# The worker pods will be given these static labels, as well as some additional dynamic labels
# to identify the task.
# Should be supplied in the format: ``key = value``
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\dags\kbxanalyticsdlservicenow_connections.py<content=>
from os import getenv
from airflow import DAG, settings
from airflow.models import Connection
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator

def ListConnections():
    return settings.Session().query(Connection)

def CreateConnections():
    try:
        # Build a connection object:
        conn = Connection(
            # conn_id: Name of the connection as displayed in the Airflow UI.
			# Snake-case; prefix with the product name.
            conn_id="kbxanalyticsdlservicenow_aws",
            # conn_type: The type of connection to create.
            # Valid conn_type values are: "azure_cosmos", "azure_data_lake", "cassandra", "cloudant", 
            # "docker", "gcpcloudsql", "google_cloud_platform", "grpc", "hive_cli", "hiveserver2", 
            # "jdbc", "jira", "mongo", "mssql", "mysql", "oracle", "pig_cli", "postgres", "presto",
            # "redis", "sqlite", "vertica", "wasb".
            conn_type="aws", 
            # host: Endpoint at which the resource exists; URL, IP address, etc.
            host="", 
            login="ChangeMeLater", 
            # Leave the password property value as-is; this will be updated via the Airflow UI.
            password="ChangeMeLater"
            # port: The port to use when creating a database type connection.
            # port=1234,
            # schema: The schema to use when creating a database type connection.
            # schema="schema"
            # extra: Used to specify additional connection type specific settings.
            # Refer to https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html for more details.
            # extra="json-formatted string"
        )

        # Get the current Airflow session:
        session = settings.Session()
        # Add the connection to the session if it doesn't already exist:
        if(conn.conn_id not in ListConnections()):
            session.add(conn)
            # Commit the newly-created connection:
            session.commit()
            return True
            
        return False
    except:
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['rahul.roy@kochgs.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlservicenow_connections', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_connections",
        #provide_context=False,
        python_callable=CreateConnections
    )

    po
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\dags\kbxanalyticsdlservicenow_variables.py<content=>
from os import getenv
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator
from airflow.models import Variable

def SetVariables():
    try:
        # Multiple variables may be set via multiple Variable.set() calls.
        # This variale DAG can be run once manually for static values or leverage
        # schedules to set variable values based on the results of other operations.
        Variable.set("kbxanalyticsdlservicenow_environment", 'dev')
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlservicenow_variables', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_variables",
        #provide_context=False,
        python_callable=SetVariables
    )

    po
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\dags\kbxtanalyticsdlservicenow_airflow.py<content=>
from os import getenv
# from tkinter import Variable
import airflow
from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.operators.glue import AwsGlueJobOperator
from ast import literal_eval
from airflow.models import Variable

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals

# Same as File Name
DAG_ID = 'kbxtanalyticsdlservicenow_airflow'
# When it should be turned on, not execution date.
START_DATE = airflow.utils.dates.days_ago(1)
# How often to Run. @daily - Once a day at Midnight
SCHEDULE_INTERVAL = '30 06 * * *'
# Who is listed as the owner of this DAG in the Airflow Web Server
DAG_OWNER_NAME = "KBX"
# List of email address to send email alerts to if this job fails

AWS_CONN_ID = 'kbxanalyticsdlservicenow_aws'
ENV = Variable.get('kbxanalyticsdlservicenow_environment')

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': START_DATE,
    'catchup': False
}

with DAG(
    DAG_ID, 
    schedule_interval=SCHEDULE_INTERVAL,
    default_args=default_args
    ) as dag:

    po_incident_raw_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-servicenow-incident-raw-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-servicenow-incident-raw-job-{ENV}",
        dag=dag,
        retries = 5,
        aws_conn_id=AWS_CONN_ID
    )

    po_incident_structured_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-servicenow-incident-structured-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-servicenow-incident-structured-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    po_incident_curated_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-servicenow-incident-curated-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-servicenow-incident-curated-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    po_task_raw_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-servicenow-task-raw-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-servicenow-task-raw-job-{ENV}",
        dag=dag,
        retries = 5,
        aws_conn_id=AWS_CONN_ID
    )

    po_task_structured_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-servicenow-task-structured-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-servicenow-task-structured-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    po_task_curated_job = AwsGlueJobOperator(
        task_id=f"execute_kbxt-dl-analytics-servicenow-task-curated-job-{ENV}", 
        job_name=f"kbxt-dl-analytics-servicenow-task-curated-job-{ENV}",
        dag=dag,
        retries = 0,
        aws_conn_id=AWS_CONN_ID
    )

    po_incident_raw_job >> po_incident_structured_job >> po_incident_curated_job

    po_task_raw_job >> po_task_structured_job >> po_task_curated_job
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\examples\kbxanalyticsdlservicenow_example_get_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable
from ast import literal_eval

def GetVariable():
    print("Performing Variable.get() action...")
    keys=literal_eval(Variable.get("kbxanalyticsdlservicenow_variable"))
    print(keys)

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
    #'schedule_interval': '0/3 * * * ?'
}

with DAG("kbxanalyticsdlservicenow_example_get_variable", default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="get_variable",
        provide_context=False,
        python_callable=GetVariable
    )

    po
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\examples\kbxanalyticsdlservicenow_example_glue_test.py<content=>
from os import getenv

from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator

def ListGlueJobs():
    try:
        gh = AwsGlueJobHook(aws_conn_id="aws_lg_nonprod")
        print(gh.list_jobs())
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlservicenow_example_glue_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_glue_jobs",
        provide_context=False,
        python_callable=ListGlueJobs
    )

    po
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\examples\kbxanalyticsdlservicenow_example_s3_conn_test.py<content=>
"""
S3 Connection Test
"""
from airflow import DAG
from airflow.hooks.base_hook import BaseHook
from airflow.operators.python_operator import PythonOperator
import boto3
from datetime import *

def ListBuckets():
    try:
        s3Conn=boto3.client("s3")
        res = s3Conn.list_buckets()
        for bkt in res["Buckets"]:
            print(bkt["Name"])

        if(len(res["Buckets"]) > 0):
            return True
        return False
    except :
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kbxanalyticsdlservicenow_example_s3_conn_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_s3_buckets",
        provide_context=False,
        python_callable=ListBuckets
    )

    po
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\examples\kbxanalyticsdlservicenow_example_set_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable

def SetVariable():
    keys=["kbxanalyticsdlservicenow_0","1","2","3","4","5"]
    print("Performing Variable.get() action...")
    Variable.set("kbxanalyticsdlservicenow_variable", keys)
    
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG("kbxanalyticsdlservicenow_example_set_variable", default_args=default_args) as dag:
    po = PythonOperator(task_id="set_variable",
        provide_context=False,
        python_callable=SetVariable
    )

    po
<path=>.\SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\plugins<content=>
<path=>SERVICENOW\Development\Orchestration\KBX.Analytics.DL.ServiceNow.Workflow\script\entrypoint.sh<content=>
#!/usr/bin/env bash

# User-provided configuration must always be respected.
#
# Therefore, this script must only derives Airflow AIRFLOW__ variables from other variables
# when the user did not provide their own configuration.

TRY_LOOP="20"

# Global defaults and back-compat
: "${AIRFLOW_HOME:="/usr/local/airflow"}"
: "${AIRFLOW__CORE__FERNET_KEY:=${FERNET_KEY:=$(python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)")}}"
: "${AIRFLOW__CORE__EXECUTOR:=${EXECUTOR:-Sequential}Executor}"

# Load DAGs examples (default: Yes)
if [[ -z "$AIRFLOW__CORE__LOAD_EXAMPLES" && "${LOAD_EX:=n}" == n ]]; then
  AIRFLOW__CORE__LOAD_EXAMPLES=False
fi

export \
  AIRFLOW_HOME \
  AIRFLOW__CORE__EXECUTOR \
  AIRFLOW__CORE__FERNET_KEY \
  AIRFLOW__CORE__LOAD_EXAMPLES \

# Install custom python package if requirements.txt is present
if [ -e "/requirements.txt" ]; then
    $(command -v pip) install --user -r /requirements.txt
fi

wait_for_port() {
  local name="$1" host="$2" port="$3"
  local j=0
  while ! nc -z "$host" "$port" >/dev/null 2>&1 < /dev/null; do
    j=$((j+1))
    if [ $j -ge $TRY_LOOP ]; then
      echo >&2 "$(date) - $host:$port still not reachable, giving up"
      exit 1
    fi
    echo "$(date) - waiting for $name... $j/$TRY_LOOP"
    sleep 5
  done
}

# Other executors than SequentialExecutor drive the need for an SQL database, here PostgreSQL is used
if [ "$AIRFLOW__CORE__EXECUTOR" != "SequentialExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the database
  if [ -z "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" ]; then
    # Default values corresponding to the default compose files
    : "${POSTGRES_HOST:="postgres"}"
    : "${POSTGRES_PORT:="5432"}"
    : "${POSTGRES_USER:="airflow"}"
    : "${POSTGRES_PASSWORD:="airflow"}"
    : "${POSTGRES_DB:="airflow"}"
    : "${POSTGRES_EXTRAS:-""}"

    AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
    export AIRFLOW__CORE__SQL_ALCHEMY_CONN

    # Check if the user has provided explicit Airflow configuration for the broker's connection to the database
    if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
      AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
      export AIRFLOW__CELERY__RESULT_BACKEND
    fi
  else
    if [[ "$AIRFLOW__CORE__EXECUTOR" == "CeleryExecutor" && -z "$AIRFLOW__CELERY__RESULT_BACKEND" ]]; then
      >&2 printf '%s\n' "FATAL: if you set AIRFLOW__CORE__SQL_ALCHEMY_CONN manually with CeleryExecutor you must also set AIRFLOW__CELERY__RESULT_BACKEND"
      exit 1
    fi

    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    POSTGRES_ENDPOINT=$(echo -n "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" | cut -d '/' -f3 | sed -e 's,.*@,,')
    POSTGRES_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    POSTGRES_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Postgres" "$POSTGRES_HOST" "$POSTGRES_PORT"
fi

# CeleryExecutor drives the need for a Celery broker, here Redis is used
if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the broker
  if [ -z "$AIRFLOW__CELERY__BROKER_URL" ]; then
    # Default values corresponding to the default compose files
    : "${REDIS_PROTO:="redis://"}"
    : "${REDIS_HOST:="redis"}"
    : "${REDIS_PORT:="6379"}"
    : "${REDIS_PASSWORD:=""}"
    : "${REDIS_DBNUM:="1"}"

    # When Redis is secured by basic auth, it does not handle the username part of basic auth, only a token
    if [ -n "$REDIS_PASSWORD" ]; then
      REDIS_PREFIX=":${REDIS_PASSWORD}@"
    else
      REDIS_PREFIX=
    fi

    AIRFLOW__CELERY__BROKER_URL="${REDIS_PROTO}${REDIS_PREFIX}${REDIS_HOST}:${REDIS_PORT}/${REDIS_DBNUM}"
    export AIRFLOW__CELERY__BROKER_URL
  else
    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    REDIS_ENDPOINT=$(echo -n "$AIRFLOW__CELERY__BROKER_URL" | cut -d '/' -f3 | sed -e 's,.*@,,')
    REDIS_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    REDIS_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Redis" "$REDIS_HOST" "$REDIS_PORT"
fi

case "$1" in
  webserver)
    airflow initdb
    if [ "$AIRFLOW__CORE__EXECUTOR" = "LocalExecutor" ] || [ "$AIRFLOW__CORE__EXECUTOR" = "SequentialExecutor" ]; then
      # With the "Local" and "Sequential" executors it should all run in one container.
      airflow scheduler &
    fi
    exec airflow webserver
    ;;
  worker|scheduler)
    # Give the webserver time to run initdb.
    sleep 10
    exec airflow "$@"
    ;;
  flower)
    sleep 10
    exec airflow "$@"
    ;;
  version)
    exec airflow "$@"
    ;;
  *)
    # The command is something like bash, not an airflow subcommand. Just run it in the right environment.
    exec "$@"
    ;;
esac
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\.dockerignore<content=>
.git
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\.gitignore<content=>
### Vim ###
[._]*.s[a-w][a-z]
[._]s[a-w][a-z]
*.un~
Session.vim
.netrwhist
*~

### SublimeText ###
# cache files for sublime text
*.tmlanguage.cache
*.tmPreferences.cache
*.stTheme.cache

# workspace files are user-specific
*.sublime-workspace

# project files should be checked into the repository, unless a significant
# proportion of contributors will probably not be using SublimeText
# *.sublime-project

# sftp configuration file
sftp-config.json

# Python
__pycache__
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-airflow-$(environment)' 
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: 'deployments/$(Build.Repository.Name)/'
 
stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**/!(*_example_*)'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Copy
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)dags'
        globExpressions: '**/!(*_example_*)'
        targetFolder: 'dags'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'text/plain'
        cacheControl: 'max-age=0'


<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\CodeTemplateDatalakeReadme.md<content=>
# Creating a template from KBX.DL.CodeTemplates

- Run powershell as administrator

- Clone this repo locally
    ```POWERSHELL
    git clone https://kbxltrans@dev.azure.com/kbxltrans/Platform-Data/_git/KBX.DL.CodeTemplates.Workflow
    ```

- Execute the powershell script **CreateNewDatalakeProjectFromCodeTemplate.ps1** and install wtforms.

    ```POWERSHELL
    ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.[ProductName].DL.[DomainName] -LOAD_EX 
    ```

    ```POWERSHELL
    pip3 install wtforms==2.3.3
    ```

- Navigate to the new solutions directory

    ```POWERSHELL
    cd ..\KBX.[ProductName].DL.[DomainName]
    ```

**NOTE**: When the script is downloaded you may get an error about running the script because its unsigned. To allow the script to run execute the following
```POWERSHELL
unblock-file -path CreateNewDatalakeProjectFromCodeTemplate.ps1
```

## Project Setup
- Update the docker-compose.yaml replace the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_DEFAULT_REGION with your values, if your job needs to access AWS.

## Project ReadMe Files
- Review the ReadMe.md file

## Run Solution
- Bring up containers
```POWERSHELL
  docker-compose -f docker-compose.yml up
```
- Navigate to the Administration at http://localhost:8080/
- Bring down containers "CTRL +c"
```POWERSHELL
  docker-compose -f docker-compose.yml down
```

## Project Pipeline Setup
This will deploy the "dags" folder to an Airflow environment, which must exist prior to the pipeline deployment.
- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Congrats
- You have completed setup of your solution.  Please remove this file.<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\CreateNewDatalakeProjectFromCodeTemplate.ps1<content=>
<#
.SYNOPSIS
Rename all the template files to a new project name

.PARAMETER ProjectName
The Name of the Project. This takes on the naming convention of KBX.[Product].DL.[Domain]  ex) KBX.eDock.DL.Legacy

.PARAMETER LOAD_EX
Load examples. OPTIONS: y, n

.EXAMPLE
. ./CreateNewDatalakeProjectFromCodeTemplate.ps1 -ProjectName KBX.eDock.DL.Legacy -LOAD_EX y

#>

Param
(
	[Parameter(Mandatory = $true, HelpMessage = "Enter project name. Format: KBX.[Product].[Domain]:")]
	[String]
	$ProjectName,

	[Parameter(Mandatory = $true, HelpMessage = "Do you want to load examples (ex. y):")]
	[String]
	$LOAD_EX #OPTIONS: y, n
)

$TemplateProject = "KBX.DL.CodeTemplates.Workflow"
$EntityName = ""

#Change these to accomidate new templates
$oldProjectName = "KBX.DL.DEV.TEMPLATE"
$replacementEntityName = "DEVTEMPLATE"
$replacementEntityNameLower = "devtemplate"
$replacementEntityNamePascal = "DevTemplate"
$replacementEntityNameUpper = "DEVTEMPLATE"
$newEntityNameLower = $EntityName.ToLower()
$newEntityNameUpper = $EntityName.ToUpper()
$replacementLoadExLower = "devtemplateloadex"
$replacementProjectAliasLower = "projectnametemplatereplace"
$newProjectAliasLower = $ProjectName.ToLower().Replace('.',"").Replace('_',"").Replace('-',"")

$replacementProductName = "\[PRODUCT\]"
$replacementProductNameLower = "\[product\]"
$productName = $ProjectName.Split(".")[1]
$productNameLower = $productName.ToLower()

#Dont change below this comment
$excludedFoldersNames = @("node_modules", "bin", "obj", "Packages", "TestResults", ".vs", ".Resharper", ".git")
$excludedFiles = @("nomatch.txt")
$excludedTypes = @("*.jpg", "*.ico", "*.gif", "*.svg")
$templateFiles = @("CodeTemplateDatalakeReadme.md", "CreateNewDatalakeProjectFromCodeTemplate.ps1")

$itemCounter = 0
$TemplateToClonePath = "..\$TemplateProject"
$RepoFilePath = "..\"
$TemplateType = ([string]$TemplateProject).replace("KBX.DL.CodeTemplates", "")
$FullProjectName = "$ProjectName$TemplateType"
$Destination = "$RepoFilePath\$FullProjectName"
Write-Host $Destination
$templatePath = Resolve-Path $TemplateToClonePath
$Already = Test-Path "$Destination"
$lastExitCode = 0

If ($Already -eq $True) {
	Write-Error "Project already exists" -ErrorAction:Stop
}
If ( (Test-Path "$templatePath") -eq $False) {
	Write-Error "Invalid TemplateProject Provided" -ErrorAction:Stop
}
New-Item -Path $RepoFilePath -Name "$FullProjectName" -ItemType directory | Out-Null

$to = (Resolve-Path "$Destination").Path
$from = (Resolve-Path "$TemplateToClonePath").Path

Write-Host "Cloning template files into new project folder..." -ForegroundColor White -BackgroundColor Blue

$matchString = $("\\" + ($excludedFoldersNames -join "\\|\\") + "\\")
#append for forward slash folders on UNIX based systems, MacOS, Linux
$matchString = $matchString + $("/" + ($excludedFoldersNames -join "/|/") + "/")
$dirsToProcess = Get-ChildItem -Path $from -Directory -Recurse |
Where-Object { ($_.PSIsContainer) -and ($_.FullName -notmatch $matchString ) }

Write-Host "Cloning project files..."
foreach ($dir in $dirsToProcess) {
	if ($excludedFoldersNames -notcontains $dir.Name) {
		$newPath = Join-Path $to $dir.Parent.FullName.Substring($from.length)
		$newFullPath = Join-Path $to $dir.FullName.Substring($from.length)
		If ((Test-Path $newFullPath) -eq $False) {
			New-Item -Path $newPath -name $dir.Name -ItemType "directory" | Out-Null
		}
		Get-ChildItem -Path $dir.FullName -File |
		Where-Object { $excludedFiles -notcontains $_.Name } |
		Select-Object -expandproperty FullName |
		Copy-Item -Destination {
			Join-Path $to $_.Substring($from.length)
		} -Force
	}
}

Write-Host "Cloning solution files..."
Get-ChildItem -Path $from -File |
Where-Object { $excludedFiles -notcontains $_.Name } |
Select-Object -expandproperty FullName |
Copy-Item -Destination $to -Force

Write-Host "Processing template files..." -ForegroundColor White -BackgroundColor Blue

Write-Host "Renaming folders..."
Get-ChildItem -Path $Destination -Filter "*$($oldProjectName)*" -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $oldProjectName, $ProjectName) }
Get-ChildItem -Path $Destination -Filter "*$($replacementEntityNamePascal)*" -Recurse -Directory | ForEach-Object -Process { Rename-Item -Path $_.FullName -NewName ($_.name -replace $replacementEntityNamePascal, $EntityName) }

Write-Host "Renaming files..."
Get-ChildItem -Path $Destination -Filter *.sln | Rename-Item -NewName { $_.name -replace $oldProjectName, $ProjectName }
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($oldProjectName)", $ProjectName } -PassThru | ForEach-Object -Process {
	$itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementProjectAliasLower)", $newProjectAliasLower } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}
Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes | Rename-Item -NewName { $_.name -replace "$($replacementEntityName)", $EntityName } -PassThru | ForEach-Object -Process {
    $itemCounter = $itemCounter + 1
}

if ($LOAD_EX -eq 'y') {
	Write-Host "Loading examples..."
	Copy-Item -Path "$to\examples\*" -Destination "$to\dags" -Recurse
}

Write-Host "Scanning file contents for replacements..."
$Items = Get-ChildItem -Path $Destination -File -Recurse -Exclude $excludedTypes

#order of the replaces matters
$Items | ForEach-Object -Process {
	$i++
	Write-Progress -Activity "Scanning file contents for replacements" -Status "$i% Complete:" -PercentComplete ($i / $itemCounter * 100)
	(Get-Content $_.PSPath) |
	Foreach-Object { $_ -creplace $oldProjectName, $ProjectName -creplace $replacementProjectAliasLower, $newProjectAliasLower -creplace $replacementLoadExLower, $LOAD_EX -creplace $replacementEntityNameLower, $newEntityNameLower -creplace $replacementEntityNameUpper, $newEntityNameUpper -creplace $replacementEntityNamePascal, $EntityName -creplace $replacementEntityName, $EntityName -creplace $replacementProductNameLower, $productNameLower -creplace $replacementProductName, $productName  } |
	Set-Content $_.PSPath
}

Write-Progress -Activity "Scanning file contents for replacements" -Completed

Write-Host "Removing template PowerShell script and readme..."
$templateFiles | ForEach-Object -Process {
	Remove-Item "$Destination\$_" -Force
}

If ($lastExitCode -eq "0") {
	Write-Host "$ProjectName Has Been Created" -ForegroundColor White -BackgroundColor Green
}
else {
	Write-Host "$ProjectName Has Been Created With Errors. Code: $($lastExitCode)" -ForegroundColor White -BackgroundColor Red
}













<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\docker-compose.yml<content=>
version: '3.7'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            options:
                max-size: 10m
                max-file: "3"

    webserver:
        image: puckel/docker-airflow:latest
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=devtemplateloadex
            - EXECUTOR=Local
            # AWS
            # Only used for local development
            - AWS_ACCESS_KEY_ID=
            - AWS_SECRET_ACCESS_KEY=
            - AWS_DEFAULT_REGION=us-east-1
            # Encryption
            # Can be static, it's only used for local development
            - FERNET_KEY=lUDB1r1SNvYy4kZAhA-4z8PqC0x8UQfYmo4uFP8UFcg=
        logging:
            options:
                max-size: 10m
                max-file: "3"
        volumes:
            - ./dags:/usr/local/airflow/dags
            - ./plugins:/usr/local/airflow/plugins
            - ./requirements.txt:/requirements.txt
        ports:
            - "8080:8080"
        build: .
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\Dockerfile<content=>
# VERSION 1.10.9
# AUTHOR: Matthieu "Puckel_" Roisil
# DESCRIPTION: Basic Airflow container
# BUILD: docker build --rm -t puckel/docker-airflow .
# SOURCE: https://github.com/puckel/docker-airflow

FROM python:3.7-slim-buster
LABEL maintainer="KBX"

# Never prompt the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux

# Airflow
ARG AIRFLOW_VERSION=1.10.9
ARG AIRFLOW_USER_HOME=/usr/local/airflow
ARG AIRFLOW_DEPS=""
ARG PYTHON_DEPS=""
ENV AIRFLOW_HOME=${AIRFLOW_USER_HOME}

# Define en_US.
ENV LANGUAGE en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LC_ALL en_US.UTF-8
ENV LC_CTYPE en_US.UTF-8
ENV LC_MESSAGES en_US.UTF-8

# Disable noisy "Handling signal" log messages:
# ENV GUNICORN_CMD_ARGS --log-level WARNING

RUN set -ex \
    && buildDeps=' \
        freetds-dev \
        libkrb5-dev \
        libsasl2-dev \
        libssl-dev \
        libffi-dev \
        libpq-dev \
        git \
    ' \
    && apt-get update -yqq \
    && apt-get upgrade -yqq \
    && apt-get install -yqq --no-install-recommends \
        $buildDeps \
        freetds-bin \
        build-essential \
        default-libmysqlclient-dev \
        apt-utils \
        curl \
        rsync \
        netcat \
        locales \
    && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \
    && locale-gen \
    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \
    && useradd -ms /bin/bash -d ${AIRFLOW_USER_HOME} airflow \
    && pip install -U pip setuptools wheel \
    && pip install pytz \
    && pip install pyOpenSSL \
    && pip install ndg-httpsclient \
    && pip install pyasn1 \
    && pip install apache-airflow[crypto,celery,postgres,hive,jdbc,mysql,ssh${AIRFLOW_DEPS:+,}${AIRFLOW_DEPS}]==${AIRFLOW_VERSION} \
    && pip install 'redis==3.2' \
    && if [ -n "${PYTHON_DEPS}" ]; then pip install ${PYTHON_DEPS}; fi \
    && apt-get purge --auto-remove -yqq $buildDeps \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf \
        /var/lib/apt/lists/* \
        /tmp/* \
        /var/tmp/* \
        /usr/share/man \
        /usr/share/doc \
        /usr/share/doc-base

COPY script/entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_USER_HOME}/airflow.cfg

RUN chown -R airflow: ${AIRFLOW_USER_HOME}

EXPOSE 8080 5555 8793

RUN pip install apache-airflow-backport-providers-amazon \
 && pip install apache-airflow-backport-providers-apache-spark \
 && pip install apache-airflow-backport-providers-jdbc \
 && pip install apache-airflow-backport-providers-datadog \
 && pip install apache-airflow-backport-providers-postgres \
 && pip install sqlalchemy==1.3.13

USER airflow
WORKDIR ${AIRFLOW_USER_HOME}
ENTRYPOINT ["/entrypoint.sh"]
CMD ["webserver"]
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\LICENSE<content=>
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2017 Matthieu "Puckel_" Roisil

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\package-lock.json<content=>
{
  "name": "KBX.DL.CodeTemplates.Airflow",
  "lockfileVersion": 2,
  "requires": true,
  "packages": {}
}
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\package.json<content=>
{}
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\README.md<content=>
# docker-airflow

This repository contains **Dockerfile** of [apache-airflow](https://github.com/apache/incubator-airflow) for [Docker](https://www.docker.com/)'s [automated build](https://registry.hub.docker.com/u/puckel/docker-airflow/) published to the public [Docker Hub Registry](https://registry.hub.docker.com/).

## Information

* Based on Python (3.7-slim-buster) official Image [python:3.7-slim-buster](https://hub.docker.com/_/python/) and uses the official [Postgres](https://hub.docker.com/_/postgres/) as backend and [Redis](https://hub.docker.com/_/redis/) as queue
* Install [Docker](https://www.docker.com/)
* Install [Docker Compose](https://docs.docker.com/compose/install/)
* This follows the Airflow release from this page [Python Package Index](https://pypi.python.org/pypi/apache-airflow)

## Installation

This solution has been created by a code template.

## Build

Set any necessary environment variables in the docker-compose.yml file

* LOAD_EX can be set to "n" if you do not want to deploy the sample DAGs
* AWS_ACCESS_KEY_ID should be set to an appropriate AWS access key Id
* AWS_SECRET_ACCESS_KEY should be set to the key value of the aforementioned AWS access key Id

Compose and start a container from the docker-compose.yml file:

* docker-compose -f docker-compose.yml up

A docker container will now be visible and manageable in Docker Desktop.
The Airflow Web UI can be accessed at: [localhost:8080](http://localhost:8080/)

## Configuration

* Use the variable template DAG to set Airflow variables which your other DAGs require.  Please refer to the [yourprojectname]_variables.py template DAG
* Use the connection template DAG to set Airflow connections; connections should be named [yourprojectname]_[resource]; for example, kbxdlcodetemplatesworkflow_aws for a project named KBX.DL.CodeTemplates.Workflow.  Please refer to the [yourprojectname]_connections.py template DAG

## Management

Stop and remove the container along with any associated resources created via the aforementioned "up" command:

* docker-compose -f docker-compose.yml down

List running containers:

* docker ps

Access the shell of a running container as the default user:

* docker exec -ti "container name" bash

Access the shell of a running container as root:

* docker exec -u 0 -ti "container name" bash

## Pitfalls

* **Connections and Variables** are not persisted between containers unless you include them as environment variables in the docker-compose YAML file.
* Manually refreshing Airflow Web UI pages excessively will cause the instance to become temporarily unresponsive
* Ensure that the "dag" parameter is set for all operators to avoid vague error messages
* A DAG must be in the "On" state to be triggered by schedule or manually
* DAGs which leverage dynamic operator generation based on the results of a query will execute that query every time the "DAG bag" is filled or the DAG is accessed if the query exists within the same DAG.  The query which the dynamic operator generation relies on should exist in a separate DAG (scheduled as per requirements) where the results are written to an Airflow variable; this variable will then be referenced in the DAG containing dynamic operator generation
* Implementation of sub-DAGs requires explicitly setting the "schedule_interval" and "start_date" parameters of the parent DAG instead of passing them as part of the "default_args" parameter to avoid vague error messages
* Provider packages expose operators, hooks, and sensors but only the operators provide "out-of-the-box" functionality; resort to wiring up the hook and sensor only if an operator is unavailable or cannot meet a specific requirement
* For an unscheduled DAG either do not reference the "schedule_interval" property or set it to None (the type, not a string); for example, schedule_interval=None
* Setting the "start_date" parameter of any DAG to the current or a future date/time will cause the scheduler to fail; jobs will look as though they have started but not execute any tasks and be stuck in a "running" state
* **start_date** is counterintuitive but by design; for example, a job scheduled hourly and starting at 1400h will actually execute at 1500h.  All times in airflow are UTC and so don't execute at your local time.  Avoid using datetime.now() to offset this. Please see https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag and https://marclamberti.com/blog/apache-airflow-best-practices-1/ for further information

## References

* puckel Airflow Docker image documentation - https://hub.docker.com/r/puckel/docker-airflow
* Airflow core concepts - https://airflow.apache.org/docs/apache-airflow/stable/concepts/index.html
* Airflow best practices - https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html
* Airflow lesser-known tips and tricks - https://medium.com/datareply/airflow-lesser-known-tips-tricks-and-best-practises-cf4d4a90f8f<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\requirements.txt<content=>
# KBX does not support the automation of this file, create a help request
wtforms==2.3.3
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\config\airflow.cfg<content=>
[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Users must supply an Airflow connection id that provides access to the storage
# location.
remote_log_conn_id =
remote_base_log_folder =
encrypt_s3_logs = False

# Logging level
logging_level = INFO

# Logging level for Flask-appbuilder UI
fab_logging_level = WARN

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
# Example: logging_config_class = my.path.default_local_settings.LOGGING_CONFIG
logging_config_class =

# Flag to enable/disable Colored logs in Console
# Colour the logs when the controlling terminal is a TTY.
colored_console_log = True

# Log format for when Colored logs is enabled
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d}} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Format of Log line
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename format
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Name of handler to read task instance logs.
# Default to use task handler.
task_log_reader = task

# Hostname by providing a path to a callable, which will resolve the hostname.
# The format is "package:function".
#
# For example, default value "socket:getfqdn" means that result from getfqdn() of "socket"
# package will be used as hostname.
#
# No argument should be required in the function specified.
# If using IP address as hostname is preferred, use value ``airflow.utils.net:get_host_ip_address``
hostname_callable = socket:getfqdn

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = utc

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
# sql_alchemy_conn = sqlite:////tmp/airflow.db

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool. 0 indicates no limit.
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
# When the number of checked-out connections reaches the size set in pool_size,
# additional connections will be returned up to this limit.
# When those additional connections are returned to the pool, they are disconnected and discarded.
# It follows then that the total number of simultaneous connections the pool will allow
# is pool_size + max_overflow,
# and the total number of "sleeping" connections the pool will allow is pool_size.
# max_overflow can be set to -1 to indicate no overflow limit;
# no limit will be placed on the total number of concurrent connections. Defaults to 10.
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite. If the number of DB connections is ever exceeded,
# a lower config value will allow the system to recover faster.
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout.
# Typically, this is a simple statement like "SELECT 1".
# More information here:
# https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic
sql_alchemy_pool_pre_ping = True

# The schema to use for the metadata database.
# SqlAlchemy supports databases with the concept of multiple schemas.
sql_alchemy_schema =

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = True

# Where your Airflow plugins are stored
plugins_folder = /usr/local/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = $FERNET_KEY

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor, which processes a dag file
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a ``run_as_user`` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
default_impersonation =

# What security module to use (for example kerberos)
security =

# If set to False enables some unsecure features like Charts and Ad Hoc Queries.
# In 2.0 will default to True.
secure_mode = False

# Turn unit test mode on (overwrites many configuration options with test
# values at runtime)
unit_test_mode = False

# Whether to enable pickling for xcom (note that this is insecure and allows for
# RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
enable_xcom_pickling = True

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

# Whether to override params with dag_run.conf. If you pass some key-value pairs
# through ``airflow dags backfill -c`` or
# ``airflow dags trigger -c``, the key-value pairs will override the existing ones in params.
dag_run_conf_overrides_params = False

# Worker initialisation check to validate Metadata Database connection
worker_precheck = False

# When discovering DAGs, ignore any files that don't contain the strings ``DAG`` and ``airflow``.
dag_discovery_safe_mode = True

# The number of retries each task is going to have by default. Can be overridden at dag or task level.
default_task_retries = 0

# Whether to serialises DAGs and persist them in DB.
# If set to True, Webserver reads from DB instead of parsing DAG files
# More details: https://airflow.apache.org/docs/stable/dag-serialization.html
store_serialized_dags = False

# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate.
min_serialized_dag_update_interval = 30

# On each dagrun check against defined SLAs
check_slas = True

[cli]
# In what way should the cli access the API. The LocalClient will use the
# database directly, while the json_client will use the api running on the
# webserver
api_client = airflow.api.client.local_client

# If you set web_server_url_prefix, do NOT forget to append it here, ex:
# ``endpoint_url = http://localhost:8080/myroot``
# So api will look like: ``http://localhost:8080/myroot/api/experimental/...``
endpoint_url = http://localhost:8080

[debug]
# Used only with DebugExecutor. If set to True DAG will fail with first
# failed task. Helpful for debugging purposes.
fail_fast = False

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[lineage]
# what lineage backend to use
backend =

[atlas]
sasl_enabled = False
host =
port = 21000
username =
password =

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via ``default_args``
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[hive]
# Default mapreduce queue for HiveOperator tasks
default_hive_mapred_queue =

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_key =

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# Secret key used to run your flask app
# It should be as random as possible
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use. Choices include
# sync (default), eventlet, gevent
worker_class = sync

# Log files for the gunicorn webserver. '-' means log to stderr.
access_logfile = -

# Log files for the gunicorn webserver. '-' means log to stderr.
error_logfile = -

# Expose the configuration file in the web server
expose_config = True

# Expose hostname in the web server
expose_hostname = True

# Expose stacktrace in the web server
expose_stacktrace = True

# Set to true to turn on authentication:
# https://airflow.apache.org/security.html#web-authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode. Choices include user (default) and ldapgroup.
# Ldap group filtering requires using the ldap backend
#
# Note that the ldap server needs the "memberOf" overlay to be set up
# in order to user the ldapgroup mode.
owner_mode = user

# Default DAG view. Valid values are:
# tree, graph, duration, gantt, landing_times
dag_default_view = tree

# "Default DAG orientation. Valid values are:"
# LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Puts the webserver in demonstration mode; blurs the names of Operators for
# privacy.
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
# while fetching logs from other worker machine
log_fetch_timeout_sec = 5

# Time interval (in secs) to wait before next log fetching.
log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing.
log_auto_tailing_offset = 30

# Animation speed for auto tailing log display.
log_animation_speed = 1000

# By default, the webserver shows paused DAGs. Flip this to hide paused
# DAGs by default
hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
page_size = 100

# Use FAB-based webserver with RBAC feature
rbac = False

# Define the color of navigation bar
navbar_color = #007A87

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
enable_proxy_fix = False

# Number of values to trust for ``X-Forwarded-For``.
# More info: https://werkzeug.palletsprojects.com/en/0.16.x/middleware/proxy_fix/
proxy_fix_x_for = 1

# Number of values to trust for ``X-Forwarded-Proto``
proxy_fix_x_proto = 1

# Number of values to trust for ``X-Forwarded-Host``
proxy_fix_x_host = 1

# Number of values to trust for ``X-Forwarded-Port``
proxy_fix_x_port = 1

# Number of values to trust for ``X-Forwarded-Prefix``
proxy_fix_x_prefix = 1

# Set secure flag on session cookie
cookie_secure = False

# Set samesite policy on session cookie
cookie_samesite =

# Default setting for wrap toggle on DAG code and TI log views.
default_wrap = False

# Allow the UI to be rendered in a frame
x_frame_enabled = True

# Send anonymous user activity to your analytics tool
# choose from google_analytics, segment, or metarouter
# analytics_tool =

# Unique ID of your account in the analytics tool
# analytics_id =

# Update FAB permissions and sync security manager roles
# on webserver startup
update_fab_perms = True

# Minutes of non-activity before logged out from UI
# 0 means never get forcibly logged out
force_log_out_after = 0

# The UI cookie lifetime in days
session_lifetime_days = 30

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]

# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
# smtp_user =
# Example: smtp_password = airflow
# smtp_password =
smtp_port = 25
smtp_mail_from = airflow@example.com

[sentry]

# Sentry (https://docs.sentry.io) integration
sentry_dsn =

[celery]

# This section only applies if you are using the CeleryExecutor in
# ``[core]`` section above
# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# The maximum and minimum concurrency that will be used when starting workers with the
# ``airflow celery worker`` command (always keep minimum processes, but grow
# to maximum if necessary). Note the value should be max_concurrency,min_concurrency
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Example: worker_autoscale = 16,12
worker_autoscale = 16,12

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://redis:6379/1

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it ``airflow flower``. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# The root URL for Flower
# Example: flower_url_prefix = /flower
flower_url_prefix =

# This defines the port that Celery Flower runs on
flower_port = 5555

# Securing Flower with Basic Authentication
# Accepts user:password pairs separated by a comma
# Example: flower_basic_auth = user1:password1,user2:password2
flower_basic_auth =

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default

# How many processes CeleryExecutor uses to sync task state.
# 0 means to use max(1, number of cores - 1) processes.
sync_parallelism = 0

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# In case of using SSL
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

# Celery Pool implementation.
# Choices include: prefork (default), eventlet, gevent or solo.
# See:
# https://docs.celeryproject.org/en/latest/userguide/workers.html#concurrency
# https://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html
pool = prefork

# The number of seconds to wait before timing out ``send_task_to_executor`` or
# ``fetch_celery_task_state`` operations.
operation_timeout = 2

[celery_broker_transport_options]

# This section is for specifying options which can be passed to the
# underlying celery broker transport. See:
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_transport_options
# The visibility timeout defines the number of seconds to wait for the worker
# to acknowledge the task before the message is redelivered to another worker.
# Make sure to increase the visibility timeout to match the time of the longest
# ETA you're planning to use.
# visibility_timeout is only supported for Redis and SQS celery brokers.
# See:
# http://docs.celeryproject.org/en/master/userguide/configuration.html#std:setting-broker_transport_options
# Example: visibility_timeout = 21600
# visibility_timeout =

[dask]

# This section only applies if you are using the DaskExecutor in
# [core] section above
# The IP address and port of the Dask cluster's scheduler.
cluster_address = 127.0.0.1:8786

# TLS/ SSL settings to access a secured Dask scheduler.
tls_ca =
tls_cert =
tls_key =

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# After how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# The number of times to try to schedule each DAG file
# -1 indicates unlimited number
num_runs = -1

# The number of seconds to wait between consecutive DAG file processing
processor_poll_interval = 1

# after how much time (seconds) a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
dag_dir_list_interval = 300

# How often should stats be printed to the logs. Setting to 0 will disable printing stats
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
# This is used by the health check in the "/health" endpoint
scheduler_health_check_threshold = 30
child_process_log_directory = /usr/local/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is False,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
catchup_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# If this is too high, SQL query performance may be impacted by one
# or more of the following:
# - reversion to full table scan
# - complexity of query predicate
# - excessive locking
# Additionally, you may hit the maximum allowable query length for your db.
# Set this to 0 for no limit (not advised)
max_tis_per_query = 512

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# If you want to avoid send all the available metrics to StatsD,
# you can configure an allow list of prefixes to send only the metrics that
# start with the elements of the list (e.g: scheduler,executor,dagrun)
statsd_allow_list =

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
max_threads = 2
authenticate = False

# Turn off scheduler use of cron intervals by setting this to False.
# DAGs submitted manually in the web UI or with trigger_dag will still run.
use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
# Only has effect if schedule_interval is set to None in DAG
allow_trigger_in_future = False

[ldap]
# set this to ldaps://<your.ldap.server>:<port>
uri =
user_filter = objectClass=*
user_name_attr = uid
group_member_attr = memberOf
superuser_filter =
data_profiler_filter =
bind_user = cn=Manager,dc=example,dc=com
bind_password = insecure
basedn = dc=example,dc=com
cacert = /etc/ca/ldap_ca.crt
search_scope = LEVEL

# This setting allows the use of LDAP servers that either return a
# broken schema, or do not return a schema.
ignore_malformed_schema = False

[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

# Number of cpu cores required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_cpu = 1

# Memory in MB required for running one task instance using
# 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
# command on a mesos slave
task_memory = 256

# Enable framework checkpointing for mesos
# See http://mesos.apache.org/documentation/latest/slave-recovery/
checkpoint = False

# Failover timeout in milliseconds.
# When checkpointing is enabled and this option is set, Mesos waits
# until the configured timeout for
# the MesosExecutor framework to re-register after a failover. Mesos
# shuts down running tasks if the
# MesosExecutor framework fails to re-register within this timeframe.
# Example: failover_timeout = 604800
# failover_timeout =

# Enable framework authentication for mesos
# See http://mesos.apache.org/documentation/latest/configuration/
authenticate = False

# Mesos credentials, if authentication is enabled
# Example: default_principal = admin
# default_principal =
# Example: default_secret = admin
# default_secret =

# Optional Docker Image to run on slave before running the command
# This image should be accessible from mesos slave i.e mesos slave
# should be able to pull this docker image before executing the command.
# Example: docker_image_slave = puckel/docker-airflow
# docker_image_slave =

[kerberos]
ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
api_rev = v3

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

[elasticsearch]
# Elasticsearch host
host =

# Format of the log_id, which is used to query for a given tasks logs
log_id_template = {{dag_id}}-{{task_id}}-{{execution_date}}-{{try_number}}

# Used to mark the end of a log stream for a task
end_of_log_mark = end_of_log

# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
# Code will construct log_id using the log_id template from the argument above.
# NOTE: The code will prefix the https:// automatically, don't include that here.
frontend =

# Write the task logs to the stdout of the worker, rather than the default files
write_stdout = False

# Instead of the default log formatter, write the log lines as JSON
json_format = False

# Log fields to also attach to the json output, if enabled
json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
use_ssl = False
verify_certs = True

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository =
worker_container_tag =
worker_container_image_pull_policy = IfNotPresent

# If True (default), worker pods will be deleted upon termination
delete_worker_pods = True

# Number of Kubernetes Worker Pod creation calls per scheduler loop
worker_pods_creation_batch_size = 1

# The Kubernetes namespace where airflow workers should be created. Defaults to ``default``
namespace = default

# The name of the Kubernetes ConfigMap containing the Airflow Configuration (this file)
# Example: airflow_configmap = airflow-configmap
airflow_configmap =

# The name of the Kubernetes ConfigMap containing ``airflow_local_settings.py`` file.
#
# For example:
#
# ``airflow_local_settings_configmap = "airflow-configmap"`` if you have the following ConfigMap.
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     airflow_local_settings.py: |
#         def pod_mutation_hook(pod):
#             ...
#     airflow.cfg: |
#         ...
# Example: airflow_local_settings_configmap = airflow-configmap
airflow_local_settings_configmap =

# For docker image already contains DAGs, this is set to ``True``, and the worker will
# search for dags in dags_folder,
# otherwise use git sync or dags volume claim to mount DAGs
dags_in_image = False

# For either git sync or volume mounted DAGs, the worker will look in this subpath for DAGs
dags_volume_subpath =

# For DAGs mounted via a volume claim (mutually exclusive with git-sync and host path)
dags_volume_claim =

# For volume mounted logs, the worker will look in this subpath for logs
logs_volume_subpath =

# A shared volume claim for the logs
logs_volume_claim =

# For DAGs mounted via a hostPath volume (mutually exclusive with volume claim and git-sync)
# Useful in local environment, discouraged in production
dags_volume_host =

# A hostPath volume for the logs
# Useful in local environment, discouraged in production
logs_volume_host =

# A list of configMapsRefs to envFrom. If more than one configMap is
# specified, provide a comma separated list: configmap_a,configmap_b
env_from_configmap_ref =

# A list of secretRefs to envFrom. If more than one secret is
# specified, provide a comma separated list: secret_a,secret_b
env_from_secret_ref =

# Git credentials and repository for DAGs mounted via Git (mutually exclusive with volume claim)
git_repo =
git_branch =
git_subpath =

# The specific rev or hash the git_sync init container will checkout
# This becomes GIT_SYNC_REV environment variable in the git_sync init container for worker pods
git_sync_rev =

# Use git_user and git_password for user authentication or git_ssh_key_secret_name
# and git_ssh_key_secret_key for SSH authentication
git_user =
git_password =
git_sync_root = /git
git_sync_dest = repo

# Mount point of the volume if git-sync is being used.
# i.e. /usr/local/airflow/dags
git_dags_folder_mount_point =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-secrets.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: airflow-secrets
#   data:
#     # key needs to be gitSshKey
#     gitSshKey: <base64_encoded_data>
# Example: git_ssh_key_secret_name = airflow-secrets
git_ssh_key_secret_name =

# To get Git-sync SSH authentication set up follow this format
#
# ``airflow-configmap.yaml``:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: airflow-configmap
#   data:
#     known_hosts: |
#         github.com ssh-rsa <...>
#     airflow.cfg: |
#         ...
# Example: git_ssh_known_hosts_configmap_name = airflow-configmap
git_ssh_known_hosts_configmap_name =

# To give the git_sync init container credentials via a secret, create a secret
# with two fields: GIT_SYNC_USERNAME and GIT_SYNC_PASSWORD (example below) and
# add ``git_sync_credentials_secret = <secret_name>`` to your airflow config under the
# ``kubernetes`` section
#
# Secret Example:
#
# .. code-block:: yaml
#
#   ---
#   apiVersion: v1
#   kind: Secret
#   metadata:
#     name: git-credentials
#   data:
#     GIT_SYNC_USERNAME: <base64_encoded_git_username>
#     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
git_sync_credentials_secret =

# For cloning DAGs from git repositories into volumes: https://github.com/kubernetes/git-sync
git_sync_container_repository = k8s.gcr.io/git-sync
git_sync_container_tag = v3.1.1
git_sync_init_container_name = git-sync-clone
git_sync_run_as_user = 65533

# The name of the Kubernetes service account to be associated with airflow workers, if any.
# Service accounts are required for workers that require access to secrets or cluster resources.
# See the Kubernetes RBAC documentation for more:
# https://kubernetes.io/docs/admin/authorization/rbac/
worker_service_account_name =

# Any image pull secrets to be given to worker pods, If more than one secret is
# required, provide a comma separated list: secret_a,secret_b
image_pull_secrets =

# GCP Service Account Keys to be provided to tasks run on Kubernetes Executors
# Should be supplied in the format: key-name-1:key-path-1,key-name-2:key-path-2
gcp_service_account_keys =

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has.
# cluster_context =
# config_file =

# Affinity configuration as a single line formatted JSON object.
# See the affinity model for top-level key names (e.g. ``nodeAffinity``, etc.):
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#affinity-v1-core
affinity =

# A list of toleration objects as a single line formatted JSON array
# See:
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.12/#toleration-v1-core
tolerations =

# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis.
# See:
# https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
# Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely
# for kubernetes api responses, which will cause the scheduler to hang.
# The timeout is specified as [connect timeout, read timeout]
kube_client_request_args = {{"_request_timeout" : [60,60] }}

# Specifies the uid to run the first process of the worker pods containers as
run_as_user =

# Specifies a gid to associate with all containers in the worker pods
# if using a git_ssh_key_secret_name use an fs_group
# that allows for the key to be read, e.g. 65533
fs_group =

[kubernetes_node_selectors]

# The Key-value pairs to be given to worker pods.
# The worker pods will be scheduled to the nodes of the specified key-value pairs.
# Should be supplied in the format: key = value

[kubernetes_annotations]

# The Key-value annotations pairs to be given to worker pods.
# Should be supplied in the format: key = value

[kubernetes_environment_variables]

# The scheduler sets the following environment variables into your workers. You may define as
# many environment variables as needed and the kubernetes launcher will set them in the launched workers.
# Environment variables in this section are defined as follows
# ``<environment_variable_key> = <environment_variable_value>``
#
# For example if you wanted to set an environment variable with value `prod` and key
# ``ENVIRONMENT`` you would follow the following format:
# ENVIRONMENT = prod
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_secrets]

# The scheduler mounts the following secrets into your workers as they are launched by the
# scheduler. You may define as many secrets as needed and the kubernetes launcher will parse the
# defined secrets and mount them as secret environment variables in the launched workers.
# Secrets in this section are defined as follows
# ``<environment_variable_mount> = <kubernetes_secret_object>=<kubernetes_secret_key>``
#
# For example if you wanted to mount a kubernetes secret key named ``postgres_password`` from the
# kubernetes secret object ``airflow-secret`` as the environment variable ``POSTGRES_PASSWORD`` into
# your workers you would follow the following format:
# ``POSTGRES_PASSWORD = airflow-secret=postgres_credentials``
#
# Additionally you may override worker airflow settings with the ``AIRFLOW__<SECTION>__<KEY>``
# formatting as supported by airflow normally.

[kubernetes_labels]

# The Key-value pairs to be given to worker pods.
# The worker pods will be given these static labels, as well as some additional dynamic labels
# to identify the task.
# Should be supplied in the format: ``key = value``
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\dags\projectnametemplatereplace_connections.py<content=>
from os import getenv
from airflow import DAG, settings
from airflow.models import Connection
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator

def ListConnections():
    return settings.Session().query(Connection)

def CreateConnections():
    try:
        # Build a connection object:
        conn = Connection(
            # conn_id: Name of the connection as displayed in the Airflow UI.
			# Snake-case; prefix with the product name.
            conn_id="projectnametemplatereplace_connDescription",
            # conn_type: The type of connection to create.
            # Valid conn_type values are: "azure_cosmos", "azure_data_lake", "cassandra", "cloudant", 
            # "docker", "gcpcloudsql", "google_cloud_platform", "grpc", "hive_cli", "hiveserver2", 
            # "jdbc", "jira", "mongo", "mssql", "mysql", "oracle", "pig_cli", "postgres", "presto",
            # "redis", "sqlite", "vertica", "wasb".
            conn_type="", 
            # host: Endpoint at which the resource exists; URL, IP address, etc.
            host="", 
            login="", 
            # Leave the password property value as-is; this will be updated via the Airflow UI.
            password="ChangeMeLater",
            # port: The port to use when creating a database type connection.
            # port=1234,
            # schema: The schema to use when creating a database type connection.
            # schema="schema"
            # extra: Used to specify additional connection type specific settings.
            # Refer to https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html for more details.
            # extra="json-formatted string"
        )

        # Get the current Airflow session:
        session = settings.Session()
        # Add the connection to the session if it doesn't already exist:
        if(conn.conn_id not in ListConnections()):
            session.add(conn)
            # Commit the newly-created connection:
            session.commit()
            return True
            
        return False
    except:
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_connections', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_connections",
        provide_context=False,
        python_callable=CreateConnections
    )

    po<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\dags\projectnametemplatereplace_variables.py<content=>
from os import getenv
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python_operator import PythonOperator
from airflow.models import Variable

def SetVariables():
    try:
        # Multiple variables may be set via multiple Variable.set() calls.
        # This variale DAG can be run once manually for static values or leverage
        # schedules to set variable values based on the results of other operations.
        Variable.set("projectnametemplatereplace_variableDescription", 'value/values')
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_variables', default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="create_variables",
        provide_context=False,
        python_callable=SetVariables
    )

    po<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_get_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable
from ast import literal_eval

def GetVariable():
    print("Performing Variable.get() action...")
    keys=literal_eval(Variable.get("projectnametemplatereplace_variable"))
    print(keys)

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
    #'schedule_interval': '0/3 * * * ?'
}

with DAG("projectnametemplatereplace_example_get_variable", default_args=default_args, schedule_interval=None) as dag:
    po = PythonOperator(task_id="get_variable",
        provide_context=False,
        python_callable=GetVariable
    )

    po<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_glue_test.py<content=>
from os import getenv

from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.amazon.aws.hooks.glue import AwsGlueJobHook
from airflow.operators.python_operator import PythonOperator

def ListGlueJobs():
    try:
        gh = AwsGlueJobHook(aws_conn_id="aws_lg_nonprod")
        print(gh.list_jobs())
        return True
    except:
        raise Exception

# Configure
# Execution actually occurrs after start_date + scheduled_interval has passed, so the
# below configuration would execute after 30 minutes, and wouldn't execute for all intervals
# from 1/1/2021 to today. All times are in UTC.
# 'start_date': datetime(2021,1,1), # When it should be turned on, not execution date.
# 'schedule_interval': timedelta(minutes=30), # Schedule format in time or cron tab
# 'catchup': False, # Don't backfill for passed intervals
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'catchup': False,
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_example_glue_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_glue_jobs",
        provide_context=False,
        python_callable=ListGlueJobs
    )

    po<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_s3_conn_test.py<content=>
"""
S3 Connection Test
"""
from airflow import DAG
from airflow.hooks.base_hook import BaseHook
from airflow.operators.python_operator import PythonOperator
import boto3
from datetime import *

def ListBuckets():
    try:
        s3Conn=boto3.client("s3")
        res = s3Conn.list_buckets()
        for bkt in res["Buckets"]:
            print(bkt["Name"])

        if(len(res["Buckets"]) > 0):
            return True
        return False
    except :
        raise Exception

default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG('projectnametemplatereplace_example_s3_conn_test', default_args=default_args) as dag:
    po = PythonOperator(task_id="list_s3_buckets",
        provide_context=False,
        python_callable=ListBuckets
    )

    po<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\examples\projectnametemplatereplace_example_set_variable.py<content=>
from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
import boto3
from airflow.models import Variable

def SetVariable():
    keys=["projectnametemplatereplace_0","1","2","3","4","5"]
    print("Performing Variable.get() action...")
    Variable.set("projectnametemplatereplace_variable", keys)
    
default_args = {
    'owner': 'KBX',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'schedule_interval': None, # timedelta(minutes=30),
    'email': ['something@here.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

with DAG("projectnametemplatereplace_example_set_variable", default_args=default_args) as dag:
    po = PythonOperator(task_id="set_variable",
        provide_context=False,
        python_callable=SetVariable
    )

    po<path=>.\SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\plugins<content=>
<path=>SERVICENOW\Development\Orchestration\KBX.DL.CodeTemplates.Workflow\script\entrypoint.sh<content=>
#!/usr/bin/env bash

# User-provided configuration must always be respected.
#
# Therefore, this script must only derives Airflow AIRFLOW__ variables from other variables
# when the user did not provide their own configuration.

TRY_LOOP="20"

# Global defaults and back-compat
: "${AIRFLOW_HOME:="/usr/local/airflow"}"
: "${AIRFLOW__CORE__FERNET_KEY:=${FERNET_KEY:=$(python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)")}}"
: "${AIRFLOW__CORE__EXECUTOR:=${EXECUTOR:-Sequential}Executor}"

# Load DAGs examples (default: Yes)
if [[ -z "$AIRFLOW__CORE__LOAD_EXAMPLES" && "${LOAD_EX:=n}" == n ]]; then
  AIRFLOW__CORE__LOAD_EXAMPLES=False
fi

export \
  AIRFLOW_HOME \
  AIRFLOW__CORE__EXECUTOR \
  AIRFLOW__CORE__FERNET_KEY \
  AIRFLOW__CORE__LOAD_EXAMPLES \

# Install custom python package if requirements.txt is present
if [ -e "/requirements.txt" ]; then
    $(command -v pip) install --user -r /requirements.txt
fi

wait_for_port() {
  local name="$1" host="$2" port="$3"
  local j=0
  while ! nc -z "$host" "$port" >/dev/null 2>&1 < /dev/null; do
    j=$((j+1))
    if [ $j -ge $TRY_LOOP ]; then
      echo >&2 "$(date) - $host:$port still not reachable, giving up"
      exit 1
    fi
    echo "$(date) - waiting for $name... $j/$TRY_LOOP"
    sleep 5
  done
}

# Other executors than SequentialExecutor drive the need for an SQL database, here PostgreSQL is used
if [ "$AIRFLOW__CORE__EXECUTOR" != "SequentialExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the database
  if [ -z "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" ]; then
    # Default values corresponding to the default compose files
    : "${POSTGRES_HOST:="postgres"}"
    : "${POSTGRES_PORT:="5432"}"
    : "${POSTGRES_USER:="airflow"}"
    : "${POSTGRES_PASSWORD:="airflow"}"
    : "${POSTGRES_DB:="airflow"}"
    : "${POSTGRES_EXTRAS:-""}"

    AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
    export AIRFLOW__CORE__SQL_ALCHEMY_CONN

    # Check if the user has provided explicit Airflow configuration for the broker's connection to the database
    if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
      AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}${POSTGRES_EXTRAS}"
      export AIRFLOW__CELERY__RESULT_BACKEND
    fi
  else
    if [[ "$AIRFLOW__CORE__EXECUTOR" == "CeleryExecutor" && -z "$AIRFLOW__CELERY__RESULT_BACKEND" ]]; then
      >&2 printf '%s\n' "FATAL: if you set AIRFLOW__CORE__SQL_ALCHEMY_CONN manually with CeleryExecutor you must also set AIRFLOW__CELERY__RESULT_BACKEND"
      exit 1
    fi

    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    POSTGRES_ENDPOINT=$(echo -n "$AIRFLOW__CORE__SQL_ALCHEMY_CONN" | cut -d '/' -f3 | sed -e 's,.*@,,')
    POSTGRES_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    POSTGRES_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Postgres" "$POSTGRES_HOST" "$POSTGRES_PORT"
fi

# CeleryExecutor drives the need for a Celery broker, here Redis is used
if [ "$AIRFLOW__CORE__EXECUTOR" = "CeleryExecutor" ]; then
  # Check if the user has provided explicit Airflow configuration concerning the broker
  if [ -z "$AIRFLOW__CELERY__BROKER_URL" ]; then
    # Default values corresponding to the default compose files
    : "${REDIS_PROTO:="redis://"}"
    : "${REDIS_HOST:="redis"}"
    : "${REDIS_PORT:="6379"}"
    : "${REDIS_PASSWORD:=""}"
    : "${REDIS_DBNUM:="1"}"

    # When Redis is secured by basic auth, it does not handle the username part of basic auth, only a token
    if [ -n "$REDIS_PASSWORD" ]; then
      REDIS_PREFIX=":${REDIS_PASSWORD}@"
    else
      REDIS_PREFIX=
    fi

    AIRFLOW__CELERY__BROKER_URL="${REDIS_PROTO}${REDIS_PREFIX}${REDIS_HOST}:${REDIS_PORT}/${REDIS_DBNUM}"
    export AIRFLOW__CELERY__BROKER_URL
  else
    # Derive useful variables from the AIRFLOW__ variables provided explicitly by the user
    REDIS_ENDPOINT=$(echo -n "$AIRFLOW__CELERY__BROKER_URL" | cut -d '/' -f3 | sed -e 's,.*@,,')
    REDIS_HOST=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f1)
    REDIS_PORT=$(echo -n "$POSTGRES_ENDPOINT" | cut -d ':' -f2)
  fi

  wait_for_port "Redis" "$REDIS_HOST" "$REDIS_PORT"
fi

case "$1" in
  webserver)
    airflow initdb
    if [ "$AIRFLOW__CORE__EXECUTOR" = "LocalExecutor" ] || [ "$AIRFLOW__CORE__EXECUTOR" = "SequentialExecutor" ]; then
      # With the "Local" and "Sequential" executors it should all run in one container.
      airflow scheduler &
    fi
    exec airflow webserver
    ;;
  worker|scheduler)
    # Give the webserver time to run initdb.
    sleep 10
    exec airflow "$@"
    ;;
  flower)
    sleep 10
    exec airflow "$@"
    ;;
  version)
    exec airflow "$@"
    ;;
  *)
    # The command is something like bash, not an airflow subcommand. Just run it in the right environment.
    exec "$@"
    ;;
esac
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\README.md<content=>
KBX.Analytics.DL.ServiceNow.Incident.Structured
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'incident' # Determined by CodeTemplate ProjectName parameter.
  domain: 'servicenow'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Incident.Structured/KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Incident.Structured/KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Incident.Structured/KBX.Analytics.DL.ServiceNow.Incident.Structured.Jobs
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Incident.Structured/KBX.Analytics.DL.ServiceNow.Incident.Structured.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain
      }

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Jobs\transform.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Incident.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
#date_partition_override = "ingest_date=2022-09-07"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-servicenow-incident-raw-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-servicenow-incident-structured-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'incident' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'incident'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'
JOB_NAME = args.JOB_NAME

# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATE_DATE_PARTITION_TO_PROCESS_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition : {date_partition}')
job.logger().info(f, f'is_replay : {is_replay}')

#JSON_FROM_SCHEMA = '{"fields":[{"metadata":{},"name":"result","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"active","nullable":true,"type":"string"},{"metadata":{},"name":"activity_due","nullable":true,"type":"string"},{"metadata":{},"name":"additional_assignee_list","nullable":true,"type":"string"},{"metadata":{},"name":"agile_story","nullable":true,"type":"string"},{"metadata":{},"name":"approval","nullable":true,"type":"string"},{"metadata":{},"name":"approval_history","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"business_duration","nullable":true,"type":"string"},{"metadata":{},"name":"business_service","nullable":true,"type":"string"},{"metadata":{},"name":"calendar_duration","nullable":true,"type":"string"},{"metadata":{},"name":"close_notes","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_by","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci_business_app","nullable":true,"type":"string"},{"metadata":{},"name":"comments","nullable":true,"type":"string"},{"metadata":{},"name":"company","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"contract","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_display","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_id","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"due_date","nullable":true,"type":"string"},{"metadata":{},"name":"escalation","nullable":true,"type":"string"},{"metadata":{},"name":"expected_start","nullable":true,"type":"string"},{"metadata":{},"name":"follow_up","nullable":true,"type":"string"},{"metadata":{},"name":"group_list","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"string"},{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"order","nullable":true,"type":"string"},{"metadata":{},"name":"parent","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"string"},{"metadata":{},"name":"route_reason","nullable":true,"type":"string"},{"metadata":{},"name":"short_description","nullable":true,"type":"string"},{"metadata":{},"name":"skills","nullable":true,"type":"string"},{"metadata":{},"name":"sla_due","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_document","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_esignature_configuration","nullable":true,"type":"string"},{"metadata":{},"name":"state","nullable":true,"type":"string"},{"metadata":{},"name":"sys_class_name","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_on","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain_path","nullable":true,"type":"string"},{"metadata":{},"name":"sys_id","nullable":true,"type":"string"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"string"},{"metadata":{},"name":"sys_tags","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_on","nullable":true,"type":"string"},{"metadata":{},"name":"task_effective_number","nullable":true,"type":"string"},{"metadata":{},"name":"time_worked","nullable":true,"type":"string"},{"metadata":{},"name":"u_all_classes_configuration_items","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_escalate","nullable":true,"type":"string"},{"metadata":{},"name":"u_estimated_delivery_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_catalog_item","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_customer","nullable":true,"type":"string"},{"metadata":{},"name":"u_manual_routing","nullable":true,"type":"string"},{"metadata":{},"name":"u_new_hire","nullable":true,"type":"string"},{"metadata":{},"name":"u_start_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_support_tier","nullable":true,"type":"string"},{"metadata":{},"name":"universal_request","nullable":true,"type":"string"},{"metadata":{},"name":"upon_approval","nullable":true,"type":"string"},{"metadata":{},"name":"upon_reject","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"user_input","nullable":true,"type":"string"},{"metadata":{},"name":"watch_list","nullable":true,"type":"string"},{"metadata":{},"name":"work_end","nullable":true,"type":"string"},{"metadata":{},"name":"work_notes","nullable":true,"type":"string"},{"metadata":{},"name":"work_start","nullable":true,"type":"string"}],"type":"struct"},"type":"array"}}],"type":"struct"}'
#schemaFromJson = StructType.fromJson(json.loads(JSON_FROM_SCHEMA))

# functions

#Flatten array of structs and structs
job.logger().info(f, f'###################_TASK-2_UDF_###################')

def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df
#%% last curated ingest date
job.logger().info(f, f'###################_TASK-3_LAST_CURATED_INGEST_DATE_###################')

try :
    last_curated = spark.sql(f'''
    select 
        distinct ingest_date 
    from 
        {curated_db}.{tbl}
    where
        ingest_date < '{date_partition.split('=')[1]}'
    order by ingest_date desc
    ''')
    last_curated.show()
    last_curated = last_curated.collect()[0][0]
    last_curated_ingest_date = f"ingest_date='{last_curated}'"
    job.logger().info(f, f'last_curated_ingest_date_to_process {last_curated_ingest_date}')
    curated = True
    job.logger().info(f, f'curated : {curated}')

except Exception as e:
    job.logger().info(f, e)
    curated = False
    job.logger().info(f, f'curated : {curated}')

# raise Exception('Forced Exception')

#%%------------------------------------------Job Start------------------------------------------
try :
    # All files for a date partition that haven't been processed are 
    # returned, so be cognizent of the size of this dataframe.

    job.logger().info(f, f'###################_TASK-4_JOB_START_READ_DATAFRAME_###################')
    # raw file df
    # df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=JSON_FROM_SCHEMA)
    df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, file_format='json')
except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")


#%%------------------------------------------Job Process------------------------------------------

try:    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-5_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])
        
        job.logger().info(f, f'raw_dataframe')
        df.printSchema()
        df.show()

        # flattening df
        df_flatten = flatten(df)

        job.logger().info(f, f'df_flatten')
        df_flatten.printSchema()
        df_flatten.show()
        job.logger().info(f, f'df_flatten.count() : {df_flatten.count()}')

        df_flatten.createOrReplaceTempView('raw_data')
        df_structured = spark.sql(f'''
            select
                '{JOB_ID}'                                                                                 			       job_id,
                '{SOURCE}'                                                                                  			   source,                                                          
                cast(TRIM(parent) as string)                                                                               parent,
                cast(TRIM(u_vendor_name) as string)                                                                 u_vendor_name,
                cast(TRIM(caused_by) as string)                                                                         caused_by,
                cast(TRIM(watch_list) as string)                                                                       watch_list,
                cast(TRIM(upon_reject) as string)                                                                     upon_reject,
                case 
                    when TRIM(sys_updated_on) <> ''
                    then to_timestamp(TRIM(sys_updated_on), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                sys_updated_on,
                cast(TRIM(origin_table) as string)                                                                   origin_table,
                cast(TRIM(u_escalate) as string)                                                                       u_escalate,
                cast(TRIM(x_lomei_logmein_re_logmein_session_counter) as string)                  x_lomei_logmein_re_logmein_session_counter,
                cast(TRIM(approval_history) as string)                                                           approval_history,
                cast(TRIM(skills) as string)                                                                               skills,
                cast(TRIM(number) as string)                                                                               number,
                cast(TRIM(u_type_of_fix) as string)                                                                 u_type_of_fix,
                cast(TRIM(state) as string)                                                                                 state,
                cast(TRIM(sys_created_by) as string)                                                               sys_created_by,
                cast(TRIM(knowledge) as string)                                                                         knowledge,
                cast(TRIM(order) as string)                                                                                 order,
                cast(TRIM(cmdb_ci) as string)                                                                             cmdb_ci,
                cast(TRIM(cmdb_ci_business_app) as string)                                                   cmdb_ci_business_app,
                cast(TRIM(contract) as string)                                                                           contract,
                cast(TRIM(impact) as string)                                                                               impact,
                cast(TRIM(active) as string)                                                                               active,
                cast(TRIM(u_is_24x7_validated) as string)                                                     u_is_24x7_validated,
                cast(TRIM(u_hr_case) as string)                                                                         u_hr_case,
                cast(TRIM(priority) as string)                                                                           priority,
                cast(TRIM(sys_domain_path) as string)                                                             sys_domain_path,
                cast(TRIM(x_lomei_logmein_re_logmein_session_id) as string)                 x_lomei_logmein_re_logmein_session_id,
                cast(TRIM(u_office_coordinates) as string)                                                   u_office_coordinates,
                cast(TRIM(u_support_tier) as string)                                                               u_support_tier,
                cast(TRIM(business_duration) as string)                                                         business_duration,
                cast(TRIM(group_list) as string)                                                                       group_list,
                case 
                    when TRIM(u_start_date) <> ''
                    then to_timestamp(TRIM(u_start_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                  u_start_date,
                case 
                    when TRIM(x_lomei_logmein_re_logmein_closing_time) <> ''
                    then to_timestamp(TRIM(x_lomei_logmein_re_logmein_closing_time), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                       x_lomei_logmein_re_logmein_closing_time,
                cast(TRIM(universal_request) as string)                                                         universal_request,
                cast(TRIM(short_description) as string)                                                         short_description,
                cast(TRIM(correlation_display) as string)                                                     correlation_display,
                case 
                    when TRIM(work_start) <> ''
                    then to_timestamp(TRIM(work_start), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                    work_start,
                cast(TRIM(additional_assignee_list) as string)                                           additional_assignee_list,
                cast(TRIM(u_survey_initiated_by_user) as string)                                       u_survey_initiated_by_user,
                cast(TRIM(notify) as string)                                                                               notify,
                cast(TRIM(sys_class_name) as string)                                                               sys_class_name,
                cast(TRIM(closed_by) as string)                                                                         closed_by,
                case 
                    when TRIM(follow_up) <> ''
                    then to_timestamp(TRIM(follow_up), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                     follow_up,
                cast(TRIM(parent_incident) as string)                                                             parent_incident,
                cast(TRIM(reopened_by) as string)                                                                     reopened_by,
                cast(TRIM(reassignment_count) as integer)      		                                           reassignment_count,
                cast(TRIM(assigned_to) as string)                                                                     assigned_to,
                cast(TRIM(u_custom_reference_2) as string)                                                   u_custom_reference_2,
                cast(TRIM(u_custom_reference_1) as string)                                                   u_custom_reference_1,
                /*case 
                    when TRIM(sla_due) <> ''
                    then to_timestamp(TRIM(sla_due), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                       sla_due,*/
                cast(TRIM(sla_due) as string)                                                                             sla_due,
                cast(TRIM(u_koch_customer) as string)                                                             u_koch_customer,
                cast(TRIM(agile_story) as string)                                                                     agile_story,
                cast(TRIM(escalation) as string)                                                                       escalation,
                cast(TRIM(upon_approval) as string)                                                                 upon_approval,
                cast(TRIM(correlation_id) as string)                                                               correlation_id,
                cast(TRIM(u_customer) as string)                                                                       u_customer,
                cast(TRIM(u_koch_catalog_item) as string)                                                     u_koch_catalog_item,
                cast(TRIM(u_vendor_ticket_number) as string)                                               u_vendor_ticket_number,
                cast(TRIM(made_sla) as string)                                                                           made_sla,
                cast(TRIM(u_resolution_ci_as_originating_ci) as string)                         u_resolution_ci_as_originating_ci,
                case 
                    when TRIM(u_custom_date_2) <> ''
                    then to_timestamp(TRIM(u_custom_date_2), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                               u_custom_date_2,
                case 
                    when TRIM(u_custom_date_1) <> ''
                    then to_timestamp(TRIM(u_custom_date_1), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                               u_custom_date_1,
                cast(TRIM(u_manual_routing) as string)                                                           u_manual_routing,
                cast(TRIM(sn_esign_document) as string)                                                         sn_esign_document,
                cast(TRIM(child_incidents) as integer)                           		                          child_incidents,
                cast(TRIM(u_catalog_item) as string)                                                               u_catalog_item,
                cast(TRIM(task_effective_number) as string)                                                 task_effective_number,
                cast(TRIM(u_estimated_delivery_date) as string)                                         u_estimated_delivery_date,
                cast(TRIM(u_building) as string)                                                                       u_building,
                cast(TRIM(resolved_by) as string)                                                                     resolved_by,
                cast(TRIM(sys_updated_by) as string)                                                               sys_updated_by,
                cast(TRIM(opened_by) as string)                                                                         opened_by,
                cast(TRIM(user_input) as string)                                                                       user_input,
                case 
                    when TRIM(sys_created_on) <> ''
                    then to_timestamp(TRIM(sys_created_on), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                sys_created_on,
                cast(TRIM(sys_domain) as string)                                                                       sys_domain,
                cast(TRIM(route_reason) as string)                                                                   route_reason,
                cast(TRIM(calendar_stc) as string)                                                                   calendar_stc,
                case 
                    when TRIM(closed_at) <> ''
                    then to_timestamp(TRIM(closed_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                     closed_at,
                cast(TRIM(business_service) as string)                                                           business_service,
                cast(TRIM(business_impact) as string)                                                             business_impact,
                cast(TRIM(rfc) as string)                                                                                     rfc,
                cast(TRIM(time_worked) as string)                                                                     time_worked,
                cast(TRIM(expected_start) as string)                                                               expected_start,
                case 
                    when TRIM(opened_at) <> ''
                    then to_timestamp(TRIM(opened_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                     opened_at,
                cast(TRIM(u_new_hire) as string)                                                                       u_new_hire,
                case 
                    when TRIM(work_end) <> ''
                    then to_timestamp(TRIM(work_end), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                      work_end,
                cast(TRIM(caller_id) as string)                                                                         caller_id,
                case 
                    when TRIM(reopened_time) <> ''
                    then to_timestamp(TRIM(reopened_time), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                  reopened_time,
                case 
                    when TRIM(resolved_at) <> ''
                    then to_timestamp(TRIM(resolved_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                    resolved_at,
                case 
                    when TRIM(x_lomei_logmein_re_logmein_pickup_time) <> ''
                    then to_timestamp(TRIM(x_lomei_logmein_re_logmein_pickup_time), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                         x_lomei_logmein_re_logmein_pickup_time,
                cast(TRIM(subcategory) as string)                                                                      subcategory,
                cast(TRIM(u_resolution_ci) as string)                                                              u_resolution_ci,
                cast(TRIM(close_code) as string)                                                                        close_code,
                cast(TRIM(assignment_group) as string)                                                            assignment_group,
                cast(TRIM(x_lomei_logmein_re_logmein_work_time) as string)                    x_lomei_logmein_re_logmein_work_time,
                cast(TRIM(u_defect) as string)                                                                            u_defect,
                cast(TRIM(business_stc) as string)                                                                    business_stc,
                cast(TRIM(cause) as string)                                                                                  cause,
                cast(TRIM(description) as string)                                                                      description,
                cast(TRIM(u_custom_text_1) as string)                                                              u_custom_text_1,
                cast(TRIM(origin_id) as string)                                                                          origin_id,
                cast(TRIM(calendar_duration) as string)                                                          calendar_duration,
                cast(TRIM(close_notes) as string)                                                                      close_notes,
                cast(TRIM(u_custom_text_2) as string)                                                              u_custom_text_2,
                cast(TRIM(sys_id) as string)                                                                                sys_id,
                cast(TRIM(contact_type) as string)                                                                    contact_type,
                cast(TRIM(sn_esign_esignature_configuration) as string)                          sn_esign_esignature_configuration,
                cast(TRIM(incident_state) as string)                                                                incident_state,
                cast(TRIM(urgency) as string)                                                                              urgency,
                cast(TRIM(problem_id) as string)                                                                        problem_id,
                cast(TRIM(company) as string)                                                                              company,
                case 
                    when TRIM(activity_due) <> ''
                    then to_timestamp(TRIM(activity_due), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                   activity_due,
                cast(TRIM(comments) as string)                                                                            comments,
                cast(TRIM(approval) as string)                                                                            approval,
                cast(TRIM(u_floor) as string)                                                                              u_floor,
                case 
                    when TRIM(due_date) <> ''
                    then to_timestamp(TRIM(due_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                                       due_date,
                cast(TRIM(sys_mod_count) as integer)               					                                 sys_mod_count,
                cast(TRIM(reopen_count) as integer)                                 		                          reopen_count,
                cast(TRIM(sys_tags) as string)                                                                            sys_tags,
                cast(TRIM(u_all_classes_configuration_items) as string)                          u_all_classes_configuration_items,
                cast(TRIM(location) as string)                                                                            location,
                cast(TRIM(category) as string)                                                                            category
            from raw_data
        ''')
        df_structured = df_structured.distinct()
        df_structured.createOrReplaceTempView('structured')
        
        if not curated :
            df_transformed = spark.sql('''
                select
                    *
                from
                    structured
                where
                    assignment_group = 'KBXL Systems Coordinators'
                        or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
            ''')
        else :
            df_transformed = spark.sql(f'''
                select
                    *
                from
                    structured
                where
                    assignment_group = 'KBXL Systems Coordinators'
                        or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
                        or
                    number in (select 
                                    number
                                from
                                    {curated_db}.{tbl}
                                where
                                    {last_curated_ingest_date}
                                    )
            ''')

        job.logger().info(f, f'df_transformed')
        df_transformed.printSchema()
        df_transformed.show()
        job.logger().info(f, f'df_transformed.count() : {df_transformed.count()}')

        job.logger().info(f, f'###################_TASK-6_COMMIT_FILE_###################')
        # Commit files
        if not df_transformed.rdd.isEmpty() :
            job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))
        
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-7_REFRESH_PARTITION/RUN_CRAWLER_###################')
        df_table = spark.sql(f'''show tables in {structured_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
                job.logger().info(f, f"partion {date_partition.split('=')[1]} added to {tbl}")
            except Exception as e:
                job.logger().info(f, e)
        else :
            job.logger().info(f, f'initiating {crawler} run for first time')
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler {crawler} is {state.lower()}.")
                    state_previous = state
        
        
        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        job.logger().info(f, f'###################_TASK-8_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%%------------------------------------------Job End------------------------------------------

job.runtime().end()

# %%
<path=>.\SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured\KBX.Analytics.DL.ServiceNow.Incident.Structured.Jobs\lib<content=>
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\.gitignore<content=>
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/
**/Properties/launchSettings.json

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk 
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush
.cr/

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output 
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder 
.mfractor/
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\README.md<content=>
KBX.Analytics.DL.ServiceNow.Task.Structured
============

## Introduction 

This solutions is reponsible for transforming the data and cataloging it.  It has python scripts that are scheduled and ran on spark with Glue to transform the data, then subsequent crawlers to catalog that transformed data. 

## Installation

This solution has been created by a code template.

You should only have to edit below this line in transform.py **##### YOUR CODE START #####**

## Authentication

KOCHID CLI: https://docs.kochid.com/docs/cli/

After KOCHID CLI is installed, add the custom aws.yaml file to the C:/Users/[userid]/.kochid path.

## KbxtDlPy

This is a KBXT python library used for batch processing and logging.  Install it by opening a console as administrator and navigating to your solutions /lib directory, then run:

``` Powershell
%> python -m pip install KbxtDlPy-0.3.2-py3-none-any.whl
```

## transform.py

- Starts a new Job from KbxtDlPy.Harness.
- Gets all files from **bucket_source** in the current days partition or the date partition specified by **date_partition_override**
  and applies a supplied **json_schema** to the resulting dataframe, inferring the schema if none is supplied.
- Writes the dataframe to the same date partition processed into the the **bucket_target**.
- Commits the Job.

## Run Solution

There are a couple different options to run your solution:

1. Visual Studio Code Jupyter extension and executing cell code blocks
2. Run menu command in Visual Studio Code
3. python.exe transform.py
    - Limited debug ability

## Deployment Setup

- https://dev.azure.com/kbxltrans/Infrastructure/_wiki/wikis/Infrastructure.wiki/608

## Common Errors

#### **Error**
```Powershell
Exception: Cannot begin transaction; the cursor is locked.  Either the previous job is still running is in an error state.
```
#### **Fix**
Delete the _cursor folder in your source s3 bucket.
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure\azure-pipelines.yml<content=>
parameters:
- name: environment
  displayName: Environment
  default: dev
  values:
  - dev
  - qa
  - preprod
  - prod
- name: stagesource
  displayName: Source Stage
  default: Source transformation name, such as structured
- name: stagetarget
  displayName: Target Stage
  default: Target transformation name, such as curated

trigger: none

pool:
  vmImage: 'ubuntu-latest'

variables:
  product: 'analytics' # This can be hard-coded since the solution is named per product
  entity: 'task' # Determined by CodeTemplate ProjectName parameter.
  domain: 'servicenow'
  
  stagesource: ${{ replace(lower(parameters.stagesource),' ','') }}  
  stagetarget: ${{ replace(lower(parameters.stagetarget),' ','') }}  

  prefix: 'kbxt-dl' # DO NOT CHANGE
  awsCredentials: '$(prefix)-$(product)-service-agent-$(environment)'
  environment: ${{ parameters.environment }}  
  bucketName: '$(prefix)-$(product)-automation-$(environment)'
  infrastructureLocalPath: '$(Build.Repository.LocalPath)/$(Build.Repository.Name).Infrastructure'  
  sourceFolder: '$(Build.Repository.LocalPath)/'
  bucketTargetFolder: '$(Build.Repository.Name)/'
  ismaster: $[eq(variables['Build.SourceBranchName'], 'master')] 
  stackName: '$(prefix)-$(product)-$(entity)-$(stagetarget)-$(environment)-cf-stack' 
  stackArguments: "--no-fail-on-empty-changeset --stack-name $(stackName) --template-file $(infrastructureLocalPath)/Transform.yaml --parameter-overrides Environment=$(environment) Product=$(product) Entity=$(entity) StageSource=$(stagesource) StageTarget=$(stagetarget) Domain=$(domain)"
  stackPolicyUrl: "https://$(bucketName).s3.amazonaws.com/$(Build.Repository.Name)/$(Build.Repository.Name).Infrastructure/Templates/stackpolicy.json"

stages:
# Build
- stage: Build
  jobs:  
  - job: Clean
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 's3'
        awsSubCommand: 'rm'
        awsArguments: "s3://$(bucketName)/$(bucketTargetFolder) --recursive"
        failOnStandardError: true
  - job: Upload
    dependsOn: [Clean]
    steps:
    - task: S3Upload@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        bucketName: '$(bucketName)'
        sourceFolder: '$(sourceFolder)'
        globExpressions: '**'
        targetFolder: '$(bucketTargetFolder)'
        keyManagement: 'awsManaged'
        encryptionAlgorithm: 'AES256'
        contentType: 'yaml'
        cacheControl: 'max-age=0'

# Test

# Deploy
# Only allow master, for now, to be deployed to development as a fale safe until other env are up
- stage: Deploy
  jobs:  
  - job: Deploy
    dependsOn: []
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'deploy'
        awsArguments: "$(stackArguments)"
        failOnStandardError: true
  - job: Secure
    dependsOn: [Deploy]
    steps:
    - task: AWSCLI@1
      inputs:
        awsCredentials: '$(awsCredentials)'
        regionName: 'us-east-1'
        awsCommand: 'cloudformation'
        awsSubCommand: 'set-stack-policy'
        awsArguments: "--stack-name $(stackName) --stack-policy-url $(stackPolicyUrl)"
        failOnStandardError: true

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure\Transform.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: DL Transform deployments.

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          Default: Basic Configuration
        Parameters:
          - Product
          - Domain
          - Entity
          - StageSource
          - StageTarget
          - Environment
          - Prefix
          - PrefixDatabase
          - BLC
          - CostCenter
    ParameterLabels:
      Product:
        Description: What product is this associated with
      Domain:
        Description: The solution's Domain
      Entity:
        Description: Entity name, such as servicenow
      StageSource:
        Description: StageSource name, such as structured
      StageTarget:
        Description: StageTarget name, such as curated
      Environment:
        Description: Logical environment name to distinguish globally unique resources
      Prefix:
        Description: The prefix the resources will have
      PrefixDatabase:
        Description: The prefix the database will have
      BLC:
        Description: BLC tagged on this resource
      CostCenter:
        Description: CostCenter tagged on this resource

Parameters:
  InfrastructurePath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Task.Structured/KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Task.Structured/KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure  
  JobsPath:
    Type: String
    Default: KBX.Analytics.DL.ServiceNow.Task.Structured/KBX.Analytics.DL.ServiceNow.Task.Structured.Jobs
    AllowedValues:
      - KBX.Analytics.DL.ServiceNow.Task.Structured/KBX.Analytics.DL.ServiceNow.Task.Structured.Jobs
  Product:
    Type: String
  Domain:
    Type: String
  Entity:
    Type: String
  StageSource:
    Type: String
  StageTarget:
    Type: String
  Environment:
    Type: String
  Prefix:
    Type: String
    Default: kbxt-dl
    AllowedValues:
      - kbxt-dl
  PrefixAutomation:
    Type: String
    Default: kbxt-dl-data
    AllowedValues:
      - kbxt-dl-data
  PrefixDatabase:
    Type: String
    Default: kbxt_dl
    AllowedValues:
      - kbxt_dl
  BLC:
    Type: String
    Default: 8018
    AllowedValues:
      - 8018
  CostCenter:
    Type: String
    Default: 56907
    AllowedValues:
      - 56907
  AgentRolePrefix:
    Type: String
    Default: kbxt-dl-analytics-service-role
    AllowedValues:
      - kbxt-dl-analytics-service-role

Resources:
  TransformedStorageStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Storage.yaml'
      Parameters:        
        BucketName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        Environment: !Ref Environment
        Product: !Ref Product
        BLC: !Ref BLC
        CostCenter: !Ref CostCenter
    
  TransformJobStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Job.yaml'
      Parameters:
        JobName: !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, "job", !Ref Environment]]
        JobFile: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/${JobsPath}/transform.py'
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        TempDir: !Sub 's3://${Prefix}-${Product}-automation-${Environment}/temp'
        AdditionalPythonModules: !Sub 's3://${PrefixAutomation}-automation-${Environment}/KBX.DL.Platform/KBX.DL.Platform/Libraries/dist/KbxtDlPy-1.1.0-py3-none-any.whl'
        BucketSource: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageSource}-${Environment}'
        BucketTarget: !Sub '${Prefix}-${Product}-${Domain}-${Entity}-${StageTarget}-${Environment}'
        Entity: !Ref Entity
        Product: !Ref Product
        Domain: !Ref Domain
        Environment: !Ref Environment
        Prefix: !Ref Prefix

  TransformCrawlersStack:
    Type: 'AWS::CloudFormation::Stack'
    Properties:
      TemplateURL: !Sub 'https://${Prefix}-${Product}-automation-${Environment}.s3.amazonaws.com/${InfrastructurePath}/Templates/Crawler.yaml'
      Parameters:      
        AgentRole: !Join ['-', [!Ref AgentRolePrefix, !Ref Environment]]
        DatabaseName: !Join ['_', [!Ref PrefixDatabase, !Ref Product, "db", !Ref StageTarget, !Ref Environment]]
        Schedule: "cron(15 0 * * ? *)"
        Path: !Join ['/', ["s3:/", !Join ['-', [!Ref Prefix, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]], ""]]
        Name: !Join ['_', [!Ref PrefixDatabase, !Ref Product, !Ref Domain, !Ref Entity, !Ref StageTarget, !Ref Environment]]
        TablePrefix: !Join ['_', [!Ref Domain, ""]]
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure\Templates\Crawler.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Crawler template.

Parameters:
  AgentRole:
    Type: String
  DatabaseName:
    Type: String 
  Schedule:
    Type: String
  Path:
    Type: String
  Name:
    Type: String
  TablePrefix:
    Type: String

Resources:
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ['-', [!Ref Name, "crawler"]]
      Role: !Ref AgentRole
      DatabaseName: !Ref DatabaseName
      TablePrefix: !Ref TablePrefix
      Targets:
        S3Targets:
          - Path: !Ref Path
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableLevelConfiguration\":2},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure\Templates\Job.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Job template.

Parameters:
  JobName:
    Type: String
  JobFile:
    Type: String
  TempDir:
    Type: String
  AgentRole:
    Type: String
  AdditionalPythonModules:
    Type: String
  BucketSource:
    Type: String
  BucketTarget:
    Type: String
  Entity:
    Type: String
  Product:
    Type: String
  Domain:
    Type: String
  Prefix:
    Type: String
  Environment:
    Type: String

Resources:
  Job:
    Type: "AWS::Glue::Job"
    Properties:
      GlueVersion: "2.0"
      Timeout: 15
      WorkerType: "G.1X"
      NumberOfWorkers: 2
      MaxRetries: 0
      Role: !Ref AgentRole
      Name: !Ref JobName
      Command: {
        "Name" : "glueetl",
        "ScriptLocation": !Ref JobFile
      }
      DefaultArguments: {
        "--scriptLocation": !Ref JobFile,
        "--TempDir": !Ref TempDir,
        "--job-bookmark-option": "job-bookmark-disable",
        "--extra-py-files": !Ref AdditionalPythonModules,
        "--additional-python-modules": !Join [',', [ !Ref AdditionalPythonModules, pyarrow, awswrangler]],
        "--enable-rename-algorithm-v2": "true",
        "--enable-glue-datacatalog": "true",
        "--enable-metrics": "true",
        "--enable-continuous-cloudwatch-log": "false",
        "--enable-continuous-log-filter": "true",
        "--date_partition_override" : "",
        "--prefix_source" : !Ref Entity,
        "--bucket_source" : !Ref BucketSource,
        "--bucket_target" : !Ref BucketTarget,
        "--Environment" : !Ref Environment,
        "--Prefix" : !Ref Prefix,
        "--Product" : !Ref Product,
        "--Entity" : !Ref Entity,
        "--Domain" : !Ref Domain
      }

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure\Templates\stackpolicy.json<content=>
{
    "Statement" : [        
      {
        "Effect" : "Allow",
        "Action" : "Update:*",
        "Principal": "*",
        "Resource" : "*"
      },
      {
        "Effect" : "Deny",
        "Action" : ["Update:Replace","Update:Delete"],
        "Principal": "*",
        "Condition" : {
          "StringEquals" : {
            "ResourceType" : ["AWS::S3::Bucket"]
          }
        }
      }
    ]
  }
  
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure\Templates\Storage.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Storage template.

Parameters:
  BucketName:
    Type: String
  Environment:
    Type: String
  Product:
    Type: String
  BLC:
    Type: String
  CostCenter:
    Type: String

Resources:
  Bucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties: 
      AccessControl: Private
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        IgnorePublicAcls: true
        BlockPublicPolicy: true
        RestrictPublicBuckets: true            
      VersioningConfiguration: 
        Status: Enabled      
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      LifecycleConfiguration:
        Rules:
          - Id: PruneAbandonedMultipartUploadsRule
            Status: "Enabled"
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
      Tags:
        - Key: blc
          Value: !Ref BLC
        - Key: costcenter
          Value: !Ref CostCenter
        - Key: type
          Value: storage
        - Key: description
          Value: !Ref BucketName
        - Key: env
          Value: !Ref Environment
        - Key: status
          Value: active

<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Infrastructure\Templates\TriggerScheduled.yaml<content=>
AWSTemplateFormatVersion: 2010-09-09
Description: Data Catalog Trigger template.

Parameters:
  JobName:
    Type: String
  Schedule:
    Type: String

Resources:
  Trigger:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ['-', [!Ref JobName, "trigger", "scheduled"]]
      Type: SCHEDULED
      Schedule: !Ref Schedule
      StartOnCreation: true
      Actions:
        - JobName: !Ref JobName
<path=>SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Jobs\transform.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException
import time
import logging

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Task.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
parser.add_argument('--JOB_NAME')

args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
#date_partition_override = "ingest_date=2022-09-07"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-servicenow-task-raw-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-servicenow-task-structured-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'task' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'task'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'
JOB_NAME = args.JOB_NAME


# KbxtDlPy
from KbxtDlPy.Harness import Job
job = Job(name=JOB_NAME, level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# update logger
logger = logging.getLogger(name = JOB_NAME)
log_format = "%(asctime)s %(levelname)-8s JOB_NAME:%(name)s %(message)s"

date_format = "%Y-%m-%d %H:%M:%S"
logger.setLevel(logging.INFO)
log_stream = sys.stdout

if logger.handlers:
    for handler in logger.handlers:
        logger.removeHandler(handler)
        
logging.basicConfig(level=logging.INFO, format=log_format, stream=log_stream, datefmt=date_format)

job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')

curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%% INGEST DATE
job.logger().info(f, f'###################_TASK-1_CALCULATE_DATE_PARTITION_TO_PROCESS_###################')
# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

job.logger().info(f, f'date_partition : {date_partition}')
job.logger().info(f, f'is_replay : {is_replay}')

#JSON_FROM_SCHEMA = '{"fields":[{"metadata":{},"name":"result","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"active","nullable":true,"type":"string"},{"metadata":{},"name":"activity_due","nullable":true,"type":"string"},{"metadata":{},"name":"additional_assignee_list","nullable":true,"type":"string"},{"metadata":{},"name":"agile_story","nullable":true,"type":"string"},{"metadata":{},"name":"approval","nullable":true,"type":"string"},{"metadata":{},"name":"approval_history","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"business_duration","nullable":true,"type":"string"},{"metadata":{},"name":"business_service","nullable":true,"type":"string"},{"metadata":{},"name":"calendar_duration","nullable":true,"type":"string"},{"metadata":{},"name":"close_notes","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_by","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci_business_app","nullable":true,"type":"string"},{"metadata":{},"name":"comments","nullable":true,"type":"string"},{"metadata":{},"name":"company","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"contract","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_display","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_id","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"due_date","nullable":true,"type":"string"},{"metadata":{},"name":"escalation","nullable":true,"type":"string"},{"metadata":{},"name":"expected_start","nullable":true,"type":"string"},{"metadata":{},"name":"follow_up","nullable":true,"type":"string"},{"metadata":{},"name":"group_list","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"string"},{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"order","nullable":true,"type":"string"},{"metadata":{},"name":"parent","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"string"},{"metadata":{},"name":"route_reason","nullable":true,"type":"string"},{"metadata":{},"name":"short_description","nullable":true,"type":"string"},{"metadata":{},"name":"skills","nullable":true,"type":"string"},{"metadata":{},"name":"sla_due","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_document","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_esignature_configuration","nullable":true,"type":"string"},{"metadata":{},"name":"state","nullable":true,"type":"string"},{"metadata":{},"name":"sys_class_name","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_on","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain_path","nullable":true,"type":"string"},{"metadata":{},"name":"sys_id","nullable":true,"type":"string"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"string"},{"metadata":{},"name":"sys_tags","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_on","nullable":true,"type":"string"},{"metadata":{},"name":"task_effective_number","nullable":true,"type":"string"},{"metadata":{},"name":"time_worked","nullable":true,"type":"string"},{"metadata":{},"name":"u_all_classes_configuration_items","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_escalate","nullable":true,"type":"string"},{"metadata":{},"name":"u_estimated_delivery_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_catalog_item","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_customer","nullable":true,"type":"string"},{"metadata":{},"name":"u_manual_routing","nullable":true,"type":"string"},{"metadata":{},"name":"u_new_hire","nullable":true,"type":"string"},{"metadata":{},"name":"u_start_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_support_tier","nullable":true,"type":"string"},{"metadata":{},"name":"universal_request","nullable":true,"type":"string"},{"metadata":{},"name":"upon_approval","nullable":true,"type":"string"},{"metadata":{},"name":"upon_reject","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"user_input","nullable":true,"type":"string"},{"metadata":{},"name":"watch_list","nullable":true,"type":"string"},{"metadata":{},"name":"work_end","nullable":true,"type":"string"},{"metadata":{},"name":"work_notes","nullable":true,"type":"string"},{"metadata":{},"name":"work_start","nullable":true,"type":"string"}],"type":"struct"},"type":"array"}}],"type":"struct"}'
#schemaFromJson = StructType.fromJson(json.loads(JSON_FROM_SCHEMA))

# functions

#Flatten array of structs and structs
job.logger().info(f, f'###################_TASK-2_UDF_###################')

def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df
#%% last curated ingest date
job.logger().info(f, f'###################_TASK-3_LAST_CURATED_INGEST_DATE_###################')

try :
    last_curated = spark.sql(f'''
    select 
        distinct ingest_date 
    from 
        {curated_db}.{tbl}
    where
        ingest_date < '{date_partition.split('=')[1]}'
    order by ingest_date desc
    ''')
    last_curated.show()
    last_curated = last_curated.collect()[0][0]
    last_curated_ingest_date = f"ingest_date='{last_curated}'"
    job.logger().info(f, f'last_curated_ingest_date_to_process {last_curated_ingest_date}')
    curated = True
    job.logger().info(f, f'curated : {curated}')

except Exception as e:
    job.logger().info(f, e)
    curated = False
    job.logger().info(f, f'curated : {curated}')

# raise Exception('Forced Exception')

#%%------------------------------------------Job Start------------------------------------------
try :
    # All files for a date partition that haven't been processed are 
    # returned, so be cognizent of the size of this dataframe.

    job.logger().info(f, f'###################_TASK-4_JOB_START_READ_DATAFRAME_###################')
    # raw file df
    # df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=JSON_FROM_SCHEMA)
    df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, file_format='json')
except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")

#%%------------------------------------------Job Process------------------------------------------

try:    
    if (df is not None):
        job.logger().info(f, f'###################_TASK-5_START_JOB_PROCESS_###################')
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])
        
        job.logger().info(f, f'raw_dataframe')
        df.printSchema()
        df.show()

        # flattening df
        df_flatten = flatten(df)

        job.logger().info(f, f'df_flatten')
        df_flatten.printSchema()
        df_flatten.show()
        job.logger().info(f, f'df_flatten.count() : {df_flatten.count()}')

        df_flatten.createOrReplaceTempView('raw_data')
        df_structured = spark.sql(f'''
            select
                '{JOB_ID}'                                                                                job_id,
                '{SOURCE}'                                                                                source,                                                          
                cast(TRIM(u_koch_catalog_item) as string)                                    u_koch_catalog_item,
                cast(TRIM(parent) as string)                                                              parent,
                cast(TRIM(made_sla) as string)                                                          made_sla,
                case 
                    when TRIM(u_custom_date_2) <> ''
                    then to_timestamp(TRIM(u_custom_date_2), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                              u_custom_date_2,
                cast(TRIM(watch_list) as string)                                                      watch_list,
                case 
                    when TRIM(u_custom_date_1) <> ''
                    then to_timestamp(TRIM(u_custom_date_1), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                              u_custom_date_1,
                cast(TRIM(u_manual_routing) as string)                                          u_manual_routing,
                cast(TRIM(sn_esign_document) as string)                                        sn_esign_document,
                cast(TRIM(upon_reject) as string)                                                    upon_reject,
                case 
                    when TRIM(sys_updated_on) <> ''
                    then to_timestamp(TRIM(sys_updated_on), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                               sys_updated_on,
                cast(TRIM(task_effective_number) as string)                                task_effective_number,
                cast(TRIM(u_escalate) as string)                                                      u_escalate,
                cast(TRIM(u_estimated_delivery_date) as string)                        u_estimated_delivery_date,
                cast(TRIM(approval_history) as string)                                          approval_history,
                cast(TRIM(skills) as string)                                                              skills,
                cast(TRIM(number) as string)                                                              number,
                cast(TRIM(sys_updated_by) as string)                                              sys_updated_by,
                cast(TRIM(opened_by) as string)                                                        opened_by,
                cast(TRIM(user_input) as string)                                                      user_input,
                case 
                    when TRIM(sys_created_on) <> ''
                    then to_timestamp(TRIM(sys_created_on), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                               sys_created_on,
                cast(TRIM(sys_domain) as string)                                                      sys_domain,
                cast(TRIM(state) as string)                                                                state,
                cast(TRIM(route_reason) as string)                                                  route_reason,
                cast(TRIM(sys_created_by) as string)                                              sys_created_by,
                cast(TRIM(knowledge) as string)                                                        knowledge,
                cast(TRIM(order) as string)                                                                order,
                case 
                    when TRIM(closed_at) <> ''
                    then to_timestamp(TRIM(closed_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    closed_at,
                cast(TRIM(cmdb_ci) as string)                                                            cmdb_ci,
                cast(TRIM(cmdb_ci_business_app) as string)                                  cmdb_ci_business_app,
                cast(TRIM(contract) as string)                                                          contract,
                cast(TRIM(impact) as string)                                                              impact,
                cast(TRIM(active) as string)                                                              active,
                cast(TRIM(business_service) as string)                                          business_service,
                cast(TRIM(priority) as string)                                                          priority,
                cast(TRIM(sys_domain_path) as string)                                            sys_domain_path,
                cast(TRIM(time_worked) as string)                                                    time_worked,
                cast(TRIM(expected_start) as string)                                              expected_start,
                case 
                    when TRIM(opened_at) <> ''
                    then to_timestamp(TRIM(opened_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    opened_at,
                cast(TRIM(u_support_tier) as string)                                              u_support_tier,
                cast(TRIM(business_duration) as string)                                        business_duration,
                cast(TRIM(group_list) as string)                                                      group_list,
                cast(TRIM(u_new_hire) as string)                                                      u_new_hire,
                case 
                    when TRIM(work_end) <> ''
                    then to_timestamp(TRIM(work_end), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                     work_end,
                /*cast(TRIM(work_notes) as string)                                                      work_notes,*/
                case 
                    when TRIM(u_start_date) <> ''
                    then to_timestamp(TRIM(u_start_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                 u_start_date,
                cast(TRIM(universal_request) as string)                                        universal_request,
                cast(TRIM(short_description) as string)                                        short_description,
                cast(TRIM(correlation_display) as string)                                    correlation_display,
                case 
                    when TRIM(work_start) <> ''
                    then to_timestamp(TRIM(work_start), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                   work_start,
                cast(TRIM(assignment_group) as string)                                          assignment_group,
                cast(TRIM(additional_assignee_list) as string)                          additional_assignee_list,
                cast(TRIM(description) as string)                                                    description,
                cast(TRIM(u_custom_text_1) as string)                                            u_custom_text_1,
                cast(TRIM(calendar_duration) as string)                                        calendar_duration,
                cast(TRIM(close_notes) as string)                                                    close_notes,
                cast(TRIM(sys_class_name) as string)                                              sys_class_name,
                cast(TRIM(u_custom_text_2) as string)                                            u_custom_text_2,
                cast(TRIM(closed_by) as string)                                                        closed_by,
                case 
                    when TRIM(follow_up) <> ''
                    then to_timestamp(TRIM(follow_up), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    follow_up,
                cast(TRIM(sys_id) as string)                                                              sys_id,
                cast(TRIM(contact_type) as string)                                                  contact_type,
                cast(TRIM(sn_esign_esignature_configuration) as string)        sn_esign_esignature_configuration,
                cast(TRIM(urgency) as string)                                                            urgency,
                cast(TRIM(company) as string)                                                            company,
                cast(TRIM(reassignment_count) as bigint)                                      reassignment_count,
                cast(TRIM(activity_due) as string)                                                  activity_due,
                cast(TRIM(assigned_to) as string)                                                    assigned_to,
                cast(TRIM(u_custom_reference_2) as string)                                  u_custom_reference_2,
                cast(TRIM(comments) as string)                                                          comments,
                cast(TRIM(u_custom_reference_1) as string)                                  u_custom_reference_1,
                cast(TRIM(approval) as string)                                                          approval,
                /*case 
                    when TRIM(sla_due) <> ''
                    then to_timestamp(TRIM(sla_due), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                      sla_due,*/
                cast(TRIM(sla_due) as string)                                                            sla_due,
                cast(TRIM(u_koch_customer) as string)                                            u_koch_customer,
                case 
                    when TRIM(due_date) <> ''
                    then to_timestamp(TRIM(due_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                     due_date,
                cast(TRIM(sys_mod_count) as bigint)                                                sys_mod_count,
                cast(TRIM(sys_tags) as string)                                                          sys_tags,
                cast(TRIM(agile_story) as string)                                                    agile_story,
                cast(TRIM(escalation) as string)                                                      escalation,
                cast(TRIM(upon_approval) as string)                                                upon_approval,
                cast(TRIM(u_all_classes_configuration_items) as string)        u_all_classes_configuration_items,
                cast(TRIM(correlation_id) as string)                                              correlation_id,
                cast(TRIM(location) as string)                                                          location
            from raw_data
        ''')
		
        df_structured = df_structured.distinct()
        df_structured.createOrReplaceTempView('structured')
        
        if not curated :
            df_transformed = spark.sql('''
                select
                    *
                from
                    structured
                where
                    assignment_group = 'KBXL Systems Coordinators'
                        or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
            ''')
        else :
            df_transformed = spark.sql(f'''
                select
                    *
                from
                    structured
                where
                    assignment_group = 'KBXL Systems Coordinators'
                        or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
                        or
                    number in (select 
                                    number
                                from
                                    {curated_db}.{tbl}
                                where
                                    {last_curated_ingest_date}
                                    )
            ''')

        job.logger().info(f, f'df_transformed')
        df_transformed.printSchema()
        df_transformed.show()
        job.logger().info(f, f'df_transformed.count() : {df_transformed.count()}')

        job.logger().info(f, f'###################_TASK-6_COMMIT_FILE_###################')
        # Commit files
        if not df_transformed.rdd.isEmpty() :
            job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))
        
        
        # Refresh Partition or if table not present run crawler to add table
        job.logger().info(f, f'###################_TASK-7_REFRESH_PARTITION/RUN_CRAWLER_###################')
        df_table = spark.sql(f'''show tables in {structured_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
                job.logger().info(f, f"partion {date_partition.split('=')[1]} added to {tbl}")
            except Exception as e:
                job.logger().info(f, e)
        else :
            job.logger().info(f, f'initiating {crawler} run for first time')
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                time.sleep(2)
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler {crawler} is {state.lower()}.")
                    state_previous = state
        
        
        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))
        job.logger().info(f, f'###################_TASK-8_JOB_RUN_SUCCESSFULL_###################')

except Exception as e:
    job.logger().info(f, f'###################_TASK-8_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception(f"3ccdb332-0d0e-4091-be38-6e0541fe11d7:{JOB_NAME}:{e}")
#%%------------------------------------------Job End------------------------------------------

job.runtime().end()

# %%
<path=>.\SERVICENOW\Development\Structured\KBX.Analytics.DL.ServiceNow.Task.Structured\KBX.Analytics.DL.ServiceNow.Task.Structured.Jobs\lib<content=>
<path=>SERVICENOW\Testing\code_structure.py<content=>
<path=>SERVICENOW\Testing\incidentHistory.py<content=>
#%% import
import pandas as pd
import requests
import datetime
from datetime import date
from datetime import datetime, timedelta
import json
import time

#from datetime import datetime
headers = {"Content-Type":"application/json","Accept":"application/json"}

servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%
TABLE = 'incident'
MAX_REC = 3000
base_api_url = f'https://kochprod.service-now.com/api/now/table/{TABLE}'
OFFSET = 0
ingest_start_time = datetime.strptime('2022-07-13', '%Y-%m-%d')
ingest_end_time = ingest_start_time - timedelta(days=1)
# ingest_start_time = f"""{ingest_start_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""
# ingest_end_time = f"""{ingest_end_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""

ingest_start_time = f'''{ingest_start_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''
ingest_end_time = f'''{ingest_end_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''


params = {
    'sysparm_query': "assignment_group=3a9eb4071b859090e6cea687bd4bcb68^ORassignment_group=5d7921630fd68a006ee822d8b1050ed5",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': MAX_REC,
    'sysparm_offset': OFFSET 
}


#%%

response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
data = response.json()
url = response.url
print(f"api headers first {response.headers}")
while ('next' in response.links and response.status_code == 200) :
    url=response.links['next']['url']
    response =  requests.get(url=url, auth=(servicenow_user, servicenow_pwd), headers=headers)
    data['result'].extend(response.json()['result'])
    print(f"api headers next {response.headers}")
    print (len(data['result']))

with open("incidentHistory.json", "w") as f:
    json.dump(data, f)

# %%
<path=>SERVICENOW\Testing\incidentHistory_multiprocessing.py<content=>
#%% import
import pandas as pd
import requests
import datetime
from datetime import date
from datetime import datetime, timedelta
import json
import time
import math

#from datetime import datetime
headers = {"Content-Type":"application/json","Accept":"application/json"}

servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%
TABLE = 'incident'
MAX_REC = 1000
base_api_url = f'https://kochprod.service-now.com/api/now/table/{TABLE}'
OFFSET = 0
ingest_start_time = datetime.strptime('2022-07-24', '%Y-%m-%d')
ingest_end_time = ingest_start_time - timedelta(days=1)
# ingest_start_time = f"""{ingest_start_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""
# ingest_end_time = f"""{ingest_end_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""

ingest_start_time = f'''{ingest_start_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''
ingest_end_time = f'''{ingest_end_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''


params = {
    'sysparm_query': "assignment_group=3a9eb4071b859090e6cea687bd4bcb68^ORassignment_group=5d7921630fd68a006ee822d8b1050ed5",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': MAX_REC,
    'sysparm_offset': OFFSET 
}

#%%
def calculate_offsets(params) :
    params = params.copy()
    params['sysparm_limit'] = 1
    response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
    print (response.headers['X-Total-Count'])
    number_of_offsets = math.ceil(int(response.headers['X-Total-Count'])/MAX_REC)
    return number_of_offsets

params_params = []

for n in range(calculate_offsets(params)) :
    parameters = params.copy()
    parameters['sysparm_offset'] = int(MAX_REC)*n
    params_params.append(parameters)

def send_request(params) :
    response = requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
    print (len(response.json()['result']))
    print (response.headers)
    return response


#%%
import concurrent.futures as cf

with cf.ProcessPoolExecutor() as executor :
    results = executor.map(send_request, params_params)

response = next(results)
data = response.json()
print (len(data['result']))

for response in results :
    data['result'].extend(response.json()['result'])
    print (len(data['result']))

with open("incidentHistory_mp.json", "w") as f:
    json.dump(data, f)

# %%
<path=>SERVICENOW\Testing\ingest_incident.py<content=>
#%%------------------------------------------ingest------------------------------------------

import os
import sys
import argparse
import requests
import json
import boto3
import botocore
from datetime import datetime
from datetime import timedelta

# Timer
start_time = datetime.utcnow()
ingest_start_time = f'''{(start_time - timedelta(days=1)).strftime("'%Y-%m-%d','00:00:00'")}'''
ingest_end_time = f'''{start_time.strftime("'%Y-%m-%d','00:00:00'")}'''

# file
f = os.path.basename(__file__)

# secret
servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%------------------------------------------Init------------------------------------------
from KbxtDlPy.Harness import Job
job = Job(name="ingest", level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")
job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')
job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
parser.add_argument('--sysparm_limit')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')

args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target
#bucket_target = "kbxt-dl-analytics-servicenow-incident-raw-dev" #remove

prefix_target = args.prefix_target
#prefix_target = "incident" #remove

sysparm_limit = args.sysparm_limit
#sysparm_limit = 5000 #remove

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'incident'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'
OFFSET = 0

table = prefix_target
base_api_url = f'https://kochprod.service-now.com/api/now/table/{table}'
headers = {"Content-Type":"application/json","Accept":"application/json"}
crawler = bucket_target.replace('-', '_')+'-crawler'

params = {
    'sysparm_query': f"sys_updated_on>=javascript:gs.dateGenerate({ingest_start_time})^sys_updated_on<javascript:gs.dateGenerate({ingest_end_time})",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': sysparm_limit,
    'sysparm_offset': OFFSET
}

job.logger().info(f,f'''bucket_target={bucket_target},#, 
prefix_target={prefix_target},#, 
base_api_url = {base_api_url},#, 
params = {params}
''')

job.logger().info(f, f'servicenow table to injest : {table}')
job.logger().info(f, f'base_api_url : {base_api_url}')
job.logger().info(f, f'ingest_start_time : {ingest_start_time}')
job.logger().info(f, f'ingest_end_time : {ingest_end_time}')
job.logger().info(f, f'sysparm_limit : {sysparm_limit}')
job.logger().info(f, f'params : {params}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'prefix_target : {prefix_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'crawler : {crawler}')

#variables

#determine s3 key
File = f.split('.')[0]
devtemplateprojectname = f"kbx.{PRODUCT}.dl.{DOMAIN}.{ENTITY}.{File}"


ingest_date = start_time.strftime('%Y-%m-%d')
extract_date = f'ingest_date={ingest_date}'
extract_datetime = start_time.strftime("%Y%m%d%H%M%S%f")

filepath = f"{prefix_target}/{extract_date}/{devtemplateprojectname}+py+{extract_datetime}.json"
job.logger().info(f, f"object key={filepath}")

job.logger().info(f, f'###################_TASK-1_DEF_UDF_###################')
# User Defined Functions

def uploadFile(inputStream, filePath, bucketName):
    s3_resource = boto3.resource('s3')
    s3_client = boto3.client('s3')

    def isBucketExists():
        try:
            s3_resource.meta.client.head_bucket(Bucket=bucketName)
        except botocore.exceptions.ClientError as e:
            return False
        else :
            return True
    #logger  
    if (not isBucketExists()):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = s3_resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream)
    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
        job.logger().info(f, f"File uploaded at {filePath}")
        return True
    else :
        job.logger().info(f, f"Upload failed with HTTPStatusCode {res.get('HTTPStatusCode')}")
        return False


#%%------------------------------------------Job Process ------------------------------------------
job.logger().info(f, f'###################_TASK-2_START_INGESTION_###################')

try:
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))

    response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
    data = response.json()
    url = response.url
    job.logger().info(f, f'requested_url : {url}')

    job.logger().info(f, f"api headers first {response.headers}")
    while ('next' in response.links and response.status_code == 200) :
        url=response.links['next']['url']
        job.logger().info(f, f'requested_url : {url}')
        response =  requests.get(url=url, auth=(servicenow_user, servicenow_pwd), headers=headers)
        data['result'].extend(response.json()['result'])
        job.logger().info(f, f"api headers next {response.headers}")
        #raise Exception('force exception')
    else :
        if response.status_code != 200 :
            job.logger().critical(f, f"api response status code : {response.status_code}")
            raise Exception(f"api response status code : {response.status_code}")
        else :
            job.logger().info(f, f"api response status code : {response.status_code}")
    
    input_stream = bytes(json.dumps(data).encode('UTF-8'))
    if uploadFile(input_stream, filepath, bucket_target) :
        job.logger().info(f, f"File s3a://{bucket_target}/{filepath} uploaded successfully")
        job.logger().info(f, f'###################_TASK-3_JOB_RUN_SUCCESSFULL_###################')
    else :
        raise Exception(f"Upload Failed")

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().info(f, f'###################_TASK-3_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

# %%<path=>SERVICENOW\Testing\ingest_incident_TEST.py<content=>
#%%------------------------------------------ingest------------------------------------------

import os
import sys
import argparse
import requests
import json
import boto3
import botocore
from datetime import datetime
from datetime import timedelta

# Timer
start_time = datetime.utcnow()
ingest_start_time = f'''{(start_time - timedelta(days=7)).strftime("'%Y-%m-%d','00:00:00'")}'''
ingest_end_time = f'''{start_time.strftime("'%Y-%m-%d','00:00:00'")}'''

# file
f = os.path.basename(__file__)

# secret
servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%------------------------------------------Init------------------------------------------
from KbxtDlPy.Harness import Job
job = Job(name="ingest", level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")
job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')
job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
parser.add_argument('--sysparm_limit')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')

args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target
bucket_target = "kbxt-dl-analytics-servicenow-incident-raw-dev" #remove

prefix_target = args.prefix_target
prefix_target = "incident" #remove

sysparm_limit = args.sysparm_limit
sysparm_limit = 10000 #remove

ENV = args.Environment
ENV = 'dev'
PRODUCT = args.Product
PRODUCT = 'analytics'
ENTITY = args.Entity
ENTITY = 'incident'
DOMAIN = args.Domain
DOMAIN = 'servicenow'
OFFSET = 0

table = prefix_target
base_api_url = f'https://kochprod.service-now.com/api/now/table/{table}'
headers = {"Content-Type":"application/json","Accept":"application/json"}
crawler = bucket_target.replace('-', '_')+'-crawler'

params = {
    'sysparm_query': f"sys_updated_on>=javascript:gs.dateGenerate({ingest_start_time})^sys_updated_on<javascript:gs.dateGenerate({ingest_end_time})",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': sysparm_limit,
    'sysparm_offset': OFFSET
}

job.logger().info(f,f'''bucket_target={bucket_target},#, 
prefix_target={prefix_target},#, 
base_api_url = {base_api_url},#, 
params = {params}
''')

job.logger().info(f, f'servicenow table to injest : {table}')
job.logger().info(f, f'base_api_url : {base_api_url}')
job.logger().info(f, f'ingest_start_time : {ingest_start_time}')
job.logger().info(f, f'ingest_end_time : {ingest_end_time}')
job.logger().info(f, f'sysparm_limit : {sysparm_limit}')
job.logger().info(f, f'params : {params}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'prefix_target : {prefix_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'crawler : {crawler}')

#variables

#determine s3 key
File = f.split('.')[0]
devtemplateprojectname = f"kbx.{PRODUCT}.dl.{DOMAIN}.{ENTITY}.{File}"


ingest_date = start_time.strftime('%Y-%m-%d')
extract_date = f'ingest_date={ingest_date}'
extract_datetime = start_time.strftime("%Y%m%d%H%M%S%f")

filepath = f"{prefix_target}/{extract_date}/{devtemplateprojectname}+py+{extract_datetime}.json"
job.logger().info(f, f"object key={filepath}")

job.logger().info(f, f'###################_TASK-1_DEF_UDF_###################')
# User Defined Functions

def uploadFile(inputStream, filePath, bucketName):
    s3_resource = boto3.resource('s3')
    s3_client = boto3.client('s3')

    def isBucketExists():
        try:
            s3_resource.meta.client.head_bucket(Bucket=bucketName)
        except botocore.exceptions.ClientError as e:
            return False
        else :
            return True
    #logger  
    if (not isBucketExists()):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = s3_resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream)
    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
        job.logger().info(f, f"File uploaded at {filePath}")
        return True
    else :
        job.logger().info(f, f"Upload failed with HTTPStatusCode {res.get('HTTPStatusCode')}")
        return False


#%%------------------------------------------Job Process ------------------------------------------
job.logger().info(f, f'###################_TASK-2_START_INGESTION_###################')

try:
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))

    response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
    data = response.json()
    url = response.url
    job.logger().info(f, f'requested_url : {url}')

    job.logger().info(f, f"api headers first {response.headers}")
    while ('next' in response.links and response.status_code == 200) :
        url=response.links['next']['url']
        job.logger().info(f, f'requested_url : {url}')
        response =  requests.get(url=url, auth=(servicenow_user, servicenow_pwd), headers=headers)
        data['result'].extend(response.json()['result'])
        job.logger().info(f, f"api headers next {response.headers}")
        #raise Exception('force exception')
    else :
        if response.status_code != 200 :
            job.logger().critical(f, f"api response status code : {response.status_code}")
            raise Exception(f"api response status code : {response.status_code}")
        else :
            job.logger().info(f, f"api response status code : {response.status_code}")
    
    input_stream = bytes(json.dumps(data).encode('UTF-8'))
    # if uploadFile(input_stream, filepath, bucket_target) :
    #     job.logger().info(f, f"File s3a://{bucket_target}/{filepath} uploaded successfully")
    #     job.logger().info(f, f'###################_TASK-3_JOB_RUN_SUCCESSFULL_###################')
    # else :
    #     raise Exception(f"Upload Failed")

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().info(f, f'###################_TASK-3_JOB_FAILED_###################')
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

# %%<path=>SERVICENOW\Testing\ingest_task_TEST.py<content=>
#%%------------------------------------------ingest------------------------------------------

import os
import sys
import argparse
import requests
import json
import boto3
import botocore
from datetime import datetime
from datetime import timedelta

# Timer
start_time = datetime.utcnow()
ingest_start_time = f'''{(start_time - timedelta(days=1)).strftime("'%Y-%m-%d','00:00:00'")}'''
ingest_end_time = f'''{start_time.strftime("'%Y-%m-%d','00:00:00'")}'''

# file
f = os.path.basename(__file__)

# secret
servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%------------------------------------------Init------------------------------------------
from KbxtDlPy.Harness import Job
job = Job(name="ingest", level="INFO") #overload Job(name="ingest", level="DEBUG", protocol="s3n")

job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_target', nargs='?', const='', type=str, default='')
parser.add_argument('--sysparm_limit')


args, unknown = parser.parse_known_args()

bucket_target = args.bucket_target
#bucket_target = "kbxt-dl-analytics-servicenow-task-raw-dev" #remove

prefix_target = args.prefix_target
#prefix_target = "task" #remove

sysparm_limit = args.sysparm_limit
#sysparm_limit = 5000 #remove

OFFSET = 0
table = bucket_target.split('-')[-3]
base_api_url = f'https://kochprod.service-now.com/api/now/table/{table}'
headers = {"Content-Type":"application/json","Accept":"application/json"}

params = {
    'sysparm_query': f"sys_updated_on>=javascript:gs.dateGenerate({ingest_start_time})^sys_updated_on<javascript:gs.dateGenerate({ingest_end_time})",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': sysparm_limit,
    'sysparm_offset': OFFSET,
    'sys_class_name': 'sc_task'
}

job.logger().info(f,f'''bucket_target={bucket_target},#, 
prefix_target={prefix_target},#, 
base_api_url = {base_api_url},#, 
params = {params}
''')

#variables

# determine s3 key
from_bucket_name = bucket_target.replace('-', '.')
ProductName = from_bucket_name.split('.')[2]
Domain = from_bucket_name.split('.')[3]
EntityName = from_bucket_name.split('.')[4]
File = f.split('.')[0]
devtemplateprojectname = f"kbx.{ProductName}.dl.{Domain}.{EntityName}.{File}"

ingest_date = start_time.strftime('%Y-%m-%d')
extract_date = f'ingest_date={ingest_date}'
extract_datetime = start_time.strftime("%Y%m%d%H%M%S%f")

job.logger().info(f, f"ingesting data from {ingest_start_time} to {ingest_end_time}")

filepath = f"{prefix_target}/{extract_date}/{devtemplateprojectname}+py+{extract_datetime}.json"
job.logger().info(f, f"object key={filepath}")

# User Defined Functions

def uploadFile(inputStream, filePath, bucketName):
    s3_resource = boto3.resource('s3')
    s3_client = boto3.client('s3')

    def isBucketExists():
        try:
            s3_resource.meta.client.head_bucket(Bucket=bucketName)
        except botocore.exceptions.ClientError as e:
            return False
        else :
            return True
    #logger  
    if (not isBucketExists()):
        raise Exception("Upload failed. Bucket {} does not exist".format(bucketName))

    obj = s3_resource.Object(bucketName, filePath)
    response = obj.put(Body=inputStream)
    res = response.get("ResponseMetadata")

    if res.get('HTTPStatusCode') == 200:
        job.logger().info(f, f"File uploaded at {filePath}")
        return True
    else :
        job.logger().info(f, f"Upload failed with HTTPStatusCode {res.get('HTTPStatusCode')}")
        return False


#%%------------------------------------------Job Process ------------------------------------------
try:
    job.logger().info(f, "Ingest job started in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))

    response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
    data = response.json()
    url = response.url
    job.logger().info(f, f"api headers first {response.headers}")
    while ('next' in response.links and response.status_code == 200) :
        url=response.links['next']['url']
        response =  requests.get(url=url, auth=(servicenow_user, servicenow_pwd), headers=headers)
        data['result'].extend(response.json()['result'])
        job.logger().info(f, f"api headers next {response.headers}")
        #raise Exception('force exception')
    else :
        if response.status_code != 200 :
            job.logger().critical(f, f"api response status code : {response.status_code}")
            raise Exception(f"api response status code : {response.status_code}")
        else :
            job.logger().info(f, f"api response status code : {response.status_code}")
    
    input_stream = bytes(json.dumps(data).encode('UTF-8'))
    if uploadFile(input_stream, filepath, bucket_target) :
        job.logger().info(f, f"File s3a://{bucket_target}/{filepath} uploaded successfully")
    else :
        raise Exception(f"Upload Failed")

    ##### YOUR CODE END #####

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))
<path=>SERVICENOW\Testing\pagination_api.py<content=>
#%% import
import pandas as pd
import requests
import datetime
from datetime import date
from datetime import datetime, timedelta
import json
import time

#from datetime import datetime
headers = {"Content-Type":"application/json","Accept":"application/json"}

user = 'SRV_KBXTReport'
pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%
TABLE = 'incident'
MAX_REC = 50
BASE_API_URL = f'https://kochprod.service-now.com/api/now/table/{TABLE}'
OFFSET = 0
ingest_start_time = datetime.strptime('2022-07-13', '%Y-%m-%d')
ingest_end_time = ingest_start_time - timedelta(days=1)
# ingest_start_time = f"""{ingest_start_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""
# ingest_end_time = f"""{ingest_end_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""

ingest_start_time = f'''{ingest_start_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''
ingest_end_time = f'''{ingest_end_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''


params = {
    'sysparm_query': f"sys_updated_on>=javascript:gs.dateGenerate({ingest_end_time})^sys_updated_on<javascript:gs.dateGenerate({ingest_start_time})",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': MAX_REC,
    'sysparm_offset': OFFSET
}


#%%
response =  requests.get(url=BASE_API_URL, params=params, auth=(user, pwd), headers=headers)
data = response.json()
while ('next' in response.links) :
    print('next')
    response =  requests.get(url=BASE_API_URL, params=params, auth=(user, pwd), headers=headers)
    data['result'].extend(response.json()['result'])
    print('done')
    time.sleep(3)


# %%
<path=>SERVICENOW\Testing\taskHistory.py<content=>
#%% import
import pandas as pd
import requests
import datetime
from datetime import date
from datetime import datetime, timedelta
import json
import time

#from datetime import datetime
headers = {"Content-Type":"application/json","Accept":"application/json"}

servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%
TABLE = 'task'
MAX_REC = 3000
base_api_url = f'https://kochprod.service-now.com/api/now/table/{TABLE}'
OFFSET = 0
ingest_start_time = datetime.strptime('2022-07-13', '%Y-%m-%d')
ingest_end_time = ingest_start_time - timedelta(days=1)
# ingest_start_time = f"""{ingest_start_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""
# ingest_end_time = f"""{ingest_end_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""

ingest_start_time = f'''{ingest_start_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''
ingest_end_time = f'''{ingest_end_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''


params = {
    'sysparm_query': "assignment_group=3a9eb4071b859090e6cea687bd4bcb68^ORassignment_group=5d7921630fd68a006ee822d8b1050ed5",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': MAX_REC,
    'sysparm_offset': OFFSET,
    'sys_class_name': 'sc_task' 
}


#%%

response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
data = response.json()
url = response.url
print(f"api headers first {response.headers}")
while ('next' in response.links and response.status_code == 200) :
    url=response.links['next']['url']
    response =  requests.get(url=url, auth=(servicenow_user, servicenow_pwd), headers=headers)
    data['result'].extend(response.json()['result'])
    print(f"api headers next {response.headers}")
    print (len(data['result']))

with open("taskHistory.json", "w") as f:
    json.dump(data, f)

# %%
<path=>SERVICENOW\Testing\taskHistory_multiprocessing.py<content=>
#%% import
import pandas as pd
import requests
import datetime
from datetime import date
from datetime import datetime, timedelta
import json
import time
import math

#from datetime import datetime
headers = {"Content-Type":"application/json","Accept":"application/json"}

servicenow_user = 'SRV_KBXTReport'
servicenow_pwd = 'I66zCVk7fAcvuitZi2Yq'

#%%
TABLE = 'task'
MAX_REC = 1000
base_api_url = f'https://kochprod.service-now.com/api/now/table/{TABLE}'
OFFSET = 0
ingest_start_time = datetime.strptime('2022-07-13', '%Y-%m-%d')
ingest_end_time = ingest_start_time - timedelta(days=1)
# ingest_start_time = f"""{ingest_start_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""
# ingest_end_time = f"""{ingest_end_time.strftime("'%Y-%m-%d'%%2C'%H%%3A%M%%3A%S'")}"""

ingest_start_time = f'''{ingest_start_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''
ingest_end_time = f'''{ingest_end_time.strftime("'%Y-%m-%d','%H:%M:%S'")}'''


params = {
    'sysparm_query': "assignment_group=3a9eb4071b859090e6cea687bd4bcb68^ORassignment_group=5d7921630fd68a006ee822d8b1050ed5",
    'sysparm_display_value': 'true',
    'sysparm_exclude_reference_link': 'true',
    'sysparm_limit': MAX_REC,
    'sysparm_offset': OFFSET,
    'sys_class_name': 'sc_task' 
}

#%%
def calculate_offsets(params) :
    params = params.copy()
    params['sysparm_limit'] = 1
    response =  requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
    response.headers['X-Total-Count']
    number_of_offsets = math.ceil(int(response.headers['X-Total-Count'])/MAX_REC)
    return number_of_offsets

params_params = []

for n in range(calculate_offsets(params)) :
    parameters = params.copy()
    parameters['sysparm_offset'] = int(MAX_REC)*n
    params_params.append(parameters)

def send_request(params) :
    response = requests.get(url=base_api_url, params=params, auth=(servicenow_user, servicenow_pwd), headers=headers)
    print (len(response.json()['result']))
    print (response.headers)
    return response


#%%
import concurrent.futures as cf

with cf.ProcessPoolExecutor() as executor :
    results = executor.map(send_request, params_params)

response = next(results)
data = response.json()
print (len(data['result']))

for response in results :
    data['result'].extend(response.json()['result'])
    print (len(data['result']))

with open("taskHistory_mp.json", "w") as f:
    json.dump(data, f)
# %%
<path=>SERVICENOW\Testing\task_structured_test.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Task.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
from KbxtDlPy.Harness import Job

job = Job(name="transform", level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")
job.logger().info(f, f'###################_TASK-0_INITIALIZING_PARAMETERS_###################')
# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
#date_partition_override = "ingest_date=2022-09-07"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
bucket_source = "kbxt-dl-analytics-servicenow-task-raw-qa" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
bucket_target = "kbxt-dl-analytics-servicenow-task-structured-qa" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
prefix_source = 'task' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
ENV = 'dev'
PRODUCT = args.Product
PRODUCT = 'analytics'
ENTITY = args.Entity
ENTITY = 'task'
DOMAIN = args.Domain
DOMAIN = 'servicenow'


curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

job.logger().info(f, f'date_partition_override : {date_partition_override}')
job.logger().info(f, f'bucket_source : {bucket_source}')
job.logger().info(f, f'bucket_target : {bucket_target}')
job.logger().info(f, f'ENV : {ENV}')
job.logger().info(f, f'PRODUCT : {PRODUCT}')
job.logger().info(f, f'ENTITY : {ENTITY}')
job.logger().info(f, f'DOMAIN : {DOMAIN}')
job.logger().info(f, f'curated_db : {curated_db}')
job.logger().info(f, f'structured_db : {structured_db}')
job.logger().info(f, f'tbl : {tbl}')
job.logger().info(f, f'crawler : {crawler}')

#%%UDF

def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df
#%% last curated ingest date

df_master_data = spark.read.format('json').load('masterData/sc_task.json')
df_master_data_flatten = flatten(df_master_data)
df_master_data_flatten.createOrReplaceTempView('master_raw_data')
df_master_raw_data_processed = spark.sql(f'''
            select                                                        
                cast(TRIM(u_koch_catalog_item) as string)                                    u_koch_catalog_item,
                cast(TRIM(parent) as string)                                                              parent,
                cast(TRIM(made_sla) as string)                                                          made_sla,
                case 
                    when TRIM(u_custom_date_2) <> ''
                    then to_timestamp(TRIM(u_custom_date_2), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                              u_custom_date_2,
                cast(TRIM(watch_list) as string)                                                      watch_list,
                case 
                    when TRIM(u_custom_date_1) <> ''
                    then to_timestamp(TRIM(u_custom_date_1), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                              u_custom_date_1,
                cast(TRIM(u_manual_routing) as string)                                          u_manual_routing,
                cast(TRIM(sn_esign_document) as string)                                        sn_esign_document,
                cast(TRIM(upon_reject) as string)                                                    upon_reject,
                TRIM(sys_updated_on)                                                              sys_updated_on,
                cast(TRIM(task_effective_number) as string)                                task_effective_number,
                cast(TRIM(u_escalate) as string)                                                      u_escalate,
                cast(TRIM(u_estimated_delivery_date) as string)                        u_estimated_delivery_date,
                cast(TRIM(approval_history) as string)                                          approval_history,
                cast(TRIM(skills) as string)                                                              skills,
                cast(TRIM(number) as string)                                                              number,
                cast(TRIM(sys_updated_by) as string)                                              sys_updated_by,
                cast(TRIM(opened_by) as string)                                                        opened_by,
                cast(TRIM(user_input) as string)                                                      user_input,
                case 
                    when TRIM(sys_created_on) <> ''
                    then to_timestamp(TRIM(sys_created_on), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                               sys_created_on,
                cast(TRIM(sys_domain) as string)                                                      sys_domain,
                cast(TRIM(state) as string)                                                                state,
                cast(TRIM(route_reason) as string)                                                  route_reason,
                cast(TRIM(sys_created_by) as string)                                              sys_created_by,
                cast(TRIM(knowledge) as string)                                                        knowledge,
                cast(TRIM(order) as string)                                                                order,
                case 
                    when TRIM(closed_at) <> ''
                    then to_timestamp(TRIM(closed_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    closed_at,
                cast(TRIM(cmdb_ci) as string)                                                            cmdb_ci,
                cast(TRIM(cmdb_ci_business_app) as string)                                  cmdb_ci_business_app,
                cast(TRIM(contract) as string)                                                          contract,
                cast(TRIM(impact) as string)                                                              impact,
                cast(TRIM(active) as string)                                                              active,
                cast(TRIM(business_service) as string)                                          business_service,
                cast(TRIM(priority) as string)                                                          priority,
                cast(TRIM(sys_domain_path) as string)                                            sys_domain_path,
                cast(TRIM(time_worked) as string)                                                    time_worked,
                cast(TRIM(expected_start) as string)                                              expected_start,
                case 
                    when TRIM(opened_at) <> ''
                    then to_timestamp(TRIM(opened_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    opened_at,
                cast(TRIM(u_support_tier) as string)                                              u_support_tier,
                cast(TRIM(business_duration) as string)                                        business_duration,
                cast(TRIM(group_list) as string)                                                      group_list,
                cast(TRIM(u_new_hire) as string)                                                      u_new_hire,
                case 
                    when TRIM(work_end) <> ''
                    then to_timestamp(TRIM(work_end), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                     work_end,
                /*cast(TRIM(work_notes) as string)                                                      work_notes,*/
                case 
                    when TRIM(u_start_date) <> ''
                    then to_timestamp(TRIM(u_start_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                 u_start_date,
                cast(TRIM(universal_request) as string)                                        universal_request,
                cast(TRIM(short_description) as string)                                        short_description,
                cast(TRIM(correlation_display) as string)                                    correlation_display,
                case 
                    when TRIM(work_start) <> ''
                    then to_timestamp(TRIM(work_start), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                   work_start,
                cast(TRIM(assignment_group) as string)                                          assignment_group,
                cast(TRIM(additional_assignee_list) as string)                          additional_assignee_list,
                cast(TRIM(description) as string)                                                    description,
                cast(TRIM(u_custom_text_1) as string)                                            u_custom_text_1,
                cast(TRIM(calendar_duration) as string)                                        calendar_duration,
                cast(TRIM(close_notes) as string)                                                    close_notes,
                cast(TRIM(sys_class_name) as string)                                              sys_class_name,
                cast(TRIM(u_custom_text_2) as string)                                            u_custom_text_2,
                cast(TRIM(closed_by) as string)                                                        closed_by,
                case 
                    when TRIM(follow_up) <> ''
                    then to_timestamp(TRIM(follow_up), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    follow_up,
                cast(TRIM(sys_id) as string)                                                              sys_id,
                cast(TRIM(contact_type) as string)                                                  contact_type,
                cast(TRIM(sn_esign_esignature_configuration) as string)        sn_esign_esignature_configuration,
                cast(TRIM(urgency) as string)                                                            urgency,
                cast(TRIM(company) as string)                                                            company,
                cast(TRIM(reassignment_count) as bigint)                                      reassignment_count,
                cast(TRIM(activity_due) as string)                                                  activity_due,
                cast(TRIM(assigned_to) as string)                                                    assigned_to,
                cast(TRIM(u_custom_reference_2) as string)                                  u_custom_reference_2,
                cast(TRIM(comments) as string)                                                          comments,
                cast(TRIM(u_custom_reference_1) as string)                                  u_custom_reference_1,
                cast(TRIM(approval) as string)                                                          approval,
                /*case 
                    when TRIM(sla_due) <> ''
                    then to_timestamp(TRIM(sla_due), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                      sla_due,*/
                cast(TRIM(sla_due) as string)                                                            sla_due,
                cast(TRIM(u_koch_customer) as string)                                            u_koch_customer,
                case 
                    when TRIM(due_date) <> ''
                    then to_timestamp(TRIM(due_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                     due_date,
                cast(TRIM(sys_mod_count) as bigint)                                                sys_mod_count,
                cast(TRIM(sys_tags) as string)                                                          sys_tags,
                cast(TRIM(agile_story) as string)                                                    agile_story,
                cast(TRIM(escalation) as string)                                                      escalation,
                cast(TRIM(upon_approval) as string)                                                upon_approval,
                cast(TRIM(u_all_classes_configuration_items) as string)        u_all_classes_configuration_items,
                cast(TRIM(correlation_id) as string)                                              correlation_id,
                cast(TRIM(location) as string)                                                          location
            from master_raw_data
        ''')
df_master_raw_data_processed.createOrReplaceTempView('master_raw_data_processed')

#%% minus query testing

spark.sql(f'''
select
number, 
active,
additional_assignee_list

from
{structured_db}.{tbl}
where
ingest_date='2022-09-17'

minus

select
number,
active,
additional_assignee_list
from
master_raw_data_processed
where
where sys_updated_on=to_date('2022-09-16')

''').show()

#%%

spark.sql(f'''
select
number,
active,
additional_assignee_list,
agile_story,
case 
    when approval = 'not requested' 
    then 'Not Yet Requested' 
    end approval,
approval_history
from
master_raw_data_processed
where sys_updated_on=to_date('2022-09-16')

minus

select
number, 
active,
additional_assignee_list,
agile_story,
approval,
approval_history
from
{structured_db}.{tbl}
where
ingest_date='2022-09-17'
''').show()







# %%
<path=>SERVICENOW\Testing\task_structured_transform.py<content=>
#%%------------------------------------------transform------------------------------------------

import os
import sys
import json
import boto3
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import argparse
# from awsglue.context import GlueContext
import pyspark.sql.functions as F
from pyspark.sql.utils import AnalysisException

# Timer
start_time = datetime.utcnow()

# file
f = os.path.basename(__file__)

client = boto3.client('glue')

# Interactive Shell
# change to your version of hadoop
os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'

# Spark
spark = SparkSession \
    .builder \
    .appName("KBX.Analytics.DL.ServiceNow.Task.Transform") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.hive.convertMetastoreParquet", "false") \
    .config("spark.sql.hive.caseSensitiveInferenceMode", "NEVER_INFER") \
    .config("hive.metastore.client.factory.class", "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory") \
    .enableHiveSupport() \
    .getOrCreate()

sc = spark.sparkContext
# glueContext = GlueContext(sc)
# gluespark = glueContext.spark_session

spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

# Authentication, use AWS chain, or can set explicitely
spark._jsc.hadoopConfiguration().set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain")


#%%------------------------------------------Init------------------------------------------
from KbxtDlPy.Harness import Job

job = Job(name="transform", level="INFO") #overload Job(name="transform", level="DEBUG", protocol="s3n")

# Parameters
# these are set, and can be overridden, in the Infrastructure project Job.yaml file, or if you 
# edit the Glue Job in the console.
parser = argparse.ArgumentParser()
parser.add_argument('--date_partition_override', nargs='?', const='', type=str, default='')
parser.add_argument('--bucket_source')
parser.add_argument('--bucket_target')
parser.add_argument('--prefix_source', nargs='?', const='', type=str, default='')
parser.add_argument('--Environment')
parser.add_argument('--Product')
parser.add_argument('--Entity')
parser.add_argument('--Domain')
args, unknown = parser.parse_known_args()

date_partition_override = args.date_partition_override # ex:"ingest_date=1900-01-01"
#date_partition_override = "ingest_date=2022-08-30"
bucket_source = args.bucket_source # ex:"kbxt-dl-analytics-servicenow-structured-dev"
#bucket_source = "kbxt-dl-analytics-servicenow-task-raw-dev" # delete
bucket_target = args.bucket_target # ex:"kbxt-dl-analytics-servicenow-curated-dev"
#bucket_target = "kbxt-dl-analytics-servicenow-task-structured-dev" # delete
prefix_source = args.prefix_source # ex:"<subdirectory path to date partitions>"
#prefix_source = 'task' # delete

# Prefix of files to process, in case files need to be excluded
file_prefix =  "" # ex:"part-"

ENV = args.Environment
#ENV = 'dev'
PRODUCT = args.Product
#PRODUCT = 'analytics'
ENTITY = args.Entity
#ENTITY = 'task'
DOMAIN = args.Domain
#DOMAIN = 'servicenow'


curated_db = f'kbxt_dl_analytics_db_curated_{ENV}'
structured_db = f'kbxt_dl_analytics_db_structured_{ENV}'
tbl = f'{DOMAIN}_{prefix_source}'
crawler = bucket_target.replace('-', '_')+'-crawler'

JOB_ID = str(start_time).replace('-','').replace(' ','').replace(':','').replace('.','')
SOURCE = DOMAIN.upper() + '_' + ENTITY.upper()

# Variables
err = None
bucket_target_path = "s3a://{}".format(bucket_target)
date_partition = None
if ((len(date_partition_override) <= 0)):
    date_partition = datetime.now().strftime("ingest_date=%Y-%m-%d")
    is_replay = False
else:
    date_partition = date_partition_override
    is_replay = True

#JSON_FROM_SCHEMA = '{"fields":[{"metadata":{},"name":"result","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"active","nullable":true,"type":"string"},{"metadata":{},"name":"activity_due","nullable":true,"type":"string"},{"metadata":{},"name":"additional_assignee_list","nullable":true,"type":"string"},{"metadata":{},"name":"agile_story","nullable":true,"type":"string"},{"metadata":{},"name":"approval","nullable":true,"type":"string"},{"metadata":{},"name":"approval_history","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"business_duration","nullable":true,"type":"string"},{"metadata":{},"name":"business_service","nullable":true,"type":"string"},{"metadata":{},"name":"calendar_duration","nullable":true,"type":"string"},{"metadata":{},"name":"close_notes","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_by","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci_business_app","nullable":true,"type":"string"},{"metadata":{},"name":"comments","nullable":true,"type":"string"},{"metadata":{},"name":"company","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"contract","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_display","nullable":true,"type":"string"},{"metadata":{},"name":"correlation_id","nullable":true,"type":"string"},{"metadata":{},"name":"description","nullable":true,"type":"string"},{"metadata":{},"name":"due_date","nullable":true,"type":"string"},{"metadata":{},"name":"escalation","nullable":true,"type":"string"},{"metadata":{},"name":"expected_start","nullable":true,"type":"string"},{"metadata":{},"name":"follow_up","nullable":true,"type":"string"},{"metadata":{},"name":"group_list","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"string"},{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"order","nullable":true,"type":"string"},{"metadata":{},"name":"parent","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"string"},{"metadata":{},"name":"route_reason","nullable":true,"type":"string"},{"metadata":{},"name":"short_description","nullable":true,"type":"string"},{"metadata":{},"name":"skills","nullable":true,"type":"string"},{"metadata":{},"name":"sla_due","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_document","nullable":true,"type":"string"},{"metadata":{},"name":"sn_esign_esignature_configuration","nullable":true,"type":"string"},{"metadata":{},"name":"state","nullable":true,"type":"string"},{"metadata":{},"name":"sys_class_name","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_on","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain","nullable":true,"type":"string"},{"metadata":{},"name":"sys_domain_path","nullable":true,"type":"string"},{"metadata":{},"name":"sys_id","nullable":true,"type":"string"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"string"},{"metadata":{},"name":"sys_tags","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_on","nullable":true,"type":"string"},{"metadata":{},"name":"task_effective_number","nullable":true,"type":"string"},{"metadata":{},"name":"time_worked","nullable":true,"type":"string"},{"metadata":{},"name":"u_all_classes_configuration_items","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_date_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_reference_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_1","nullable":true,"type":"string"},{"metadata":{},"name":"u_custom_text_2","nullable":true,"type":"string"},{"metadata":{},"name":"u_escalate","nullable":true,"type":"string"},{"metadata":{},"name":"u_estimated_delivery_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_catalog_item","nullable":true,"type":"string"},{"metadata":{},"name":"u_koch_customer","nullable":true,"type":"string"},{"metadata":{},"name":"u_manual_routing","nullable":true,"type":"string"},{"metadata":{},"name":"u_new_hire","nullable":true,"type":"string"},{"metadata":{},"name":"u_start_date","nullable":true,"type":"string"},{"metadata":{},"name":"u_support_tier","nullable":true,"type":"string"},{"metadata":{},"name":"universal_request","nullable":true,"type":"string"},{"metadata":{},"name":"upon_approval","nullable":true,"type":"string"},{"metadata":{},"name":"upon_reject","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"user_input","nullable":true,"type":"string"},{"metadata":{},"name":"watch_list","nullable":true,"type":"string"},{"metadata":{},"name":"work_end","nullable":true,"type":"string"},{"metadata":{},"name":"work_notes","nullable":true,"type":"string"},{"metadata":{},"name":"work_start","nullable":true,"type":"string"}],"type":"struct"},"type":"array"}}],"type":"struct"}'
#schemaFromJson = StructType.fromJson(json.loads(JSON_FROM_SCHEMA))

# functions

#Flatten array of structs and structs
def flatten(df):
   # compute Complex Fields (Lists and Structs) in Schema   
    complex_fields = dict([(field.name, field.dataType)
                            for field in df.schema.fields
                            if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    while len(complex_fields)!=0:
        col_name=list(complex_fields.keys())[0]
        # print ("Processing :"+col_name+" Type : "+str(type(complex_fields[col_name])))
    
        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded = [col(col_name+'.'+k).alias(k) for k in [ n.name for n in  complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)
    
        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):    
            df=df.withColumn(col_name,explode_outer(col_name))
    
        # recompute remaining Complex Fields in Schema       
        complex_fields = dict([(field.name, field.dataType)
                                for field in df.schema.fields
                                if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])
    return df


#%%------------------------------------------Job Start------------------------------------------

# All files for a date partition that haven't been processed are 
# returned, so be cognizent of the size of this dataframe.

try :
    df_curated_tbl_partitions = spark.sql(f'''
    SHOW PARTITIONS {curated_db}.{tbl}
    ''')
    df_curated_tbl_partitions.show(10, truncate=False)
    
    max_partition_date_df = df_curated_tbl_partitions.withColumn('partition_date', F.to_date(F.split(col('partition'), '=').getItem(1))).coalesce(1).filter(col('partition_date') < date_partition.split('=')[1]).sort(col('partition_date').desc())
    max_partition_date = max_partition_date_df.collect()[0][0]
    
    filter_clause = f"ingest_date='{max_partition_date.split('=')[1]}'"
    job.logger().info(f, f'filter_clause {filter_clause}')
    print ('#########curated_partition_to_process########')
    print(f'{filter_clause}')
    
    #filter_clause = "ingest_date='2022-07-25'" # comment
    df_curated = spark.sql(f'''
        select * from {curated_db}.{tbl} where {filter_clause}
    ''')
    df_curated.createOrReplaceTempView('curated')
    
    print ('#########df_curated.printSchema()########')
    df_curated.printSchema()
    print ('#########df_curated.count()########')
    df_curated.count()

except Exception as e:
    job.logger().info(f, e)
    df_curated = None

# raise Exception('Forced Exception')

# raw file df
# df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, schema_json=JSON_FROM_SCHEMA)
df = job.runtime().start(spark=spark, bucket=bucket_source, prefix="{}".format(file_prefix), partition=date_partition, is_replay=is_replay, path=prefix_source, file_format='json')
print ('#########JSON_DATAFRAME########')
df.printSchema()
df.show()

#%%------------------------------------------Job Process------------------------------------------

try:    
    if (df is not None):
        df.cache()
        job.logger().info(f, "Dataframe cached in (hh:mm:ss.ms) is {}.".format(datetime.now()-start_time))
        
        # Inferred schema to validate against, which is in hive (Glue), is lowercase
        df.toDF(*[c.lower() for c in df.columns])
        
        # flattening df
        df_flatten = flatten(df)

        print ('#########df_flatten########')
        df_flatten.printSchema()
        df_flatten.show()
        print ('#########df_flatten.count()########')
        print (df_flatten.count())

        df_flatten.createOrReplaceTempView('raw_data')
        df_structured = spark.sql(f'''
            select
                '{JOB_ID}'                                                                                job_id,
                '{SOURCE}'                                                                                source,                                                          
                cast(TRIM(u_koch_catalog_item) as string)                                    u_koch_catalog_item,
                cast(TRIM(parent) as string)                                                              parent,
                cast(TRIM(made_sla) as string)                                                          made_sla,
                case 
                    when TRIM(u_custom_date_2) <> ''
                    then to_timestamp(TRIM(u_custom_date_2), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                              u_custom_date_2,
                cast(TRIM(watch_list) as string)                                                      watch_list,
                case 
                    when TRIM(u_custom_date_1) <> ''
                    then to_timestamp(TRIM(u_custom_date_1), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                              u_custom_date_1,
                cast(TRIM(u_manual_routing) as string)                                          u_manual_routing,
                cast(TRIM(sn_esign_document) as string)                                        sn_esign_document,
                cast(TRIM(upon_reject) as string)                                                    upon_reject,
                case 
                    when TRIM(sys_updated_on) <> ''
                    then to_timestamp(TRIM(sys_updated_on), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                               sys_updated_on,
                cast(TRIM(task_effective_number) as string)                                task_effective_number,
                cast(TRIM(u_escalate) as string)                                                      u_escalate,
                cast(TRIM(u_estimated_delivery_date) as string)                        u_estimated_delivery_date,
                cast(TRIM(approval_history) as string)                                          approval_history,
                cast(TRIM(skills) as string)                                                              skills,
                cast(TRIM(number) as string)                                                              number,
                cast(TRIM(sys_updated_by) as string)                                              sys_updated_by,
                cast(TRIM(opened_by) as string)                                                        opened_by,
                cast(TRIM(user_input) as string)                                                      user_input,
                case 
                    when TRIM(sys_created_on) <> ''
                    then to_timestamp(TRIM(sys_created_on), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                               sys_created_on,
                cast(TRIM(sys_domain) as string)                                                      sys_domain,
                cast(TRIM(state) as string)                                                                state,
                cast(TRIM(route_reason) as string)                                                  route_reason,
                cast(TRIM(sys_created_by) as string)                                              sys_created_by,
                cast(TRIM(knowledge) as string)                                                        knowledge,
                cast(TRIM(order) as string)                                                                order,
                case 
                    when TRIM(closed_at) <> ''
                    then to_timestamp(TRIM(closed_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    closed_at,
                cast(TRIM(cmdb_ci) as string)                                                            cmdb_ci,
                cast(TRIM(cmdb_ci_business_app) as string)                                  cmdb_ci_business_app,
                cast(TRIM(contract) as string)                                                          contract,
                cast(TRIM(impact) as string)                                                              impact,
                cast(TRIM(active) as string)                                                              active,
                cast(TRIM(business_service) as string)                                          business_service,
                cast(TRIM(priority) as string)                                                          priority,
                cast(TRIM(sys_domain_path) as string)                                            sys_domain_path,
                cast(TRIM(time_worked) as string)                                                    time_worked,
                cast(TRIM(expected_start) as string)                                              expected_start,
                case 
                    when TRIM(opened_at) <> ''
                    then to_timestamp(TRIM(opened_at), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    opened_at,
                cast(TRIM(u_support_tier) as string)                                              u_support_tier,
                cast(TRIM(business_duration) as string)                                        business_duration,
                cast(TRIM(group_list) as string)                                                      group_list,
                cast(TRIM(u_new_hire) as string)                                                      u_new_hire,
                case 
                    when TRIM(work_end) <> ''
                    then to_timestamp(TRIM(work_end), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                     work_end,
                /*cast(TRIM(work_notes) as string)                                                      work_notes,*/
                case 
                    when TRIM(u_start_date) <> ''
                    then to_timestamp(TRIM(u_start_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                 u_start_date,
                cast(TRIM(universal_request) as string)                                        universal_request,
                cast(TRIM(short_description) as string)                                        short_description,
                cast(TRIM(correlation_display) as string)                                    correlation_display,
                case 
                    when TRIM(work_start) <> ''
                    then to_timestamp(TRIM(work_start), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                   work_start,
                cast(TRIM(assignment_group) as string)                                          assignment_group,
                cast(TRIM(additional_assignee_list) as string)                          additional_assignee_list,
                cast(TRIM(description) as string)                                                    description,
                cast(TRIM(u_custom_text_1) as string)                                            u_custom_text_1,
                cast(TRIM(calendar_duration) as string)                                        calendar_duration,
                cast(TRIM(close_notes) as string)                                                    close_notes,
                cast(TRIM(sys_class_name) as string)                                              sys_class_name,
                cast(TRIM(u_custom_text_2) as string)                                            u_custom_text_2,
                cast(TRIM(closed_by) as string)                                                        closed_by,
                case 
                    when TRIM(follow_up) <> ''
                    then to_timestamp(TRIM(follow_up), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                    follow_up,
                cast(TRIM(sys_id) as string)                                                              sys_id,
                cast(TRIM(contact_type) as string)                                                  contact_type,
                cast(TRIM(sn_esign_esignature_configuration) as string)        sn_esign_esignature_configuration,
                cast(TRIM(urgency) as string)                                                            urgency,
                cast(TRIM(company) as string)                                                            company,
                cast(TRIM(reassignment_count) as bigint)                                      reassignment_count,
                cast(TRIM(activity_due) as string)                                                  activity_due,
                cast(TRIM(assigned_to) as string)                                                    assigned_to,
                cast(TRIM(u_custom_reference_2) as string)                                  u_custom_reference_2,
                cast(TRIM(comments) as string)                                                          comments,
                cast(TRIM(u_custom_reference_1) as string)                                  u_custom_reference_1,
                cast(TRIM(approval) as string)                                                          approval,
                /*case 
                    when TRIM(sla_due) <> ''
                    then to_timestamp(TRIM(sla_due), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                      sla_due,*/
                cast(TRIM(sla_due) as string)                                                            sla_due,
                cast(TRIM(u_koch_customer) as string)                                            u_koch_customer,
                case 
                    when TRIM(due_date) <> ''
                    then to_timestamp(TRIM(due_date), 'MM-dd-yyyy hh:mm:ss a')
                    else to_timestamp(TRIM('01-01-1900 12:00:00 AM'), 'MM-dd-yyyy hh:mm:ss a')   
                end                                                                                     due_date,
                cast(TRIM(sys_mod_count) as bigint)                                                sys_mod_count,
                cast(TRIM(sys_tags) as string)                                                          sys_tags,
                cast(TRIM(agile_story) as string)                                                    agile_story,
                cast(TRIM(escalation) as string)                                                      escalation,
                cast(TRIM(upon_approval) as string)                                                upon_approval,
                cast(TRIM(u_all_classes_configuration_items) as string)        u_all_classes_configuration_items,
                cast(TRIM(correlation_id) as string)                                              correlation_id,
                cast(TRIM(location) as string)                                                          location
            from raw_data
        ''')
        
        df_structured.createOrReplaceTempView('structured')
        
        if df_curated == None :
            df_transformed = spark.sql('''
                select
                    *
                from
                    structured
                where
                    assignment_group = 'KBXL Systems Coordinators'
                        or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
            ''')
        else :
            df_transformed = spark.sql('''
                select
                    *
                from
                    structured
                where
                    assignment_group = 'KBXL Systems Coordinators'
                        or
                    assignment_group = 'KII KBXL TRANSPORTATION SUPPORT'
                        or
                    number in (select number from curated)
            ''')

        print ('#########df_transformed########')
        df_transformed.printSchema()
        df_transformed.show()
        print ('#########df_transformed.count()########')
        df_transformed.count()
        
        try :
            add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
            job.logger().info(f, f'add_partition {add_partition}')
            df_add_partition = spark.sql(add_partition)
        except Exception as e:
            job.logger().info(f, e)
        
        # Commit files
        if not df_transformed.rdd.isEmpty() :
            job.runtime().commit(df_transformed, prefix_source, "{}/{}/{}".format(bucket_target_path, prefix_source, date_partition))
        
        
        # Refresh Partition or if table not present run crawler to add table
        df_table = spark.sql(f'''show tables in {structured_db} like "{tbl}"''').filter(F.col('isTemporary') == 'false')
        df_table.show()
        if df_table.count() == 1 :
            try :
                add_partition = f"ALTER TABLE {structured_db}.{tbl} ADD PARTITION (ingest_date='{date_partition.split('=')[1]}')"
                job.logger().info(f, f'add_partition {add_partition}')
                df_add_partition = spark.sql(add_partition)
                job.logger().info(f, f"partion {date_partition.split('=')[1]} added to {tbl}")
            except Exception as e:
                job.logger().info(f, e)
        else :
            job.logger().info(f, f'initiating {crawler} run for first time')
            response = client.start_crawler(
                        Name=crawler
                    )
            
            response_get = client.get_crawler(Name=crawler)
            state = response_get["Crawler"]["State"]
            job.logger().info(f, f"Crawler '{crawler}' is {state.lower()}.")
            state_previous = state
            while (state != "READY") :
                response_get = client.get_crawler(Name=crawler)
                state = response_get["Crawler"]["State"]
                if state != state_previous:
                    job.logger().info(f, f"Crawler {crawler} is {state.lower()}.")
                    state_previous = state
        
        
        # Success
        job.logger().info(f, "{} : successfully saved {} records.".format(prefix_source, df_transformed.count()))

except Exception as e:
    job.logger().critical(f, e)
    raise Exception("{}:{}:{}".format(f, "67448ff3-4eef-4e3c-9379-4c935242ce10", e))

#%%------------------------------------------Job End------------------------------------------

job.runtime().end()

# %%
<path=>SERVICENOW\Testing\test_structured.py<content=>
