{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f53ef31b-3c0a-4b83-a4b5-b14309d80d40;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 595ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f53ef31b-3c0a-4b83-a4b5-b14309d80d40\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/18ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, types as T, functions as F\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"learn\")\n",
    "conf.set('spark.jars.packages', 'io.delta:delta-core_2.12:2.1.0')\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sr_No: string (nullable = true)\n",
      " |-- Date_of_Encashment: string (nullable = true)\n",
      " |-- Name_of_the_Political_Party: string (nullable = true)\n",
      " |-- Account_no._of_Political_Party: string (nullable = true)\n",
      " |-- Prefix: string (nullable = true)\n",
      " |-- Bond_Number: string (nullable = true)\n",
      " |-- redemer_Denominations: string (nullable = true)\n",
      " |-- Pay_Branch_Code: string (nullable = true)\n",
      " |-- Pay_Teller: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Sr_No: string (nullable = true)\n",
      " |-- Reference_No_(URN): string (nullable = true)\n",
      " |-- Journal_Date: string (nullable = true)\n",
      " |-- Date_of_Purchase: string (nullable = true)\n",
      " |-- Date_of_Expiry: string (nullable = true)\n",
      " |-- Name_of_the_Purchaser: string (nullable = true)\n",
      " |-- Prefix: string (nullable = true)\n",
      " |-- Bond_Number: string (nullable = true)\n",
      " |-- purchaser_Denominations: string (nullable = true)\n",
      " |-- Issue_Branch_Code: string (nullable = true)\n",
      " |-- Issue_Teller: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_redemer = T.StructType([T.StructField(nm, T.StringType(), True) for nm in ['Sr_No', 'Date_of_Encashment', 'Name_of_the_Political_Party', 'Account_no._of_Political_Party', 'Prefix', 'Bond_Number', 'redemer_Denominations', 'Pay_Branch_Code', 'Pay_Teller'] ])\n",
    "schema_purchaser = T.StructType([T.StructField(nm, T.StringType(), True) for nm in ['Sr_No', 'Reference_No_(URN)', 'Journal_Date', 'Date_of_Purchase', 'Date_of_Expiry', 'Name_of_the_Purchaser', 'Prefix', 'Bond_Number', 'purchaser_Denominations', 'Issue_Branch_Code', 'Issue_Teller', 'Status'] ])\n",
    "df_redemer = spark.read.format('csv').schema(schema_redemer).load('/home/glue_user/workspace/data-engineering/data/source/src-data-csv/electoral-bonds/bond_redemer_details.csv').filter(\"Pay_Teller is not null\")\n",
    "df_purchaser = spark.read.format('csv').schema(schema_purchaser).load('/home/glue_user/workspace/data-engineering/data/source/src-data-csv/electoral-bonds/bond_purchaser_details.csv').filter(\"purchaser_Denominations is not null\")\n",
    "\n",
    "df_redemer.printSchema()\n",
    "df_purchaser.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------------+\n",
      "|party_wise_donation|Name_of_the_Political_Party|\n",
      "+-------------------+---------------------------+\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "|          654500000|            AAM AADMI PARTY|\n",
      "+-------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_redemer.select(F.expr(\"sum(cast(replace(redemer_Denominations, ',', '') as bigint)) over(partition by Name_of_the_Political_Party)\").alias('party_wise_donation'),\n",
    "                    F.expr(\"Name_of_the_Political_Party\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 13:45:00 WARN SimpleFunctionRegistry: The function str_to_inr replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.str_to_inr(s: str) -> str>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str_to_inr(s: str) -> str :\n",
    "    n = int(s)\n",
    "    q = n%1000\n",
    "    s1 = str(n%1000)\n",
    "    n = int(n/1000)\n",
    "    s2 = ''\n",
    "    while(n):\n",
    "        r = n%100\n",
    "        n = int(n/100)\n",
    "        s2 = str(r) + ',' + s2\n",
    "    return s2+s1\n",
    "str_to_inr = F.udf(str_to_inr, T.StringType())\n",
    "spark.udf.register(\"str_to_inr\", str_to_inr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+---------------------------+------------------------------+------+-----------+---------------------+---------------+----------+\n",
      "|Sr_No|Date_of_Encashment|Name_of_the_Political_Party|Account_no._of_Political_Party|Prefix|Bond_Number|redemer_Denominations|Pay_Branch_Code|Pay_Teller|\n",
      "+-----+------------------+---------------------------+------------------------------+------+-----------+---------------------+---------------+----------+\n",
      "|   34|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       6646|             10000000|          00847|   3367975|\n",
      "|   54|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5457|             10000000|          00847|   3367975|\n",
      "|   35|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       6644|             10000000|          00847|   3367975|\n",
      "|    2|       12/Apr/2019|       ALL INDIA ANNA DR...|                   *******5199|    OC|       3975|             10000000|          00800|   2770121|\n",
      "|   36|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5470|             10000000|          00847|   3367975|\n",
      "|   44|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5474|             10000000|          00847|   3367975|\n",
      "|   45|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5459|             10000000|          00847|   3367975|\n",
      "|    1|       12/Apr/2019|       ALL INDIA ANNA DR...|                   *******5199|    OC|        775|             10000000|          00800|   2770121|\n",
      "|   46|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5475|             10000000|          00847|   3367975|\n",
      "|   37|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5455|             10000000|          00847|   3367975|\n",
      "|   47|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5464|             10000000|          00847|   3367975|\n",
      "|   39|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5460|             10000000|          00847|   3367975|\n",
      "|   48|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5453|             10000000|          00847|   3367975|\n",
      "|   41|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5451|             10000000|          00847|   3367975|\n",
      "|   49|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5472|             10000000|          00847|   3367975|\n",
      "|   43|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5454|             10000000|          00847|   3367975|\n",
      "|   50|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5471|             10000000|          00847|   3367975|\n",
      "|    3|       12/Apr/2019|       ALL INDIA ANNA DR...|                   *******5199|    OC|       3967|             10000000|          00800|   2770121|\n",
      "|   51|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5468|             10000000|          00847|   3367975|\n",
      "|   40|       12/Apr/2019|       BHARAT RASHTRA SA...|                   *******7477|    OC|       5463|             10000000|          00847|   3367975|\n",
      "+-----+------------------+---------------------------+------------------------------+------+-----------+---------------------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_redemer.withColumn('redemer_Denominations', \n",
    "                      F.expr(\"cast(replace(redemer_Denominations, ',', '') as bigint)\")).orderBy(F.expr('redemer_Denominations').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users:\n",
      "+-------+-----------+\n",
      "|user_id|       name|\n",
      "+-------+-----------+\n",
      "|   U123|   John Doe|\n",
      "|   U124|   Jane Doe|\n",
      "|   U125|Alice Smith|\n",
      "|   U126|  Bob Brown|\n",
      "+-------+-----------+\n",
      "\n",
      "addresses:\n",
      "+-------+------------+---------+-----------+\n",
      "|user_id|      street|     city|postal_code|\n",
      "+-------+------------+---------+-----------+\n",
      "|   U123|  123 Elm St|Somewhere|      12345|\n",
      "|   U123|  456 Oak St| Anywhere|      67890|\n",
      "|   U124| 789 Pine St|Somewhere|      12345|\n",
      "|   U125|101 Maple St|Elsewhere|      54321|\n",
      "|   U126|202 Birch St|  Nowhere|      98765|\n",
      "+-------+------------+---------+-----------+\n",
      "\n",
      "favorite_products:\n",
      "+-------+----------+\n",
      "|user_id|product_id|\n",
      "+-------+----------+\n",
      "|   U123|      P001|\n",
      "|   U123|      P002|\n",
      "|   U123|      P003|\n",
      "|   U124|      P004|\n",
      "|   U124|      P005|\n",
      "|   U125|      P006|\n",
      "|   U126|      P007|\n",
      "|   U126|      P008|\n",
      "|   U126|      P009|\n",
      "|   U126|      P010|\n",
      "+-------+----------+\n",
      "\n",
      "favorite_products:\n",
      "+-------+----------+------------+\n",
      "|user_id|       key|       value|\n",
      "+-------+----------+------------+\n",
      "|   U123|newsletter|  subscribed|\n",
      "|   U123|     theme|        dark|\n",
      "|   U124|newsletter|unsubscribed|\n",
      "|   U124|     theme|       light|\n",
      "|   U124|  language|          en|\n",
      "|   U125|     theme|        dark|\n",
      "|   U126|  language|          fr|\n",
      "|   U126|  currency|        euro|\n",
      "|   U126|newsletter|  subscribed|\n",
      "+-------+----------+------------+\n",
      "\n",
      "extra_info:\n",
      "+-------+------+------+\n",
      "|user_id|field1|field2|\n",
      "+-------+------+------+\n",
      "|   U123|   foo|   bar|\n",
      "|   U124|   baz|   qux|\n",
      "|   U125|   abc|   def|\n",
      "|   U126|   ghi|   jkl|\n",
      "+-------+------+------+\n",
      "\n",
      "additional_info:\n",
      "+-------+------+------+\n",
      "|user_id|field3|field4|\n",
      "+-------+------+------+\n",
      "|   U123|  foo1|  bar1|\n",
      "|   U124|  baz1|  qux1|\n",
      "|   U125|  abc1|  def1|\n",
      "|   U126|  ghi1|  jkl1|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for each table\n",
    "users_schema = T.StructType([\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"name\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "addresses_schema = T.StructType([\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"street\", T.StringType(), True),\n",
    "    T.StructField(\"city\", T.StringType(), True),\n",
    "    T.StructField(\"postal_code\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "favorite_products_schema = T.StructType([\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"product_id\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "preferences_schema = T.StructType([\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"key\", T.StringType(), True),\n",
    "    T.StructField(\"value\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "extra_info_schema = T.StructType([\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"field1\", T.StringType(), True),\n",
    "    T.StructField(\"field2\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "additional_info_schema = T.StructType([\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"field3\", T.StringType(), True),\n",
    "    T.StructField(\"field4\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "# Define data for each table\n",
    "users_data = [\n",
    "    (\"U123\", \"John Doe\"),\n",
    "    (\"U124\", \"Jane Doe\"),\n",
    "    (\"U125\", \"Alice Smith\"),\n",
    "    (\"U126\", \"Bob Brown\")\n",
    "]\n",
    "\n",
    "addresses_data = [\n",
    "    (\"U123\", \"123 Elm St\", \"Somewhere\", \"12345\"),\n",
    "    (\"U123\", \"456 Oak St\", \"Anywhere\", \"67890\"),\n",
    "    (\"U124\", \"789 Pine St\", \"Somewhere\", \"12345\"),\n",
    "    (\"U125\", \"101 Maple St\", \"Elsewhere\", \"54321\"),\n",
    "    (\"U126\", \"202 Birch St\", \"Nowhere\", \"98765\")\n",
    "]\n",
    "\n",
    "favorite_products_data = [\n",
    "    (\"U123\", \"P001\"),\n",
    "    (\"U123\", \"P002\"),\n",
    "    (\"U123\", \"P003\"),\n",
    "    (\"U124\", \"P004\"),\n",
    "    (\"U124\", \"P005\"),\n",
    "    (\"U125\", \"P006\"),\n",
    "    (\"U126\", \"P007\"),\n",
    "    (\"U126\", \"P008\"),\n",
    "    (\"U126\", \"P009\"),\n",
    "    (\"U126\", \"P010\")\n",
    "]\n",
    "\n",
    "preferences_data = [\n",
    "    (\"U123\", \"newsletter\", \"subscribed\"),\n",
    "    (\"U123\", \"theme\", \"dark\"),\n",
    "    (\"U124\", \"newsletter\", \"unsubscribed\"),\n",
    "    (\"U124\", \"theme\", \"light\"),\n",
    "    (\"U124\", \"language\", \"en\"),\n",
    "    (\"U125\", \"theme\", \"dark\"),\n",
    "    (\"U126\", \"language\", \"fr\"),\n",
    "    (\"U126\", \"currency\", \"euro\"),\n",
    "    (\"U126\", \"newsletter\", \"subscribed\")\n",
    "]\n",
    "\n",
    "extra_info_data = [\n",
    "    (\"U123\", \"foo\", \"bar\"),\n",
    "    (\"U124\", \"baz\", \"qux\"),\n",
    "    (\"U125\", \"abc\", \"def\"),\n",
    "    (\"U126\", \"ghi\", \"jkl\")\n",
    "]\n",
    "\n",
    "additional_info_data = [\n",
    "    (\"U123\", \"foo1\", \"bar1\"),\n",
    "    (\"U124\", \"baz1\", \"qux1\"),\n",
    "    (\"U125\", \"abc1\", \"def1\"),\n",
    "    (\"U126\", \"ghi1\", \"jkl1\")\n",
    "]\n",
    "\n",
    "# Create DataFrames for each table\n",
    "users_df = spark.createDataFrame(users_data, schema=users_schema)\n",
    "addresses_df = spark.createDataFrame(addresses_data, schema=addresses_schema)\n",
    "favorite_products_df = spark.createDataFrame(favorite_products_data, schema=favorite_products_schema)\n",
    "preferences_df = spark.createDataFrame(preferences_data, schema=preferences_schema)\n",
    "extra_info_df = spark.createDataFrame(extra_info_data, schema=extra_info_schema)\n",
    "additional_info_df = spark.createDataFrame(additional_info_data, schema=additional_info_schema)\n",
    "\n",
    "# Show DataFrames\n",
    "print(\"users:\")\n",
    "users_df.show()\n",
    "\n",
    "print(\"addresses:\")\n",
    "addresses_df.show()\n",
    "\n",
    "print(\"favorite_products:\")\n",
    "favorite_products_df.show()\n",
    "\n",
    "print(\"favorite_products:\")\n",
    "preferences_df.show()\n",
    "\n",
    "print(\"extra_info:\")\n",
    "extra_info_df.show()\n",
    "\n",
    "print(\"additional_info:\")\n",
    "additional_info_df.show()\n",
    "\n",
    "users_df.createOrReplaceTempView(\"users\")\n",
    "addresses_df.createOrReplaceTempView(\"addresses\")\n",
    "favorite_products_df.createOrReplaceTempView(\"favorite_products\")\n",
    "preferences_df.createOrReplaceTempView(\"preferences\")\n",
    "extra_info_df.createOrReplaceTempView(\"extra_info\")\n",
    "additional_info_df.createOrReplaceTempView(\"additional_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+\n",
      "|user_id|address                         |\n",
      "+-------+--------------------------------+\n",
      "|U124   |[789 Pine St, Somewhere, 12345] |\n",
      "|U124   |[789 Pine St, Somewhere, 12345] |\n",
      "|U125   |[101 Maple St, Elsewhere, 54321]|\n",
      "|U126   |[202 Birch St, Nowhere, 98765]  |\n",
      "|U126   |[202 Birch St, Nowhere, 98765]  |\n",
      "|U126   |[202 Birch St, Nowhere, 98765]  |\n",
      "|U126   |[202 Birch St, Nowhere, 98765]  |\n",
      "|U123   |[456 Oak St, Anywhere, 67890]   |\n",
      "|U123   |[456 Oak St, Anywhere, 67890]   |\n",
      "|U123   |[456 Oak St, Anywhere, 67890]   |\n",
      "|U123   |[123 Elm St, Somewhere, 12345]  |\n",
      "|U123   |[123 Elm St, Somewhere, 12345]  |\n",
      "|U123   |[123 Elm St, Somewhere, 12345]  |\n",
      "+-------+--------------------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- address: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sqlQuery = '''\n",
    "select\n",
    "    users.user_id,\n",
    "    array(\n",
    "        street,\n",
    "        city,\n",
    "        postal_code\n",
    "    ) address\n",
    "from\n",
    "    users\n",
    "    left join addresses on users.user_id=addresses.user_id\n",
    "    left join favorite_products on favorite_products.user_id=addresses.user_id\n",
    "'''\n",
    "df_output = spark.sql(sqlQuery)\n",
    "df_output.show(truncate=False)\n",
    "df_output.printSchema()\n",
    "df_output.toJSON().collect()\n",
    "df_output.createOrReplaceTempView('op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+\n",
      "|user_id|             address|         col|\n",
      "+-------+--------------------+------------+\n",
      "|   U124|[789 Pine St, Som...| 789 Pine St|\n",
      "|   U124|[789 Pine St, Som...|   Somewhere|\n",
      "|   U124|[789 Pine St, Som...|       12345|\n",
      "|   U124|[789 Pine St, Som...| 789 Pine St|\n",
      "|   U124|[789 Pine St, Som...|   Somewhere|\n",
      "|   U124|[789 Pine St, Som...|       12345|\n",
      "|   U125|[101 Maple St, El...|101 Maple St|\n",
      "|   U125|[101 Maple St, El...|   Elsewhere|\n",
      "|   U125|[101 Maple St, El...|       54321|\n",
      "|   U126|[202 Birch St, No...|202 Birch St|\n",
      "|   U126|[202 Birch St, No...|     Nowhere|\n",
      "|   U126|[202 Birch St, No...|       98765|\n",
      "|   U126|[202 Birch St, No...|202 Birch St|\n",
      "|   U126|[202 Birch St, No...|     Nowhere|\n",
      "|   U126|[202 Birch St, No...|       98765|\n",
      "|   U126|[202 Birch St, No...|202 Birch St|\n",
      "|   U126|[202 Birch St, No...|     Nowhere|\n",
      "|   U126|[202 Birch St, No...|       98765|\n",
      "|   U126|[202 Birch St, No...|202 Birch St|\n",
      "|   U126|[202 Birch St, No...|     Nowhere|\n",
      "+-------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/29 07:10:36 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 51792632 ms exceeds timeout 120000 ms\n",
      "24/05/29 07:10:36 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select *, explode(address) from op          \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c2: string (nullable = false)\n",
      " |-- collect_list(c1): array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlQuery = '''\n",
    "SELECT\n",
    "    c2,\n",
    "    collect_list(c1)\n",
    "FROM (\n",
    "    VALUES \n",
    "        (1, 'A'),\n",
    "        (1, 'A'),\n",
    "        (1, 'A'),\n",
    "        (2, 'B'),\n",
    "        (2, 'B'), \n",
    "        (3, 'C')\n",
    ") AS tab(c1, c2)\n",
    "group by c2\n",
    "'''\n",
    "spark.sql(sqlQuery).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|      cl|\n",
      "+--------+\n",
      "|{1 -> A}|\n",
      "|{1 -> A}|\n",
      "|{1 -> A}|\n",
      "|{2 -> B}|\n",
      "|{2 -> B}|\n",
      "|{3 -> C}|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlQuery = '''\n",
    "SELECT\n",
    "    map(c1, c2) cl\n",
    "FROM (\n",
    "    VALUES \n",
    "        (1, 'A'),\n",
    "        (1, 'A'),\n",
    "        (1, 'A'),\n",
    "        (2, 'B'),\n",
    "        (2, 'B'), \n",
    "        (3, 'C')\n",
    ") AS tab(c1, c2)\n",
    "'''\n",
    "spark.sql(sqlQuery).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'df1.value1' does not exist. Did you mean one of the following? [value1, value2, id, id]; line 1 pos 0;\n'Join Inner, (('df1.value1 * 2) = 'df2.value2)\n:- LogicalRDD [id#698, value1#699L], false\n+- LogicalRDD [id#702, value2#703L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Join using expr for complex condition\u001b[39;00m\n\u001b[1;32m      6\u001b[0m join_expr \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mexpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf1.value1 * 2 = df2.value2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m joined_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin_expr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:1540\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1538\u001b[0m         on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jseq([])\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(how, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow should be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1540\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'df1.value1' does not exist. Did you mean one of the following? [value1, value2, id, id]; line 1 pos 0;\n'Join Inner, (('df1.value1 * 2) = 'df2.value2)\n:- LogicalRDD [id#698, value1#699L], false\n+- LogicalRDD [id#702, value2#703L], false\n"
     ]
    }
   ],
   "source": [
    "# Create sample DataFrames\n",
    "df1 = spark.createDataFrame([(\"A\", 1), (\"B\", 2), (\"C\", 3)], [\"id\", \"value1\"])\n",
    "df2 = spark.createDataFrame([(\"A\", 4), (\"B\", 5), (\"D\", 6)], [\"id\", \"value2\"])\n",
    "\n",
    "# Join using expr for complex condition\n",
    "df1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|   values|\n",
      "+---+---------+\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|   [4, 5]|\n",
      "+---+---------+\n",
      "\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  1|    2|\n",
      "|  1|    3|\n",
      "|  2|    4|\n",
      "|  2|    5|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data with nested structure\n",
    "data = [(1, [1, 2, 3]), (2, [4, 5])]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"values\"])\n",
    "\n",
    "# Explode the array column\n",
    "df.show()\n",
    "df_flattened = df.select(\"id\", F.explode(\"values\").alias(\"value\"))\n",
    "\n",
    "# Show the flattened DataFrame\n",
    "df_flattened.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3111779085.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = spark.read.format('csv').options(schema=['id', 'name', '_corrupt_records'], header='true', mode='PERMISSIVE', badRecordsPath='/home/glue_user/workspace/data-engineering/tmp/badrecords').load('C:\\Users\\raulr\\OneDrive\\myProjects\\dockerBind\\awsglue\\data-engineering\\data\\source\\src-data-csv-corrupted\\actor.csv')\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "#corrupted records\n",
    "df = spark.read.format('csv').options(schema=['id', 'name', '_corrupt_records'], header='true', mode='PERMISSIVE', badRecordsPath='/home/glue_user/workspace/data-engineering/tmp/badrecords').load('C:\\Users\\raulr\\OneDrive\\myProjects\\dockerBind\\awsglue\\data-engineering\\data\\source\\src-data-csv-corrupted\\actor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|   id|           name|\n",
      "+-----+---------------+\n",
      "|30509|      Tori Amos|\n",
      "|35329|Barbara LeakeVB|\n",
      "|12445|       Don Pike|\n",
      "+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal():\n",
    "    def __init__(self, hands, legs):\n",
    "        self.hands = hands\n",
    "        self.legs = legs\n",
    "    def run(self, speed):\n",
    "        return f\"running at speed {speed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running at speed 10'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human = Animal(2, 2)\n",
    "human.run(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
