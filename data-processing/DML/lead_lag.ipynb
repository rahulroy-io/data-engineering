{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 10:46:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = SparkSession.builder.appName('leadlag').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n",
      "|product_id|product_name|sales_date|  sales|\n",
      "+----------+------------+----------+-------+\n",
      "|         1|      iphone|2023-04-01|1700000|\n",
      "|         1|      iphone|2023-05-01|1200000|\n",
      "|         1|      iphone|2023-01-01|1500000|\n",
      "|         1|      iphone|2023-02-01|1300000|\n",
      "|         1|      iphone|2023-06-01|1100000|\n",
      "|         1|      iphone|2023-03-01|1600000|\n",
      "|         2|     samsung|2023-06-01|1100000|\n",
      "|         2|     samsung|2023-05-01| 980000|\n",
      "|         2|     samsung|2023-02-01|1120000|\n",
      "|         2|     samsung|2023-01-01|1100000|\n",
      "|         2|     samsung|2023-04-01|1800000|\n",
      "|         2|     samsung|2023-03-01|1080000|\n",
      "|         3|     oneplus|2023-03-01|1160000|\n",
      "|         3|     oneplus|2023-04-01|1170000|\n",
      "|         3|     oneplus|2023-06-01|1200000|\n",
      "|         3|     oneplus|2023-05-01|1175000|\n",
      "|         3|     oneplus|2023-02-01|1120000|\n",
      "|         3|     oneplus|2023-01-01|1100000|\n",
      "+----------+------------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema=['id','name','salary','dept','gender']\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "emp_df = emp_df.select('id','name','salary','gender','dept')\n",
    "#emp_df.show()\n",
    "\n",
    "product_schema = [\"product_id\", \"product_name\", \"sales_date\", \"sales\"]\n",
    "product_df = spark.createDataFrame(data=product_data, schema=product_schema)\\\n",
    "                    .withColumn('sales_date', F.expr(\"to_date(sales_date, 'dd-MM-yyyy')\"))\\\n",
    "                    #.withColumn('previous_sales', F.expr('lag(sales) over(partition by product_id order by sales_date desc)'))\\\n",
    "                    #.withColumn('profit%', F.expr(\"cast ((sales-previous_sales)/previous_sales as decimal(10, 2))\"))\n",
    "                    \n",
    "product_df.sort(F.expr(\"product_id\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 12:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/27 12:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/27 12:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/27 12:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/27 12:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/12/27 12:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|       dts|        lg|\n",
      "+---+----------+----------+\n",
      "|  0|2023-12-01|      null|\n",
      "|  1|2023-11-01|2023-12-01|\n",
      "|  2|2023-10-01|2023-11-01|\n",
      "|  3|2023-09-01|2023-10-01|\n",
      "|  4|2023-08-01|2023-09-01|\n",
      "|  5|2023-07-01|2023-08-01|\n",
      "|  6|2023-06-01|2023-07-01|\n",
      "|  7|2023-05-01|2023-06-01|\n",
      "|  8|2023-04-01|2023-05-01|\n",
      "|  9|2023-03-01|2023-04-01|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff = spark.range(10).select('*', F.expr(\"add_months(to_date('2023-12-01'), -1*id) dts\"))\n",
    "dff.withColumn('lg', F.expr(\"lead(dts, 1) over(order by dts)\")).orderBy('id').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
